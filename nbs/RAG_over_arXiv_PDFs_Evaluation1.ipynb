{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YErqpfH9jVI"
      },
      "source": [
        "# RAG Evaluation\n",
        "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
        "\n",
        "For an introduction to RAG, you can check [this other cookbook](rag_zephyr_langchain)!\n",
        "\n",
        "RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
        "\n",
        "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance!\n",
        "So let's see how to evaluate our RAG system.\n",
        "\n",
        "### Evaluating RAG performance\n",
        "\n",
        "Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
        "\n",
        "For our evaluation pipeline, we will need:\n",
        "1. An evaluation dataset with question - answer couples (QA couples)\n",
        "2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
        "\n",
        "‚û°Ô∏è It turns out, we can use LLMs to help us all along the way!\n",
        "1. The evaluation dataset will be synthetically generated by an LLM ü§ñ, and questions will be filtered out by other LLMs ü§ñ\n",
        "2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent ü§ñ will then perform the evaluation on this synthetic dataset.\n",
        "\n",
        "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCKBvOcp9jVK"
      },
      "outputs": [],
      "source": [
        "#!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_lJFbYm9jVL"
      },
      "outputs": [],
      "source": [
        "# %reload_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oIlNZ1Mn9jVL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BOwG1Nuxtkj-"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8528b957fe2641778e0c60ed08df71b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeW8P62J9jVM"
      },
      "source": [
        "### Load your knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "YRbm5tNF9jVM"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9CKj0M9jVM"
      },
      "source": [
        "# 1. Build a synthetic dataset for evaluation\n",
        "We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
        "\n",
        "Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoEgiDg9jVM"
      },
      "source": [
        "### 1.1. Prepare source documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "from pathlib import Path\n",
        "PDFS_PATH = Path('/home/mainuser/Desktop/LLMs/RagOverArXiv/temp')\n",
        "PDFS = list(PDFS_PATH.glob('*.pdf'))\n",
        "PDFS[0], len(PDFS)\n",
        "\n",
        "reader = PdfReader(os.path.expanduser(PDFS[0]))\n",
        "pages = reader.pages\n",
        "documents = []\n",
        "for page in pages:\n",
        "  documents.append(page.extract_text())\n",
        "\n",
        "\n",
        "def load_pdf_to_string(pdf_path):\n",
        "    # Open the PDF file in binary mode\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        # Create a PDF file reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Initialize an empty string to hold the text\n",
        "        text = ''\n",
        "\n",
        "        # Loop through each page and extract the text\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            references_index= page_text.upper().find('\\nREFERENCES\\n')\n",
        "            if references_index != -1:\n",
        "              page_text = page_text[:references_index]\n",
        "              text += page_text\n",
        "              return text\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Use the function to load a PDF into a string\n",
        "text = load_pdf_to_string(os.path.expanduser(PDFS[0]))\n",
        "def get_title(pdf_path): return os.path.expanduser(pdf_path).split('/')[-1]\n",
        "\n",
        "all_docs_and_titles = [(load_pdf_to_string(os.path.expanduser(pdf_path)),get_title(pdf_path)) for pdf_path in PDFS]\n",
        "\n",
        "all_docs = [doc[0] for doc in all_docs_and_titles]\n",
        "all_titles = [doc[1] for doc in all_docs_and_titles]\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document \n",
        "\n",
        "CHUNK_SIZE = 1000 #try 2000 next\n",
        "CHUNK_OVERLAP = 30 #try 200 next\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap = CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "docs_processed  = [text_splitter.split_documents([Document(page_content=doc, metadata={'source':all_titles[idx]})]) \n",
        "         for idx,doc in enumerate(all_docs)]\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=2000,\n",
        "#     chunk_overlap=200,\n",
        "#     add_start_index=True,\n",
        "#     #separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "# )\n",
        "\n",
        "# docs_processed = []\n",
        "# for idx,doc in enumerate(all_docs):\n",
        "#     docs_processed += text_splitter.split_documents([Document(page_content=doc, metadata={'source':all_titles[idx]})])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_processed = [txt for doc in docs_processed for txt in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='FlashAttention : Fast and Memory-EÔ¨Écient Exact Attention\\nwith IO-Awareness\\nTri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher R√©y\\nyDepartment of Computer Science, Stanford University\\nzDepartment of Computer Science and Engineering, University at BuÔ¨Äalo, SUNY\\n{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,\\nchrismre@cs.stanford.edu\\nJune 24, 2022\\nAbstract\\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity\\nof self-attention are quadratic in sequence length. Approximate attention methods have attempted\\nto address this problem by trading oÔ¨Ä model quality to reduce the compute complexity, but often do\\nnot achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-\\naware‚Äîaccounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes', metadata={'source': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'})"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_processed[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "3gTOlRKO9jVM"
      },
      "outputs": [],
      "source": [
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "# langchain_docs = [\n",
        "#     LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "#     for doc in tqdm(ds)\n",
        "# ]\n",
        "\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=2000,\n",
        "#     chunk_overlap=200,\n",
        "#     add_start_index=True,\n",
        "#     separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "# )\n",
        "\n",
        "# docs_processed = []\n",
        "# for doc in langchain_docs:\n",
        "#     docs_processed += text_splitter.split_documents([doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjrNhcCh9jVN"
      },
      "source": [
        "### 1.2. Setup agents for question generation\n",
        "\n",
        "We use [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "GoRySj3Q9jVN",
        "outputId": "47dcbedc-a1fe-4b3e-82a4-5db3d49f2909"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import InferenceClient\n",
        "\n",
        "\n",
        "# repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "# llm_client = InferenceClient(\n",
        "#     model=repo_id,\n",
        "#     timeout=120,\n",
        "# )\n",
        "\n",
        "\n",
        "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "#     response = inference_client.post(\n",
        "#         json={\n",
        "#             \"inputs\": prompt,\n",
        "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
        "#             \"task\": \"text-generation\",\n",
        "#         },\n",
        "#     )\n",
        "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "# call_llm(llm_client, \"This is a test context\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Tried Mixtral4Bit, visually perhaps a bit better, but overfits on 'deep question'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        }
      ],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "generator_config = ExLlamaV2Config()\n",
        "generator_config.model_dir = \"/home/mainuser/Desktop/LLMs/MiStralInference\"\n",
        "#generator_config.model_dir = '/home/mainuser/Desktop/LLMs/Mixtral4bit'\n",
        "generator_config.prepare()\n",
        "\n",
        "generator_model = ExLlamaV2(generator_config)\n",
        "cache = ExLlamaV2Cache(generator_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "generator_model.load_autosplit(cache)\n",
        "\n",
        "generator_tokenizer = ExLlamaV2Tokenizer(generator_config)\n",
        "generator_llm = ExLlamaV2StreamingGenerator(generator_model, cache, generator_tokenizer)\n",
        "generator_llm.set_stop_conditions([generator_tokenizer.eos_token_id])\n",
        "generator_settings = ExLlamaV2Sampler.Settings()\n",
        "generator_settings.temperature = 0.85\n",
        "generator_settings.top_k = 50\n",
        "generator_settings.top_p = 0.8\n",
        "generator_settings.token_repetition_penalty = 1.01\n",
        "#generator_settings.disallow_tokens(generator_tokenizer, [generator_tokenizer.eos_token_id])\n",
        "# see if commenting out the above solved the endless generation issue (did not have with stream generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "#help(generator_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "#help(generator_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "#help(generator_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "#help(generator_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import Pipeline\n",
        "# from ragatouille import RAGPretrainedModel\n",
        "# from typing import Optional, List, Tuple\n",
        "# from langchain.docstore.document import Document\n",
        "# import time\n",
        "# # RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "\n",
        "# #     prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "\n",
        "# # )\n",
        "# RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "# from langchain.docstore.document import Document as LangchainDocument\n",
        "# def call_llm(\n",
        "#     question: str,\n",
        "#     generator_model: ExLlamaV2,\n",
        "#     generator_llm: ExLlamaV2StreamingGenerator,\n",
        "#     tokenizer: ExLlamaV2Tokenizer,\n",
        "#     settings:ExLlamaV2Sampler.Settings,\n",
        "#     max_new_tokens = 512\n",
        "# ) -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "\n",
        "#     instruction_ids = tokenizer.encode(f\"<s>[INST] {question} [/INST]\", add_bos = True)\n",
        "#     context_ids = instruction_ids if generator_llm.sequence_ids is None \\\n",
        "#             else torch.cat([generator_llm.sequence_ids, instruction_ids], dim = -1)\n",
        "\n",
        "#     max_new_tokens = max_new_tokens\n",
        "\n",
        "#     generator_llm.warmup()\n",
        "#     time_begin = time.time()\n",
        "\n",
        "#     output = generator_model.forward(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens)#, seed = 1234)#,add_eos=True)\n",
        "    \n",
        "#     time_end = time.time()\n",
        "#     time_total = time_end - time_begin\n",
        "\n",
        "#     print(output)\n",
        "#     print()\n",
        "#     print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")\n",
        "#     return output\n",
        "# #     generator.begin_stream(context_ids, settings)\n",
        "\n",
        "# # #    return generator.generate_simple(context_ids, settings,num_tokens=512)\n",
        "\n",
        "# #     while True:\n",
        "# #         chunk, eos, _ = generator.stream()\n",
        "# #         if eos: break\n",
        "# #         print(chunk, end = \"\")\n",
        "# #         sys.stdout.flush()\n",
        "# #     #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "# #     response = inference_client.post(\n",
        "# #         json={\n",
        "# #             \"inputs\": prompt,\n",
        "# #             \"parameters\": {\"max_new_tokens\": 1000},\n",
        "# #             \"task\": \"text-generation\",\n",
        "# #         },\n",
        "# #     )\n",
        "# #     return json.loads(response.decode())[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "# call_llm(question=\"How can I get my cat to like me?\", generator_model=generator_model,generator_llm=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] How can I get my cat to like me? [/INST] 1. Spend time with your cat: Cats enjoy spending time with their owners, so take some time to play with them, cuddle with them, and give them attention.\n",
            "2. Provide food and shelter: Make sure your cat has a comfortable place to sleep, eat, and drink water.\n",
            "3. Use positive reinforcement: Reward your cat with treats, praise, and affection when they behave well around you.\n",
            "4. Be patient: Cats can take time to warm up to new people, so be patient and keep trying to build a relationship with your cat.\n",
            "5. Show affection: Cats are social animals and enjoy physical touch, so try petting them, stroking their fur, and giving them gentle scratches behind their ears.\n",
            "6. Provide toys: Cats love to play, so provide them with toys that they can interact with and enjoy.\n",
            "7. Be consistent: Consistency is key when it comes to building a relationship with your cat. Make sure you are consistently spending time with them and providing them with everything they need.\n",
            "8. Avoid negative reinforcement: Avoid punishing your cat for bad behavior, as this can damage the relationship you are trying to build. Instead, focus on positive reinforcement and rewarding good behavior.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'<s>[INST] How can I get my cat to like me? [/INST] 1. Spend time with your cat: Cats enjoy spending time with their owners, so take some time to play with them, cuddle with them, and give them attention.\\n2. Provide food and shelter: Make sure your cat has a comfortable place to sleep, eat, and drink water.\\n3. Use positive reinforcement: Reward your cat with treats, praise, and affection when they behave well around you.\\n4. Be patient: Cats can take time to warm up to new people, so be patient and keep trying to build a relationship with your cat.\\n5. Show affection: Cats are social animals and enjoy physical touch, so try petting them, stroking their fur, and giving them gentle scratches behind their ears.\\n6. Provide toys: Cats love to play, so provide them with toys that they can interact with and enjoy.\\n7. Be consistent: Consistency is key when it comes to building a relationship with your cat. Make sure you are consistently spending time with them and providing them with everything they need.\\n8. Avoid negative reinforcement: Avoid punishing your cat for bad behavior, as this can damage the relationship you are trying to build. Instead, focus on positive reinforcement and rewarding good behavior.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Working except eos\n",
        "from transformers import Pipeline\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "import time\n",
        "# RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "\n",
        "#     prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "\n",
        "# )\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def call_llm(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "    tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "\n",
        "    # instruction_ids = tokenizer.encode(f\"<s>[INST] {question} [/INST]\", add_bos = True)\n",
        "    # context_ids = instruction_ids if generator.sequence_ids is None \\\n",
        "    #         else torch.cat([generator.sequence_ids, instruction_ids], dim = -1)\n",
        "\n",
        "    max_new_tokens = max_new_tokens\n",
        "\n",
        "    generator.warmup()\n",
        "    #time_begin = time.time()\n",
        "    output = generator.generate_simple(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "    #output = generator.generate_simple(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens, seed = 1234)\n",
        "    #output = generator.generate_simple(question, settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "    # time_end = time.time()\n",
        "    # time_total = time_end - time_begin\n",
        "\n",
        "    print(output)\n",
        "    print()\n",
        "    #print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")\n",
        "    return output\n",
        "#     generator.begin_stream(context_ids, settings)\n",
        "\n",
        "# #    return generator.generate_simple(context_ids, settings,num_tokens=512)\n",
        "\n",
        "#     while True:\n",
        "#         chunk, eos, _ = generator.stream()\n",
        "#         if eos: break\n",
        "#         print(chunk, end = \"\")\n",
        "#         sys.stdout.flush()\n",
        "#     #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "#     response = inference_client.post(\n",
        "#         json={\n",
        "#             \"inputs\": prompt,\n",
        "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
        "#             \"task\": \"text-generation\",\n",
        "#         },\n",
        "#     )\n",
        "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "call_llm(question=\"How can I get my cat to like me?\", generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,max_new_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "hIM_DJRo9jVN"
      },
      "outputs": [],
      "source": [
        "# QA_generation_prompt = \"\"\"\n",
        "# Your task is to write a factoid question and an answer given a context.\n",
        "# Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
        "# Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
        "# This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Output:::\n",
        "# Factoid question: (your factoid question)\n",
        "# Answer: (your answer to the factoid question)\n",
        "\n",
        "# Now here is the context.\n",
        "\n",
        "# Context: {context}\\n\n",
        "# Output:::\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
        "Your deep question should be unambigiously answerable from the context.\n",
        "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
        "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Deep question: (your deep question)\n",
        "Answer: (your answer to the deep question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFc-lVy9jVN"
      },
      "source": [
        "Now let's generate our QA couples.\n",
        "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
        "\n",
        "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8fteqDDD9jVN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 10 QA couples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|‚ñà         | 1/10 [00:01<00:10,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort.\n",
            "Implementations may also not be transferrable across GPU architectures. These limitations suggest the\n",
            "need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\n",
            "compiling to IO-aware implementations in CUDA‚Äîsimilar to eÔ¨Äorts such as Halide in image processing [ 70].\n",
            "IO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\n",
            "Attention is the most memory-intensive computation in Transformers, but every layer in a deep network\n",
            "touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\n",
            "these potential extensions in Appendix D.\n",
            "Multi-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-\n",
            "stants for computing attention on a single GPU. However, the attention computation may be parallelizable\n",
            "\n",
            "Output::: [/INST] Deep question: Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\n",
            "\n",
            "Answer: Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|‚ñà‚ñà        | 2/10 [00:02<00:10,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: cations. In this section, we highlight how to leverage system prompting to optionally enforce output\n",
            "constraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\n",
            "fine-grained content moderation, which can be useful to enforce quality content in applications.\n",
            "5.1 System prompt to enforce guardrails\n",
            "We introduce a system prompt (see below) to guide the model to generate answers within specified\n",
            "guardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\n",
            "Pareto front of model utility / guardrails enforcement, as indicated in Table 4.\n",
            "Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\n",
            "unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
            "Guardrails MT Bench\n",
            "No system prompt 6.84 ¬±0.07\n",
            "Llama 2 system prompt 6.38 ¬±0.07\n",
            "Mistral system prompt 6.58 ¬±0.05\n",
            "Table 4: System prompts. Mean official\n",
            "MT Bench score over 10 iterations with\n",
            "\n",
            "Output::: [/INST] Deep question: Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\n",
            "\n",
            "Answer: Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|‚ñà‚ñà‚ñà       | 3/10 [00:03<00:09,  1.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\n",
            "without sacrificing performance on non-code related benchmarks.\n",
            "Mistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\n",
            "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
            "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
            "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
            "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
            "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
            "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
            "\n",
            "Output::: [/INST] Deep question: Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\n",
            "\n",
            "Answer: Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:13<00:27,  4.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: ùëÅ\n",
            "ùêµùëêm\n",
            "blocks\n",
            "K1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\n",
            "4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\n",
            "divideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\n",
            "5:for1\u0014ùëó\u0014ùëáùëêdo\n",
            "6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\n",
            "7:for1\u0014ùëñ\u0014ùëáùëüdo\n",
            "8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\n",
            "9:On chip, compute Sùëñùëó=QùëñKùëá\n",
            "ùëó2Rùêµùëü\u0002ùêµùëê.\n",
            "10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\n",
            "rowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\n",
            "11:On chip, compute ùëönew\n",
            "ùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\n",
            "ùëñ=ùëíùëöùëñ\u0000ùëönew\n",
            "ùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\n",
            "ùëñ~‚Ñìùëñùëó2Rùêµùëü.\n",
            "12:Write Oùëñ diag¬π‚Ñìnew\n",
            "ùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\n",
            "ùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\n",
            "ùëñ~PùëñùëóVùëó¬∫to HBM.\n",
            "13:Write‚Ñìùëñ ‚Ñìnew\n",
            "ùëñ,ùëöùëñ ùëönew\n",
            "ùëñto HBM.\n",
            "14:end for\n",
            "15:end for\n",
            "16:Return O.\n",
            "We show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\n",
            "Theorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\n",
            "memory beyond inputs and output.\n",
            "3.2 Analysis: IO Complexity of FlashAttention\n",
            "\n",
            "Output::: [/INST] Deep question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:14<00:16,  3.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\n",
            "unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\n",
            "3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\n",
            "better than as reported in the original comparison [80].\n",
            "8Attention Memory Usage\n",
            "Sequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\n",
            "Sequence LengthRuntime (ms)\n",
            "Memory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\n",
            "1020\n",
            "FlashAttention\n",
            "Block-Sparse FlashAttentionPyTorch Attention\n",
            "Megatron AttentionLinformer Attention\n",
            "OpenAI Sparse Attention8192100Crossover Points\n",
            "20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.\n",
            "European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights\n",
            "\n",
            "Output::: [/INST] Deep question: Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\n",
            "\n",
            "Answer: The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:15<00:09,  2.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\n",
            "on Operating Systems Principles , 2023.\n",
            "[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\n",
            "Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\n",
            "xformers: A modular and hackable transformer modelling library. https://github.com/\n",
            "facebookresearch/xformers , 2022.\n",
            "[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\n",
            "electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,\n",
            "2018.\n",
            "[20] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\n",
            "Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al. Code llama: Open foundation models\n",
            "for code. arXiv preprint arXiv:2308.12950 , 2023.\n",
            "[21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\n",
            "\n",
            "Output::: [/INST] Deep question: How does paged attention affect the performance of a gauge model serving in a production environment?\n",
            "\n",
            "Answer: Paged attention can improve the performance of a gauge model serving in a production environment by reducing the memory footprint and speeding up the inference process.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:15<00:05,  1.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: our chunk size. For each chunk, we thus need to compute the attention over the cache and over the\n",
            "chunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\n",
            "godog0000100000thetoThecatsatonthe\n",
            "1matand111sawthe1000doggoto\n",
            "100000110000000011100000011110PastCacheCurrent\n",
            "Figure 3: Pre-fill and chunking. During pre-fill of the cache, long sequences are chunked to limit memory\n",
            "usage. We process a sequence in three chunks, ‚ÄúThe cat sat on‚Äù, ‚Äúthe mat and saw‚Äù, ‚Äúthe dog go to‚Äù. The figure\n",
            "shows what happens for the third chunk (‚Äúthe dog go to‚Äù): it attends itself using a causal mask (rightmost block),\n",
            "attends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\n",
            "the sliding window (left block).\n",
            "3 Results\n",
            "We compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for\n",
            "fair comparison. We measure performance on a wide variety of tasks categorized as follow:\n",
            "\n",
            "Output::: [/INST] Deep question: What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\n",
            "\n",
            "Answer: The purpose of chunking in the context of the pre-fill process in Mistral 7B is to limit memory usage by processing long sequences in smaller, more manageable chunks.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:17<00:03,  1.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: Mistral 7B\n",
            "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
            "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
            "Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux,\n",
            "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix,\n",
            "William El Sayed\n",
            "Abstract\n",
            "We introduce Mistral 7B, a 7‚Äìbillion-parameter language model engineered for\n",
            "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
            "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
            "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
            "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
            "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
            "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
            "Mistral 7B ‚Äì Instruct, that surpasses Llama 2 13B ‚Äì chat model both on human and\n",
            "\n",
            "Output::: [/INST] Deep question: Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\n",
            "\n",
            "Answer: The grouped-query attention (GQA) mechanism in Mistral 7B groups multiple queries together and processes them simultaneously, reducing the inference cost. The sliding window attention (SWA) mechanism enables the model to effectively handle sequences of arbitrary length by processing the sequence in smaller windows, further reducing the inference cost. These mechanisms, combined with other optimizations, enable Mistral 7B to outperform other language models in terms of performance and efficiency.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:26<00:04,  4.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: We apply two established techniques (tiling, recomputation) to overcome the technical challenge of\n",
            "computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea\n",
            "is that we split the inputs Q¬ñK¬ñVinto blocks, load them from slow HBM to fast SRAM, then compute the\n",
            "attention output with respect to those blocks. By scaling the output of each block by the right normalization\n",
            "factor before adding them up, we get the correct result at the end.\n",
            "Tiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large\n",
            "softmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector ùë•2Rùêµis computed as:\n",
            "ùëö¬πùë•¬∫:=max\n",
            "ùëñùë•ùëñ¬ñ ùëì¬πùë•¬∫:=\u0002\n",
            "ùëíùë•1\u0000ùëö¬πùë•¬∫¬ï¬ï¬ï ùëíùë•ùêµ\u0000ùëö¬πùë•¬∫\u0003\n",
            "¬ñ ‚Ñì¬πùë•¬∫:=‚àëÔ∏Å\n",
            "ùëñùëì¬πùë•¬∫ùëñ¬ñsoftmax¬πùë•¬∫:=ùëì¬πùë•¬∫\n",
            "‚Ñì¬πùë•¬∫¬ï\n",
            "4For vectors ùë•¬π1¬∫¬ñùë•¬π2¬∫2Rùêµ, we can decompose the softmax of the concatenated ùë•=\u0002\n",
            "ùë•¬π1¬∫ùë•¬π2¬∫\u0003\n",
            "2R2ùêµas:\n",
            "ùëö¬πùë•¬∫=ùëö¬π\u0002\n",
            "ùë•¬π1¬∫ùë•¬π2¬∫\u0003\n",
            "¬∫=max¬πùëö¬πùë•¬π1¬∫¬∫¬ñùëö¬πùë•¬π2¬∫¬∫¬∫¬ñ ùëì¬πùë•¬∫=h\n",
            "ùëíùëö¬πùë•¬π1¬∫¬∫\u0000ùëö¬πùë•¬∫ùëì¬πùë•¬π1¬∫¬∫ùëíùëö¬πùë•¬π2¬∫¬∫\u0000ùëö¬πùë•¬∫ùëì¬πùë•¬π2¬∫¬∫i\n",
            "¬ñ\n",
            "‚Ñì¬πùë•¬∫=‚Ñì¬π\u0002\n",
            "ùë•¬π1¬∫ùë•¬π2¬∫\u0003\n",
            "\n",
            "Output::: [/INST] Deep question: Can you explain how the attention output is computed in Algorithm 1?\n",
            "\n",
            "Answer: The attention output is computed by splitting the inputs Q-K-V into blocks, loading them from slow HBM to fast SRAM, and then computing the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, the correct result is obtained at the end. The softmax is decomposed using scaling for numerical stability. For vectors ùë•¬π1¬π-ùë•¬π2¬≤Rùêµ, the softmax of the concatenated ùë•=ùë•¬π1¬πùë•¬π2¬≤Rùêµ is decomposed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas and ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas. ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas is computed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬π\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:27<00:00,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
            "Your deep question should be unambigiously answerable from the context.\n",
            "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
            "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Output:::\n",
            "Deep question: (your deep question)\n",
            "Answer: (your answer to the deep question)\n",
            "\n",
            "Now here is the context.\n",
            "\n",
            "Context: or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
            "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
            "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
            "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B ‚Äì Chat model.\n",
            "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
            "large language models efficient. Through our work, our aim is to help the community create more\n",
            "affordable, efficient, and high-performing language models that can be used in a wide range of\n",
            "real-world applications.\n",
            "2 Architectural details\n",
            "Figure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\n",
            "length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher\n",
            "\n",
            "Output::: [/INST] Deep question: Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\n",
            "\n",
            "Answer: The Mistral 7B language model is designed to balance high performance and efficiency by using a sliding window attention mechanism. This mechanism allows the model to attend to a smaller portion of the input sequence at a time, reducing the number of operations and memory usage required for inference. Additionally, the model is crafted for ease of fine-tuning across a wide range of tasks, making it adaptable and efficient for various real-world applications.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "N_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    # Generate QA couple\n",
        "    # output_QA_couple = call_llm(\n",
        "    #     llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    # )\n",
        "    output_QA_couple = call_llm(question=QA_generation_prompt.format(context=sampled_context.page_content), generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    try:\n",
        "        question = output_QA_couple.split(\"Deep question: \")[-1].split(\"Answer: \")[0]\n",
        "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
        "        #assert len(answer) < 300, \"Answer is too long\"\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"context\": sampled_context.page_content,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'context': 'algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort.\\nImplementations may also not be transferrable across GPU architectures. These limitations suggest the\\nneed for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\\ncompiling to IO-aware implementations in CUDA‚Äîsimilar to eÔ¨Äorts such as Halide in image processing [ 70].\\nIO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\\nAttention is the most memory-intensive computation in Transformers, but every layer in a deep network\\ntouches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\\nthese potential extensions in Appendix D.\\nMulti-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-\\nstants for computing attention on a single GPU. However, the attention computation may be parallelizable',\n",
              "  'question': 'Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n',\n",
              "  'answer': 'Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'cations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n5.1 System prompt to enforce guardrails\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified\\nguardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\\nPareto front of model utility / guardrails enforcement, as indicated in Table 4.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\\nunethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\nGuardrails MT Bench\\nNo system prompt 6.84 ¬±0.07\\nLlama 2 system prompt 6.38 ¬±0.07\\nMistral system prompt 6.58 ¬±0.05\\nTable 4: System prompts. Mean official\\nMT Bench score over 10 iterations with',\n",
              "  'question': 'Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n',\n",
              "  'answer': 'Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,',\n",
              "  'question': 'Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n',\n",
              "  'answer': 'Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'ùëÅ\\nùêµùëêm\\nblocks\\nK1\\x96\\x95\\x95\\x95\\x96KùëáùëêandV1\\x96\\x95\\x95\\x95\\x96Vùëáùëê, of sizeùêµùëê\\x02ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ\\x96\\x95\\x95\\x95\\x96Oùëáùëüof sizeùêµùëü\\x02ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ\\x96\\x95\\x95\\x95\\x96‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1\\x96\\x95\\x95\\x95\\x96ùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\\x14ùëó\\x14ùëáùëêdo\\n6:Load Kùëó\\x96Vùëófrom HBM to on-chip SRAM.\\n7:for1\\x14ùëñ\\x14ùëáùëüdo\\n8:Load Qùëñ\\x96Oùëñ\\x96‚Ñìùëñ\\x96ùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\\x02ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\\x00~ùëöùëñùëó¬∫ 2Rùêµùëü\\x02ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ\\x96~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\\x00ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\\x00ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\\x001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\\x00ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\\x00ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention',\n",
              "  'question': 'Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22',\n",
              "  'answer': '(your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1\\x96\\x95\\x95\\x95\\x96KùëáùëêandV1\\x96\\x95\\x95\\x95\\x96Vùëáùëê, of sizeùêµùëê\\x02ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ\\x96\\x95\\x95\\x95\\x96Oùëáùëüof sizeùêµùëü\\x02ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ\\x96\\x95\\x95\\x95\\x96‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1\\x96\\x95\\x95\\x95\\x96ùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\\x14ùëó\\x14ùëáùëêdo\\n6:Load Kùëó\\x96Vùëófrom HBM to on-chip SRAM.\\n7:for1\\x14ùëñ\\x14ùëáùëüdo\\n8:Load Qùëñ\\x96Oùëñ\\x96‚Ñìùëñ\\x96ùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\\x02ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\\x00~ùëöùëñùëó¬∫ 2Rùêµùëü\\x02ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ\\x96~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\\x00ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\\x00ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\\x001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\\x00ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\\x00ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention\\n\\nOutput::: [/INST] Deep question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\\nunit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\\n3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\\nbetter than as reported in the original comparison [80].\\n8Attention Memory Usage\\nSequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\\nSequence LengthRuntime (ms)\\nMemory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\\n1020\\nFlashAttention\\nBlock-Sparse FlashAttentionPyTorch Attention\\nMegatron AttentionLinformer Attention\\nOpenAI Sparse Attention8192100Crossover Points\\n20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.\\nEuropean Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights',\n",
              "  'question': 'Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n',\n",
              "  'answer': 'The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\\non Operating Systems Principles , 2023.\\n[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\\nxformers: A modular and hackable transformer modelling library. https://github.com/\\nfacebookresearch/xformers , 2022.\\n[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,\\n2018.\\n[20] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\\nYossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al. Code llama: Open foundation models\\nfor code. arXiv preprint arXiv:2308.12950 , 2023.\\n[21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An',\n",
              "  'question': 'How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n',\n",
              "  'answer': 'Paged attention can improve the performance of a gauge model serving in a production environment by reducing the memory footprint and speeding up the inference process.',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'our chunk size. For each chunk, we thus need to compute the attention over the cache and over the\\nchunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\\ngodog0000100000thetoThecatsatonthe\\n1matand111sawthe1000doggoto\\n100000110000000011100000011110PastCacheCurrent\\nFigure 3: Pre-fill and chunking. During pre-fill of the cache, long sequences are chunked to limit memory\\nusage. We process a sequence in three chunks, ‚ÄúThe cat sat on‚Äù, ‚Äúthe mat and saw‚Äù, ‚Äúthe dog go to‚Äù. The figure\\nshows what happens for the third chunk (‚Äúthe dog go to‚Äù): it attends itself using a causal mask (rightmost block),\\nattends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\\nthe sliding window (left block).\\n3 Results\\nWe compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for\\nfair comparison. We measure performance on a wide variety of tasks categorized as follow:',\n",
              "  'question': 'What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n',\n",
              "  'answer': 'The purpose of chunking in the context of the pre-fill process in Mistral 7B is to limit memory usage by processing long sequences in smaller, more manageable chunks.',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7‚Äìbillion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B ‚Äì Instruct, that surpasses Llama 2 13B ‚Äì chat model both on human and',\n",
              "  'question': 'Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\\n\\n',\n",
              "  'answer': 'The grouped-query attention (GQA) mechanism in Mistral 7B groups multiple queries together and processes them simultaneously, reducing the inference cost. The sliding window attention (SWA) mechanism enables the model to effectively handle sequences of arbitrary length by processing the sequence in smaller windows, further reducing the inference cost. These mechanisms, combined with other optimizations, enable Mistral 7B to outperform other language models in terms of performance and efficiency.',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'We apply two established techniques (tiling, recomputation) to overcome the technical challenge of\\ncomputing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea\\nis that we split the inputs Q\\x96K\\x96Vinto blocks, load them from slow HBM to fast SRAM, then compute the\\nattention output with respect to those blocks. By scaling the output of each block by the right normalization\\nfactor before adding them up, we get the correct result at the end.\\nTiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large\\nsoftmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector ùë•2Rùêµis computed as:\\nùëö¬πùë•¬∫:=max\\nùëñùë•ùëñ\\x96 ùëì¬πùë•¬∫:=\\x02\\nùëíùë•1\\x00ùëö¬πùë•¬∫\\x95\\x95\\x95 ùëíùë•ùêµ\\x00ùëö¬πùë•¬∫\\x03\\n\\x96 ‚Ñì¬πùë•¬∫:=‚àëÔ∏Å\\nùëñùëì¬πùë•¬∫ùëñ\\x96softmax¬πùë•¬∫:=ùëì¬πùë•¬∫\\n‚Ñì¬πùë•¬∫\\x95\\n4For vectors ùë•¬π1¬∫\\x96ùë•¬π2¬∫2Rùêµ, we can decompose the softmax of the concatenated ùë•=\\x02\\nùë•¬π1¬∫ùë•¬π2¬∫\\x03\\n2R2ùêµas:\\nùëö¬πùë•¬∫=ùëö¬π\\x02\\nùë•¬π1¬∫ùë•¬π2¬∫\\x03\\n¬∫=max¬πùëö¬πùë•¬π1¬∫¬∫\\x96ùëö¬πùë•¬π2¬∫¬∫¬∫\\x96 ùëì¬πùë•¬∫=h\\nùëíùëö¬πùë•¬π1¬∫¬∫\\x00ùëö¬πùë•¬∫ùëì¬πùë•¬π1¬∫¬∫ùëíùëö¬πùë•¬π2¬∫¬∫\\x00ùëö¬πùë•¬∫ùëì¬πùë•¬π2¬∫¬∫i\\n\\x96\\n‚Ñì¬πùë•¬∫=‚Ñì¬π\\x02\\nùë•¬π1¬∫ùë•¬π2¬∫\\x03',\n",
              "  'question': 'Can you explain how the attention output is computed in Algorithm 1?\\n\\n',\n",
              "  'answer': 'The attention output is computed by splitting the inputs Q-K-V into blocks, loading them from slow HBM to fast SRAM, and then computing the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, the correct result is obtained at the end. The softmax is decomposed using scaling for numerical stability. For vectors ùë•¬π1¬π-ùë•¬π2¬≤Rùêµ, the softmax of the concatenated ùë•=ùë•¬π1¬πùë•¬π2¬≤Rùêµ is decomposed as: ùëö¬πùë•¬∫=ùëö¬π\\x02ùë•¬π1¬πùë•¬π2¬≤Rùêµas and ùëö¬πùë•¬∫=ùëö¬π\\x02ùë•¬π1¬πùë•¬π2¬≤Rùêµas. ùëö¬πùë•¬∫=ùëö¬π\\x02ùë•¬π1¬πùë•¬π2¬≤Rùêµas is computed as: ùëö¬πùë•¬∫=ùëö¬π\\x02ùë•¬π1¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬π',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B ‚Äì Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\naffordable, efficient, and high-performing language models that can be used in a wide range of\\nreal-world applications.\\n2 Architectural details\\nFigure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\\nlength, and the memory increases linearly with the number of tokens. At inference time, this incurs higher',\n",
              "  'question': 'Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n',\n",
              "  'answer': 'The Mistral 7B language model is designed to balance high performance and efficiency by using a sliding window attention mechanism. This mechanism allows the model to attend to a smaller portion of the input sequence at a time, reducing the number of operations and memory usage required for inference. Additionally, the model is crafted for ease of fine-tuning across a wide range of tasks, making it adaptable and efficient for various real-world applications.',\n",
              "  'source_doc': 'Mistral7B.pdf'}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aUlOUDv59jVN",
        "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort.\\nImplementations may also not be transferrable across GPU architectures. These limitations suggest the\\nneed for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\\ncompiling to IO-aware implementations in CUDA‚Äîsimilar to eÔ¨Äorts such as Halide in image processing [ 70].\\nIO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\\nAttention is the most memory-intensive computation in Transformers, but every layer in a deep network\\ntouches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\\nthese potential extensions in Appendix D.\\nMulti-GPU IO-Aware Methods. Ou...</td>\n",
              "      <td>Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n</td>\n",
              "      <td>Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.</td>\n",
              "      <td>FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n5.1 System prompt to enforce guardrails\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified\\nguardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\\nPareto front of model utility / guardrails enforcement, as indicated in Table 4.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\\nunethical, prejudiced, or negative content. Ensure replies promote fairne...</td>\n",
              "      <td>Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n</td>\n",
              "      <td>Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).</td>\n",
              "      <td>Mistral7B.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023Mistral 7B is rele...</td>\n",
              "      <td>Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n</td>\n",
              "      <td>Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.</td>\n",
              "      <td>Mistral7B.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix...</td>\n",
              "      <td>Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K1...</td>\n",
              "      <td>(your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashA...</td>\n",
              "      <td>FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\\nunit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\\n3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\\nbetter than as reported in the original comparison [80].\\n8Attention Memory Usage\\nSequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\\nSequence LengthRuntime (ms)\\nMemory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\\n1020\\nFlashAttention\\nBlock-Sparse FlashAttentionPyTorch Attention\\nMegatron AttentionLinformer Attention\\nOpenAI Sparse Attention8192100Crossover Points\\n20x2xFigure 3: Left:runtime of forward pass + backward pass. Righ...</td>\n",
              "      <td>Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n</td>\n",
              "      <td>The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.</td>\n",
              "      <td>FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\\non Operating Systems Principles , 2023.\\n[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\\nxformers: A modular and hackable transformer modelling library. https://github.com/\\nfacebookresearch/xformers , 2022.\\n[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,\\n2018.\\n[20] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\\nYossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al. Co...</td>\n",
              "      <td>How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n</td>\n",
              "      <td>Paged attention can improve the performance of a gauge model serving in a production environment by reducing the memory footprint and speeding up the inference process.</td>\n",
              "      <td>Mistral7B.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>our chunk size. For each chunk, we thus need to compute the attention over the cache and over the\\nchunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\\ngodog0000100000thetoThecatsatonthe\\n1matand111sawthe1000doggoto\\n100000110000000011100000011110PastCacheCurrent\\nFigure 3: Pre-fill and chunking. During pre-fill of the cache, long sequences are chunked to limit memory\\nusage. We process a sequence in three chunks, ‚ÄúThe cat sat on‚Äù, ‚Äúthe mat and saw‚Äù, ‚Äúthe dog go to‚Äù. The figure\\nshows what happens for the third chunk (‚Äúthe dog go to‚Äù): it attends itself using a causal mask (rightmost block),\\nattends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\\nthe sliding window (left block).\\n3 Results\\nW...</td>\n",
              "      <td>What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n</td>\n",
              "      <td>The purpose of chunking in the context of the pre-fill process in Mistral 7B is to limit memory usage by processing long sequences in smaller, more manageable chunks.</td>\n",
              "      <td>Mistral7B.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7‚Äìbillion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of a...</td>\n",
              "      <td>Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\\n\\n</td>\n",
              "      <td>The grouped-query attention (GQA) mechanism in Mistral 7B groups multiple queries together and processes them simultaneously, reducing the inference cost. The sliding window attention (SWA) mechanism enables the model to effectively handle sequences of arbitrary length by processing the sequence in smaller windows, further reducing the inference cost. These mechanisms, combined with other optimizations, enable Mistral 7B to outperform other language models in terms of performance and efficiency.</td>\n",
              "      <td>Mistral7B.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>We apply two established techniques (tiling, recomputation) to overcome the technical challenge of\\ncomputing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea\\nis that we split the inputs Q¬ñK¬ñVinto blocks, load them from slow HBM to fast SRAM, then compute the\\nattention output with respect to those blocks. By scaling the output of each block by the right normalization\\nfactor before adding them up, we get the correct result at the end.\\nTiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large\\nsoftmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector ùë•2Rùêµis computed as:\\nùëö¬πùë•¬∫:=max\\nùëñùë•ùëñ¬ñ ùëì¬πùë•¬∫:=\u0002\\nùëíùë•1\u0000ùëö¬πùë•¬∫¬ï¬ï¬ï ùëíùë•ùêµ\u0000ùëö¬πùë•¬∫\u0003\\n¬ñ ‚Ñì¬πùë•¬∫:=‚àëÔ∏Å\\nùëñùëì¬πùë•¬∫ùëñ¬ñsoftmax¬πùë•¬∫:=ùëì¬πùë•¬∫\\n‚Ñì¬πùë•¬∫¬ï\\n4For vectors ùë•¬π1¬∫...</td>\n",
              "      <td>Can you explain how the attention output is computed in Algorithm 1?\\n\\n</td>\n",
              "      <td>The attention output is computed by splitting the inputs Q-K-V into blocks, loading them from slow HBM to fast SRAM, and then computing the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, the correct result is obtained at the end. The softmax is decomposed using scaling for numerical stability. For vectors ùë•¬π1¬π-ùë•¬π2¬≤Rùêµ, the softmax of the concatenated ùë•=ùë•¬π1¬πùë•¬π2¬≤Rùêµ is decomposed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas and ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas. ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas is computed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì...</td>\n",
              "      <td>FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B ‚Äì Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\naffordable, efficient, and high-performing language models that can be used in a wide range of\\nreal-world applications.\\n2 Architectural details\\nFigure 1: Sliding Window Attention. The number of o...</td>\n",
              "      <td>Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n</td>\n",
              "      <td>The Mistral 7B language model is designed to balance high performance and efficiency by using a sliding window attention mechanism. This mechanism allows the model to attend to a smaller portion of the input sequence at a time, reducing the number of operations and memory usage required for inference. Additionally, the model is crafted for ease of fine-tuning across a wide range of tasks, making it adaptable and efficient for various real-world applications.</td>\n",
              "      <td>Mistral7B.pdf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           context  \\\n",
              "0  algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort.\\nImplementations may also not be transferrable across GPU architectures. These limitations suggest the\\nneed for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\\ncompiling to IO-aware implementations in CUDA‚Äîsimilar to eÔ¨Äorts such as Halide in image processing [ 70].\\nIO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\\nAttention is the most memory-intensive computation in Transformers, but every layer in a deep network\\ntouches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\\nthese potential extensions in Appendix D.\\nMulti-GPU IO-Aware Methods. Ou...   \n",
              "1  cations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n5.1 System prompt to enforce guardrails\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified\\nguardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\\nPareto front of model utility / guardrails enforcement, as indicated in Table 4.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\\nunethical, prejudiced, or negative content. Ensure replies promote fairne...   \n",
              "2  generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023Mistral 7B is rele...   \n",
              "3  ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix...   \n",
              "4  improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\\nunit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\\n3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\\nbetter than as reported in the original comparison [80].\\n8Attention Memory Usage\\nSequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\\nSequence LengthRuntime (ms)\\nMemory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\\n1020\\nFlashAttention\\nBlock-Sparse FlashAttentionPyTorch Attention\\nMegatron AttentionLinformer Attention\\nOpenAI Sparse Attention8192100Crossover Points\\n20x2xFigure 3: Left:runtime of forward pass + backward pass. Righ...   \n",
              "5  guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\\non Operating Systems Principles , 2023.\\n[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\\nxformers: A modular and hackable transformer modelling library. https://github.com/\\nfacebookresearch/xformers , 2022.\\n[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,\\n2018.\\n[20] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\\nYossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al. Co...   \n",
              "6  our chunk size. For each chunk, we thus need to compute the attention over the cache and over the\\nchunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\\ngodog0000100000thetoThecatsatonthe\\n1matand111sawthe1000doggoto\\n100000110000000011100000011110PastCacheCurrent\\nFigure 3: Pre-fill and chunking. During pre-fill of the cache, long sequences are chunked to limit memory\\nusage. We process a sequence in three chunks, ‚ÄúThe cat sat on‚Äù, ‚Äúthe mat and saw‚Äù, ‚Äúthe dog go to‚Äù. The figure\\nshows what happens for the third chunk (‚Äúthe dog go to‚Äù): it attends itself using a causal mask (rightmost block),\\nattends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\\nthe sliding window (left block).\\n3 Results\\nW...   \n",
              "7  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7‚Äìbillion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of a...   \n",
              "8  We apply two established techniques (tiling, recomputation) to overcome the technical challenge of\\ncomputing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea\\nis that we split the inputs Q¬ñK¬ñVinto blocks, load them from slow HBM to fast SRAM, then compute the\\nattention output with respect to those blocks. By scaling the output of each block by the right normalization\\nfactor before adding them up, we get the correct result at the end.\\nTiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large\\nsoftmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector ùë•2Rùêµis computed as:\\nùëö¬πùë•¬∫:=max\\nùëñùë•ùëñ¬ñ ùëì¬πùë•¬∫:=\u0002\\nùëíùë•1\u0000ùëö¬πùë•¬∫¬ï¬ï¬ï ùëíùë•ùêµ\u0000ùëö¬πùë•¬∫\u0003\\n¬ñ ‚Ñì¬πùë•¬∫:=‚àëÔ∏Å\\nùëñùëì¬πùë•¬∫ùëñ¬ñsoftmax¬πùë•¬∫:=ùëì¬πùë•¬∫\\n‚Ñì¬πùë•¬∫¬ï\\n4For vectors ùë•¬π1¬∫...   \n",
              "9  or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B ‚Äì Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\naffordable, efficient, and high-performing language models that can be used in a wide range of\\nreal-world applications.\\n2 Architectural details\\nFigure 1: Sliding Window Attention. The number of o...   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          question  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n   \n",
              "3  Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K1...   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\\n\\n   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Can you explain how the attention output is computed in Algorithm 1?\\n\\n   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                        Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).   \n",
              "2                                                                                                                                                                                                                                               Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.   \n",
              "3  (your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashA...   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Paged attention can improve the performance of a gauge model serving in a production environment by reducing the memory footprint and speeding up the inference process.   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The purpose of chunking in the context of the pre-fill process in Mistral 7B is to limit memory usage by processing long sequences in smaller, more manageable chunks.   \n",
              "7                                                                                                                                                                                                                                                                                                             The grouped-query attention (GQA) mechanism in Mistral 7B groups multiple queries together and processes them simultaneously, reducing the inference cost. The sliding window attention (SWA) mechanism enables the model to effectively handle sequences of arbitrary length by processing the sequence in smaller windows, further reducing the inference cost. These mechanisms, combined with other optimizations, enable Mistral 7B to outperform other language models in terms of performance and efficiency.   \n",
              "8  The attention output is computed by splitting the inputs Q-K-V into blocks, loading them from slow HBM to fast SRAM, and then computing the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, the correct result is obtained at the end. The softmax is decomposed using scaling for numerical stability. For vectors ùë•¬π1¬π-ùë•¬π2¬≤Rùêµ, the softmax of the concatenated ùë•=ùë•¬π1¬πùë•¬π2¬≤Rùêµ is decomposed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas and ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas. ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas is computed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì...   \n",
              "9                                                                                                                                                                                                                                                                                                                                                   The Mistral 7B language model is designed to balance high performance and efficiency by using a sliding window attention mechanism. This mechanism allows the model to attend to a smaller portion of the input sequence at a time, reducing the number of operations and memory usage required for inference. Additionally, the model is crafted for ease of fine-tuning across a wide range of tasks, making it adaptable and efficient for various real-world applications.   \n",
              "\n",
              "                                                                         source_doc  \n",
              "0  FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf  \n",
              "1                                                                     Mistral7B.pdf  \n",
              "2                                                                     Mistral7B.pdf  \n",
              "3  FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf  \n",
              "4  FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf  \n",
              "5                                                                     Mistral7B.pdf  \n",
              "6                                                                     Mistral7B.pdf  \n",
              "7                                                                     Mistral7B.pdf  \n",
              "8  FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf  \n",
              "9                                                                     Mistral7B.pdf  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',800)\n",
        "display(pd.DataFrame(outputs).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KG4dNtg9jVN"
      },
      "source": [
        "### 1.3. Setup critique agents\n",
        "\n",
        "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
        "\n",
        "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
        "- **Groundedness:** can the question be answered from the given context?\n",
        "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practicioners.\n",
        "\n",
        "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
        "We also build a critique agent for this criteria:\n",
        "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
        "\n",
        "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
        "\n",
        "üí° ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
        "\n",
        "We now build and run these critique agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05aSgTGs9jVO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "### Semi-working backup\n",
        "# question_groundedness_critique_prompt = \"\"\"\n",
        "# You will be given a context and a question.\n",
        "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "# Now here are the question and context.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Context: {context}\\n\n",
        "# Answer::: \"\"\"\n",
        "\n",
        "# question_relevance_critique_prompt = \"\"\"\n",
        "# You will be given a question.\n",
        "# Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "# Now here is the question.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Answer::: \"\"\"\n",
        "\n",
        "# question_standalone_critique_prompt = \"\"\"\n",
        "# You will be given a question.\n",
        "# Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "# The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "# Now here is the question.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "## Semi-working backup\n",
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ## Semi-working backup\n",
        "# question_groundedness_critique_prompt = \"\"\"\n",
        "# You will be given a context and a question.\n",
        "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be preceded and followd by '!!!' as in 'Total rating: !!!4.5!!!'\n",
        "\n",
        "# Now here are the question and context.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Context: {context}\\n\n",
        "# Answer::: \"\"\"\n",
        "\n",
        "# question_relevance_critique_prompt = \"\"\"\n",
        "# You will be given a question.\n",
        "# Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be preceded and followd by '!!!' as in 'Total rating: !!!4.5!!!'\n",
        "\n",
        "# Now here is the question.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Answer::: \"\"\"\n",
        "\n",
        "# question_standalone_critique_prompt = \"\"\"\n",
        "# You will be given a question.\n",
        "# Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "# The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be preceded and followd by '!!!' as in 'Total rating: !!!4.5!!!'\n",
        "\n",
        "# Now here is the question.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b9tbk7ME9jVO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating critique for each QA couple...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
            "\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\n",
            "\n",
            "\n",
            "\n",
            "Context: algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort.\n",
            "Implementations may also not be transferrable across GPU architectures. These limitations suggest the\n",
            "need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\n",
            "compiling to IO-aware implementations in CUDA‚Äîsimilar to eÔ¨Äorts such as Halide in image processing [ 70].\n",
            "IO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\n",
            "Attention is the most memory-intensive computation in Transformers, but every layer in a deep network\n",
            "touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\n",
            "these potential extensions in Appendix D.\n",
            "Multi-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-\n",
            "stants for computing attention on a single GPU. However, the attention computation may be parallelizable\n",
            "\n",
            "Answer:::  [/INST] Evaluation: This question requires knowledge of both PyTorch and IO-aware implementations in CUDA, as well as an understanding of attention algorithms and their limitations. The context provides relevant information about the need for high-level languages like PyTorch to support IO-aware implementations in CUDA, as well as the potential benefits of this approach.\n",
            "\n",
            "Total rating: 4.5\n",
            "\n",
            "Answer::: \n",
            "\n",
            "Evaluation: This question requires knowledge of both PyTorch and IO-aware implementations in CUDA, as well as an understanding of attention algorithms and their limitations. The context provides relevant information about the need for high-level languages like PyTorch to support IO-aware implementations in CUDA, as well as the potential benefits of this approach.\n",
            "\n",
            "Total rating: 4.5\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
            "\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: The question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the possibility of using a high-level language like PyTorch to write attention algorithms with IO-aware implementations in CUDA. This is an important consideration for developers who want to optimize their code for performance and efficiency.\n",
            "\n",
            "Total rating: 4.0.\n",
            "\n",
            "The question is useful for developers who want to use PyTorch to write attention algorithms with IO-aware implementations in CUDA. However, it is not a comprehensive question that covers all aspects of using PyTorch with CUDA. Developers may need to do additional research to fully understand how to optimize their code for performance and efficiency.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
            "\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: \n",
            "The question is clear and does not require any additional information to be understood. It refers to the use of PyTorch, a high-level language, to write attention algorithms with IO-aware implementations in CUDA, which is a well-known programming language and framework for GPU computing. The question does not rely on any specific technical jargon or acronyms, making it context-independent.\n",
            "\n",
            "Total rating: 5\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import re\n",
        "# print(\"Generating critique for each QA couple...\")\n",
        "# for output in tqdm(outputs[:1]):\n",
        "#     evaluations = {\n",
        "#         \"groundedness\": call_llm(question=question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), \n",
        "#                                 generator=generator_llm,\n",
        "#                                 tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "#                                 max_new_tokens=1024),\n",
        "#         \"relevance\": call_llm(question=question_relevance_critique_prompt.format(question=output[\"question\"]), \n",
        "#                                 generator=generator_llm,\n",
        "#                                 tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "#                                 max_new_tokens=1024),\n",
        "                    \n",
        "#         \"standalone\": call_llm(question=question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "#                                 generator=generator_llm,\n",
        "#                                 tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "#                                 max_new_tokens=1024)\n",
        "#     }\n",
        "#     try:\n",
        "#         # for criterion, evaluation in evaluations.items():\n",
        "#         #     # score, eval = (\n",
        "#         #     #     (evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "#         #     #     evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "#         #     # )\n",
        "#         #     score, eval = (\n",
        "#         #         float(evaluation.split(\"!!!\")[1]),\n",
        "#         #         evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "#         #     )\n",
        "#         #     output.update(\n",
        "#         #         {\n",
        "#         #             f\"{criterion}_score\": score,\n",
        "#         #             f\"{criterion}_eval\": eval,\n",
        "#         #         }\n",
        "#         #     )\n",
        "#         for criterion, evaluation in evaluations.items():\n",
        "#             score_match = re.search(r'Total rating: !!!([\\d\\.]+)!!!', evaluation)\n",
        "#             #re.search(r'Total rating: !!!([\\d\\.]+)!!!.*', evaluation)\n",
        "#             eval_match = re.search(r'Evaluation: (.*?)Total rating:', evaluation)\n",
        "            \n",
        "#             if score_match and eval_match:\n",
        "#                 score = float(score_match.group(1))\n",
        "#                 eval = eval_match.group(1).strip()\n",
        "#                 output.update(\n",
        "#                     {\n",
        "#                         f\"{criterion}_score\": score,\n",
        "#                         f\"{criterion}_eval\": eval,\n",
        "#                     }\n",
        "#                 )\n",
        "#     except Exception as e:\n",
        "#         print(\"\\033[91m\" + f\"EVALUATION:\" + \"\\033[0m\")\n",
        "#         print(evaluations)\n",
        "#         print(\"\\033[91m\" + f\"EXCEPTION: {e}\" + \"\\033[0m\")\n",
        "#         break\n",
        "#         #continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating critique for each QA couple...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\n",
            "\n",
            "\n",
            "\n",
            "Context: algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort.\n",
            "Implementations may also not be transferrable across GPU architectures. These limitations suggest the\n",
            "need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\n",
            "compiling to IO-aware implementations in CUDA‚Äîsimilar to eÔ¨Äorts such as Halide in image processing [ 70].\n",
            "IO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\n",
            "Attention is the most memory-intensive computation in Transformers, but every layer in a deep network\n",
            "touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\n",
            "these potential extensions in Appendix D.\n",
            "Multi-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-\n",
            "stants for computing attention on a single GPU. However, the attention computation may be parallelizable\n",
            "\n",
            "Answer:::  [/INST] Evaluation: This question is asking about the possibility of using PyTorch to write attention algorithms with IO-aware implementations in CUDA. The context provides some background information on the limitations of writing attention algorithms in lower-level languages and the need for a high-level language approach. It also discusses the benefits of IO-aware implementations and their potential extensions to other modules. The question is specific to PyTorch and CUDA, and the context provides relevant information to answer the question.\n",
            "\n",
            "Total rating: 4.5\n",
            "\n",
            "The answer is almost unambiguous, but there is a slight ambiguity in the question regarding the specific requirements for the IO-aware implementation. However, the context provides enough information to clarify that the IO-aware implementation should be optimal for computing attention on a single GPU and potentially parallelizable for multi-GPU scenarios.\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: The question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the possibility of using a high-level language such as PyTorch to write attention algorithms with IO-aware implementations in CUDA. This is an important consideration for developers who want to optimize their NLP models for efficient execution on GPUs.\n",
            "\n",
            "Total rating: 4. This question is highly useful for developers who need to understand the capabilities of PyTorch and CUDA for implementing attention algorithms in NLP applications.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|‚ñà         | 1/10 [00:03<00:30,  3.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: \n",
            "The question is context-independent and does not require any additional information to be understood. It refers to the technical capabilities of PyTorch in implementing attention algorithms with IO-aware implementations in CUDA.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\n",
            "\n",
            "\n",
            "\n",
            "Context: cations. In this section, we highlight how to leverage system prompting to optionally enforce output\n",
            "constraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\n",
            "fine-grained content moderation, which can be useful to enforce quality content in applications.\n",
            "5.1 System prompt to enforce guardrails\n",
            "We introduce a system prompt (see below) to guide the model to generate answers within specified\n",
            "guardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\n",
            "Pareto front of model utility / guardrails enforcement, as indicated in Table 4.\n",
            "Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\n",
            "unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
            "Guardrails MT Bench\n",
            "No system prompt 6.84 ¬±0.07\n",
            "Llama 2 system prompt 6.38 ¬±0.07\n",
            "Mistral system prompt 6.58 ¬±0.05\n",
            "Table 4: System prompts. Mean official\n",
            "MT Bench score over 10 iterations with\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "Based on the provided context, it seems that the question is asking about the ability of Mistral 7B to perform fine-grained content moderation using a system prompt to enforce guardrails. The context mentions that Mistral 7B can perform fine-grained content moderation and provides examples of how a system prompt can be used to guide the model to generate answers within specified guardrails. The context also includes a table showing the performance of Mistral 7B with and without a system prompt.\n",
            "\n",
            "Total rating: 4\n",
            "\n",
            "The question is clearly and unambiguously answerable with the provided context. However, the answer may not be fully comprehensive as it only refers to the ability of Mistral 7B to perform fine-grained content moderation using a system prompt to enforce guardrails. It does not provide any additional information about the specific use cases or applications where this feature may be useful.\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: The question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the capabilities of a specific NLP model, Mistral 7B, in performing a specific task, fine-grained content moderation. This information can help developers make informed decisions about which models to use for their specific tasks, and can also provide insights into the potential limitations and strengths of different models.\n",
            "\n",
            "Total rating: 4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|‚ñà‚ñà        | 2/10 [00:07<00:28,  3.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "This question is context-independent because it does not refer to a specific setting or document. It asks about the capabilities of a specific model (Mistral 7B) and its ability to perform a specific task (fine-grained content moderation using a system prompt to enforce guardrails). The question does not require any additional information to be understood.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "\u001b[91mEVALUATION:\u001b[0m\n",
            "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n\\n\\nContext: cations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n5.1 System prompt to enforce guardrails\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified\\nguardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\\nPareto front of model utility / guardrails enforcement, as indicated in Table 4.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\\nunethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\nGuardrails MT Bench\\nNo system prompt 6.84 ¬±0.07\\nLlama 2 system prompt 6.38 ¬±0.07\\nMistral system prompt 6.58 ¬±0.05\\nTable 4: System prompts. Mean official\\nMT Bench score over 10 iterations with\\n\\nAnswer:::  [/INST] Evaluation:\\nBased on the provided context, it seems that the question is asking about the ability of Mistral 7B to perform fine-grained content moderation using a system prompt to enforce guardrails. The context mentions that Mistral 7B can perform fine-grained content moderation and provides examples of how a system prompt can be used to guide the model to generate answers within specified guardrails. The context also includes a table showing the performance of Mistral 7B with and without a system prompt.\\n\\nTotal rating: 4\\n\\nThe question is clearly and unambiguously answerable with the provided context. However, the answer may not be fully comprehensive as it only refers to the ability of Mistral 7B to perform fine-grained content moderation using a system prompt to enforce guardrails. It does not provide any additional information about the specific use cases or applications where this feature may be useful.\", 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: The question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the capabilities of a specific NLP model, Mistral 7B, in performing a specific task, fine-grained content moderation. This information can help developers make informed decisions about which models to use for their specific tasks, and can also provide insights into the potential limitations and strengths of different models.\\n\\nTotal rating: 4\", 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independant this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation:\\nThis question is context-independent because it does not refer to a specific setting or document. It asks about the capabilities of a specific model (Mistral 7B) and its ability to perform a specific task (fine-grained content moderation using a system prompt to enforce guardrails). The question does not require any additional information to be understood.\\n\\nTotal rating: 5'}\n",
            "\u001b[91mEXCEPTION: list index out of range\u001b[0m\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\n",
            "\n",
            "\n",
            "\n",
            "Context: generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\n",
            "without sacrificing performance on non-code related benchmarks.\n",
            "Mistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\n",
            "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
            "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
            "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
            "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
            "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
            "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
            "\n",
            "Answer:::  [/INST] Evaluation: Mistral 7B is a large language model that uses grouped-query attention (GQA) and sliding window attention (SWA) as attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.\n",
            "\n",
            "Total rating: 4.5\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: The given question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the attention mechanisms used by Mistral 7B and how they contribute to its enhanced performance and efficiency. Attention mechanisms are an important aspect of NLP models, and understanding how they work can help developers improve the performance of their own models. Additionally, Mistral 7B is a state-of-the-art NLP model that has achieved impressive results on various benchmarks, so exploring its attention mechanisms can provide valuable insights into best practices for building high-performing NLP models.\n",
            "\n",
            "Total rating: 4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|‚ñà‚ñà‚ñà       | 3/10 [00:10<00:23,  3.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: \n",
            "The question is asking for information about the attention mechanisms used by Mistral 7B and how they contribute to its performance and efficiency. It is not explicitly stated what context is being referred to, so it is likely that the question is context-independent. \n",
            "\n",
            "Total rating: 4\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22\n",
            "\n",
            "Context: ùëÅ\n",
            "ùêµùëêm\n",
            "blocks\n",
            "K1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\n",
            "4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\n",
            "divideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\n",
            "5:for1\u0014ùëó\u0014ùëáùëêdo\n",
            "6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\n",
            "7:for1\u0014ùëñ\u0014ùëáùëüdo\n",
            "8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\n",
            "9:On chip, compute Sùëñùëó=QùëñKùëá\n",
            "ùëó2Rùêµùëü\u0002ùêµùëê.\n",
            "10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\n",
            "rowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\n",
            "11:On chip, compute ùëönew\n",
            "ùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\n",
            "ùëñ=ùëíùëöùëñ\u0000ùëönew\n",
            "ùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\n",
            "ùëñ~‚Ñìùëñùëó2Rùêµùëü.\n",
            "12:Write Oùëñ diag¬π‚Ñìnew\n",
            "ùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\n",
            "ùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\n",
            "ùëñ~PùëñùëóVùëó¬∫to HBM.\n",
            "13:Write‚Ñìùëñ ‚Ñìnew\n",
            "ùëñ,ùëöùëñ ùëönew\n",
            "ùëñto HBM.\n",
            "14:end for\n",
            "15:end for\n",
            "16:Return O.\n",
            "We show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\n",
            "Theorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\n",
            "memory beyond inputs and output.\n",
            "3.2 Analysis: IO Complexity of FlashAttention\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "This question is asking for the total rating of a given algorithm, specifically FlashAttention, on a scale of 1 to 5. The total rating is based on the correctness, runtime, and memory requirement of the algorithm.\n",
            "\n",
            "Total rating: 4\n",
            "\n",
            "The given algorithm, FlashAttention, is designed to solve a specific problem, which is to find the most important information in a set of blocks. The algorithm uses a softmax function to compute the probability of each block being important, and then returns the top block with the highest probability. The algorithm also uses a memory-efficient implementation, which reduces the memory requirement beyond inputs and output.\n",
            "\n",
            "However, the algorithm does not provide the runtime information, which is a crucial factor in determining the total rating. Therefore, we cannot provide a complete evaluation of the algorithm without the runtime information.\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22\n",
            "\n",
            "Answer:::  [/INST] Evaluation: This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it provides a comprehensive list of pre-trained models available on the Hugging Face Hub. The list includes models for various NLP tasks such as text classification, sentiment analysis, language translation, named entity recognition, and more. The question also includes information on the models' performance metrics, making it easier for developers to choose the right model for their specific use case.\n",
            "\n",
            "Total rating: 4.5\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:14<00:21,  3.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "\n",
            "The question is about a list of numbers and does not contain any contextual information. It is a simple request for a specific subset of the list.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "\u001b[91mEVALUATION:\u001b[0m\n",
            "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1\\x96\\x95\\x95\\x95\\x96KùëáùëêandV1\\x96\\x95\\x95\\x95\\x96Vùëáùëê, of sizeùêµùëê\\x02ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ\\x96\\x95\\x95\\x95\\x96Oùëáùëüof sizeùêµùëü\\x02ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ\\x96\\x95\\x95\\x95\\x96‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1\\x96\\x95\\x95\\x95\\x96ùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\\x14ùëó\\x14ùëáùëêdo\\n6:Load Kùëó\\x96Vùëófrom HBM to on-chip SRAM.\\n7:for1\\x14ùëñ\\x14ùëáùëüdo\\n8:Load Qùëñ\\x96Oùëñ\\x96‚Ñìùëñ\\x96ùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\\x02ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\\x00~ùëöùëñùëó¬∫ 2Rùêµùëü\\x02ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ\\x96~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\\x00ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\\x00ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\\x001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\\x00ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\\x00ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention\\n\\nAnswer:::  [/INST] Evaluation:\\nThis question is asking for the total rating of a given algorithm, specifically FlashAttention, on a scale of 1 to 5. The total rating is based on the correctness, runtime, and memory requirement of the algorithm.\\n\\nTotal rating: 4\\n\\nThe given algorithm, FlashAttention, is designed to solve a specific problem, which is to find the most important information in a set of blocks. The algorithm uses a softmax function to compute the probability of each block being important, and then returns the top block with the highest probability. The algorithm also uses a memory-efficient implementation, which reduces the memory requirement beyond inputs and output.\\n\\nHowever, the algorithm does not provide the runtime information, which is a crucial factor in determining the total rating. Therefore, we cannot provide a complete evaluation of the algorithm without the runtime information.\", 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22\\n\\nAnswer:::  [/INST] Evaluation: This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it provides a comprehensive list of pre-trained models available on the Hugging Face Hub. The list includes models for various NLP tasks such as text classification, sentiment analysis, language translation, named entity recognition, and more. The question also includes information on the models' performance metrics, making it easier for developers to choose the right model for their specific use case.\\n\\nTotal rating: 4.5\", 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independant this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22\\n\\nAnswer:::  [/INST] Evaluation:\\n\\nThe question is about a list of numbers and does not contain any contextual information. It is a simple request for a specific subset of the list.\\n\\nTotal rating: 5'}\n",
            "\u001b[91mEXCEPTION: list index out of range\u001b[0m\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\n",
            "\n",
            "\n",
            "\n",
            "Context: improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\n",
            "unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\n",
            "3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\n",
            "better than as reported in the original comparison [80].\n",
            "8Attention Memory Usage\n",
            "Sequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\n",
            "Sequence LengthRuntime (ms)\n",
            "Memory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\n",
            "1020\n",
            "FlashAttention\n",
            "Block-Sparse FlashAttentionPyTorch Attention\n",
            "Megatron AttentionLinformer Attention\n",
            "OpenAI Sparse Attention8192100Crossover Points\n",
            "20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.\n",
            "European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "Total rating: 4\n",
            "\n",
            "The question is partially answerable with the given context. The context provides information about the relationship between attention memory usage and sequence length in the MIMIC-III and ECtHR datasets, but it does not provide a clear and unambiguous answer to the question. The context also discusses the performance of attention memory usage on these datasets, but it does not provide a direct comparison between the attention memory usage and sequence length. Therefore, the total rating is 4 out of 5.\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: The given question is asking about the relationship between attention memory usage and sequence length in two NLP datasets, MIMIC-III and ECtHR. This question is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem because understanding the relationship between attention memory usage and sequence length can help optimize the performance of NLP models. Attention memory usage refers to the amount of memory used by the attention mechanism in a language model, while sequence length refers to the length of the input sequence. By analyzing the relationship between these two factors, developers can identify potential bottlenecks in their models and optimize them for better performance.\n",
            "\n",
            "Total rating: 4.\n",
            "\n",
            "The question is extremely useful because it provides a specific context and asks for a detailed analysis of the relationship between attention memory usage and sequence length in two relevant NLP datasets. This information can be very valuable to machine learning developers building NLP applications with the Hugging Face ecosystem to optimize their models for better performance.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:17<00:18,  3.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: \n",
            "The question is context-dependent since it refers to two specific datasets, MIMIC-III and ECtHR. The relationship between attention memory usage and sequence length may vary between these datasets, and understanding this relationship requires knowledge of the specific characteristics of each dataset.\n",
            "\n",
            "Total rating: 2\n",
            "\n",
            "\u001b[91mEVALUATION:\u001b[0m\n",
            "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n\\n\\nContext: improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\\nunit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\\n3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\\nbetter than as reported in the original comparison [80].\\n8Attention Memory Usage\\nSequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\\nSequence LengthRuntime (ms)\\nMemory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\\n1020\\nFlashAttention\\nBlock-Sparse FlashAttentionPyTorch Attention\\nMegatron AttentionLinformer Attention\\nOpenAI Sparse Attention8192100Crossover Points\\n20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.\\nEuropean Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights\\n\\nAnswer:::  [/INST] Evaluation:\\nTotal rating: 4\\n\\nThe question is partially answerable with the given context. The context provides information about the relationship between attention memory usage and sequence length in the MIMIC-III and ECtHR datasets, but it does not provide a clear and unambiguous answer to the question. The context also discusses the performance of attention memory usage on these datasets, but it does not provide a direct comparison between the attention memory usage and sequence length. Therefore, the total rating is 4 out of 5.\", 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: The given question is asking about the relationship between attention memory usage and sequence length in two NLP datasets, MIMIC-III and ECtHR. This question is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem because understanding the relationship between attention memory usage and sequence length can help optimize the performance of NLP models. Attention memory usage refers to the amount of memory used by the attention mechanism in a language model, while sequence length refers to the length of the input sequence. By analyzing the relationship between these two factors, developers can identify potential bottlenecks in their models and optimize them for better performance.\\n\\nTotal rating: 4.\\n\\nThe question is extremely useful because it provides a specific context and asks for a detailed analysis of the relationship between attention memory usage and sequence length in two relevant NLP datasets. This information can be very valuable to machine learning developers building NLP applications with the Hugging Face ecosystem to optimize their models for better performance.\", 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independant this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: \\nThe question is context-dependent since it refers to two specific datasets, MIMIC-III and ECtHR. The relationship between attention memory usage and sequence length may vary between these datasets, and understanding this relationship requires knowledge of the specific characteristics of each dataset.\\n\\nTotal rating: 2'}\n",
            "\u001b[91mEXCEPTION: list index out of range\u001b[0m\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: How does paged attention affect the performance of a gauge model serving in a production environment?\n",
            "\n",
            "\n",
            "\n",
            "Context: guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\n",
            "on Operating Systems Principles , 2023.\n",
            "[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\n",
            "Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\n",
            "xformers: A modular and hackable transformer modelling library. https://github.com/\n",
            "facebookresearch/xformers , 2022.\n",
            "[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\n",
            "electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,\n",
            "2018.\n",
            "[20] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\n",
            "Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al. Code llama: Open foundation models\n",
            "for code. arXiv preprint arXiv:2308.12950 , 2023.\n",
            "[21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\n",
            "\n",
            "Answer:::  [/INST] Evaluation: Paged attention is a technique used in machine learning models to improve the efficiency of processing large amounts of data. In the context of a gauge model serving in a production environment, paged attention can affect the performance of the model by impacting its ability to process data quickly and accurately. The specific impact of paged attention on the performance of a gauge model serving in a production environment may depend on the specific implementation and use case.\n",
            "\n",
            "Total rating: 3.5\n",
            "\n",
            "The question is clearly answerable with the given context, but the specific impact of paged attention on the performance of a gauge model serving in a production environment may depend on the specific implementation and use case, making it a bit less unambiguous.\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: How does paged attention affect the performance of a gauge model serving in a production environment?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "The question asks about the performance of a gauge model serving in a production environment, specifically in relation to paged attention. Paged attention is a technique used in some NLP models to improve their efficiency and scalability by dividing the input sequence into smaller chunks and processing them independently. This can be especially useful in production environments where resources are limited and processing time is critical.\n",
            "\n",
            "However, the specifics of how paged attention affects the performance of a gauge model in a production environment are not well-documented in the literature. There may be trade-offs between increased efficiency and reduced accuracy, or other factors that depend on the specifics of the model and the use case.\n",
            "\n",
            "Overall, this question has the potential to be useful for machine learning developers building NLP applications with the Hugging Face ecosystem, as it addresses an important consideration for deploying models in production environments. However, a more specific and well-defined question would be helpful for gaining a clearer understanding of the topic.\n",
            "\n",
            "Total rating: 3\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:22<00:15,  3.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: How does paged attention affect the performance of a gauge model serving in a production environment?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: This question does not depend on any additional information to be understood. It is clear what a gauge model is and what a production environment is. The only additional information needed is the concept of paged attention, which is likely well-known to someone familiar with the field of deep learning.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "\u001b[91mEVALUATION:\u001b[0m\n",
            "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n\\n\\nContext: guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\\non Operating Systems Principles , 2023.\\n[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\\nxformers: A modular and hackable transformer modelling library. https://github.com/\\nfacebookresearch/xformers , 2022.\\n[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,\\n2018.\\n[20] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\\nYossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al. Code llama: Open foundation models\\nfor code. arXiv preprint arXiv:2308.12950 , 2023.\\n[21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\n\\nAnswer:::  [/INST] Evaluation: Paged attention is a technique used in machine learning models to improve the efficiency of processing large amounts of data. In the context of a gauge model serving in a production environment, paged attention can affect the performance of the model by impacting its ability to process data quickly and accurately. The specific impact of paged attention on the performance of a gauge model serving in a production environment may depend on the specific implementation and use case.\\n\\nTotal rating: 3.5\\n\\nThe question is clearly answerable with the given context, but the specific impact of paged attention on the performance of a gauge model serving in a production environment may depend on the specific implementation and use case, making it a bit less unambiguous.\", 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation:\\nThe question asks about the performance of a gauge model serving in a production environment, specifically in relation to paged attention. Paged attention is a technique used in some NLP models to improve their efficiency and scalability by dividing the input sequence into smaller chunks and processing them independently. This can be especially useful in production environments where resources are limited and processing time is critical.\\n\\nHowever, the specifics of how paged attention affects the performance of a gauge model in a production environment are not well-documented in the literature. There may be trade-offs between increased efficiency and reduced accuracy, or other factors that depend on the specifics of the model and the use case.\\n\\nOverall, this question has the potential to be useful for machine learning developers building NLP applications with the Hugging Face ecosystem, as it addresses an important consideration for deploying models in production environments. However, a more specific and well-defined question would be helpful for gaining a clearer understanding of the topic.\\n\\nTotal rating: 3\", 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independant this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: This question does not depend on any additional information to be understood. It is clear what a gauge model is and what a production environment is. The only additional information needed is the concept of paged attention, which is likely well-known to someone familiar with the field of deep learning.\\n\\nTotal rating: 5'}\n",
            "\u001b[91mEXCEPTION: list index out of range\u001b[0m\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\n",
            "\n",
            "\n",
            "\n",
            "Context: our chunk size. For each chunk, we thus need to compute the attention over the cache and over the\n",
            "chunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\n",
            "godog0000100000thetoThecatsatonthe\n",
            "1matand111sawthe1000doggoto\n",
            "100000110000000011100000011110PastCacheCurrent\n",
            "Figure 3: Pre-fill and chunking. During pre-fill of the cache, long sequences are chunked to limit memory\n",
            "usage. We process a sequence in three chunks, ‚ÄúThe cat sat on‚Äù, ‚Äúthe mat and saw‚Äù, ‚Äúthe dog go to‚Äù. The figure\n",
            "shows what happens for the third chunk (‚Äúthe dog go to‚Äù): it attends itself using a causal mask (rightmost block),\n",
            "attends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\n",
            "the sliding window (left block).\n",
            "3 Results\n",
            "We compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for\n",
            "fair comparison. We measure performance on a wide variety of tasks categorized as follow:\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "\n",
            "The question asks about the purpose of chunking in the context of the pre-fill process in Mistral 7B. The provided context provides a clear explanation of how chunking works in the pre-fill process and its purpose in limiting memory usage. The figure also shows how the attention mask works over both the cache and the chunk, which further clarifies the purpose of chunking.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: \n",
            "Chunking is a technique used in natural language processing (NLP) to break down a sentence into smaller units, called chunks. In the context of the pre-fill process in Mistral 7B, chunking is used to identify and extract relevant information from a given sentence. This information is then used to pre-fill a document, which can help to speed up the writing process.\n",
            "\n",
            "Total rating: 4 \n",
            "\n",
            "The purpose of chunking in the context of the pre-fill process in Mistral 7B is quite clear and specific. It is used to extract relevant information from a given sentence, which can be very useful for pre-filling a document. While the exact implementation and details of the pre-fill process may vary depending on the specific use case, the overall concept of using chunking to extract relevant information is a common and widely understood technique in NLP.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:26<00:11,  3.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "This question refers to the context of the pre-fill process in Mistral 7B, which is a language model developed by Mistral AI. Chunking is a technique used in natural language processing to group similar words and phrases together. In the context of the pre-fill process in Mistral 7B, chunking is used to identify and extract relevant information from the input text.\n",
            "\n",
            "Total rating: 4\n",
            "\n",
            "The question is context-dependent on the knowledge of the pre-fill process in Mistral 7B and the use of chunking in natural language processing. However, it is still clear what the question is asking for and what the context is.\n",
            "\n",
            "\u001b[91mEVALUATION:\u001b[0m\n",
            "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n\\n\\nContext: our chunk size. For each chunk, we thus need to compute the attention over the cache and over the\\nchunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\\ngodog0000100000thetoThecatsatonthe\\n1matand111sawthe1000doggoto\\n100000110000000011100000011110PastCacheCurrent\\nFigure 3: Pre-fill and chunking. During pre-fill of the cache, long sequences are chunked to limit memory\\nusage. We process a sequence in three chunks, ‚ÄúThe cat sat on‚Äù, ‚Äúthe mat and saw‚Äù, ‚Äúthe dog go to‚Äù. The figure\\nshows what happens for the third chunk (‚Äúthe dog go to‚Äù): it attends itself using a causal mask (rightmost block),\\nattends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\\nthe sliding window (left block).\\n3 Results\\nWe compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for\\nfair comparison. We measure performance on a wide variety of tasks categorized as follow:\\n\\nAnswer:::  [/INST] Evaluation:\\n\\nThe question asks about the purpose of chunking in the context of the pre-fill process in Mistral 7B. The provided context provides a clear explanation of how chunking works in the pre-fill process and its purpose in limiting memory usage. The figure also shows how the attention mask works over both the cache and the chunk, which further clarifies the purpose of chunking.\\n\\nTotal rating: 5\", 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: \\nChunking is a technique used in natural language processing (NLP) to break down a sentence into smaller units, called chunks. In the context of the pre-fill process in Mistral 7B, chunking is used to identify and extract relevant information from a given sentence. This information is then used to pre-fill a document, which can help to speed up the writing process.\\n\\nTotal rating: 4 \\n\\nThe purpose of chunking in the context of the pre-fill process in Mistral 7B is quite clear and specific. It is used to extract relevant information from a given sentence, which can be very useful for pre-filling a document. While the exact implementation and details of the pre-fill process may vary depending on the specific use case, the overall concept of using chunking to extract relevant information is a common and widely understood technique in NLP.\", 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independant this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation:\\nThis question refers to the context of the pre-fill process in Mistral 7B, which is a language model developed by Mistral AI. Chunking is a technique used in natural language processing to group similar words and phrases together. In the context of the pre-fill process in Mistral 7B, chunking is used to identify and extract relevant information from the input text.\\n\\nTotal rating: 4\\n\\nThe question is context-dependent on the knowledge of the pre-fill process in Mistral 7B and the use of chunking in natural language processing. However, it is still clear what the question is asking for and what the context is.'}\n",
            "\u001b[91mEXCEPTION: list index out of range\u001b[0m\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\n",
            "\n",
            "\n",
            "\n",
            "Context: Mistral 7B\n",
            "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
            "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
            "Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux,\n",
            "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix,\n",
            "William El Sayed\n",
            "Abstract\n",
            "We introduce Mistral 7B, a 7‚Äìbillion-parameter language model engineered for\n",
            "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
            "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
            "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
            "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
            "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
            "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
            "Mistral 7B ‚Äì Instruct, that surpasses Llama 2 13B ‚Äì chat model both on human and\n",
            "\n",
            "Answer:::  [/INST] Evaluation: Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) mechanisms to enable faster and more efficient inference compared to other language models. GQA allows for multiple queries to be processed simultaneously, reducing the inference cost. SWA enables the model to effectively handle sequences of arbitrary length. Additionally, Mistral 7B is fine-tuned to follow instructions, making it well-suited for tasks such as code generation and reasoning.\n",
            "\n",
            "Total rating: 4.5\n",
            "\n",
            "Answer::: \n",
            "Evaluation: Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) mechanisms to enable faster and more efficient inference compared to other language models. GQA allows for multiple queries to be processed simultaneously, reducing the inference cost. SWA enables the model to effectively handle sequences of arbitrary length. Additionally, Mistral 7B is fine-tuned to follow instructions, making it well-suited for tasks such as code generation and reasoning.\n",
            "\n",
            "Total rating: 4.5\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it provides insight into the performance of the Mistral 7B language model. The question specifically mentions two attention mechanisms, grouped-query attention (GQA) and sliding window attention (SWA), which are used in Mistral 7B to enable faster and more efficient inference. By understanding how these mechanisms work and how they contribute to the overall performance of the model, developers can make informed decisions about which models to use for their specific applications.\n",
            "\n",
            "Total rating: 4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:30<00:07,  3.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: This question is context-independent and can be understood without any additional information. It references specific mechanisms in the Mistral 7B language model and describes their potential benefits in terms of inference speed and efficiency. The question is also clear and concise, making it easy to understand.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Can you explain how the attention output is computed in Algorithm 1?\n",
            "\n",
            "\n",
            "\n",
            "Context: We apply two established techniques (tiling, recomputation) to overcome the technical challenge of\n",
            "computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea\n",
            "is that we split the inputs Q¬ñK¬ñVinto blocks, load them from slow HBM to fast SRAM, then compute the\n",
            "attention output with respect to those blocks. By scaling the output of each block by the right normalization\n",
            "factor before adding them up, we get the correct result at the end.\n",
            "Tiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large\n",
            "softmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector ùë•2Rùêµis computed as:\n",
            "ùëö¬πùë•¬∫:=max\n",
            "ùëñùë•ùëñ¬ñ ùëì¬πùë•¬∫:=\u0002\n",
            "ùëíùë•1\u0000ùëö¬πùë•¬∫¬ï¬ï¬ï ùëíùë•ùêµ\u0000ùëö¬πùë•¬∫\u0003\n",
            "¬ñ ‚Ñì¬πùë•¬∫:=‚àëÔ∏Å\n",
            "ùëñùëì¬πùë•¬∫ùëñ¬ñsoftmax¬πùë•¬∫:=ùëì¬πùë•¬∫\n",
            "‚Ñì¬πùë•¬∫¬ï\n",
            "4For vectors ùë•¬π1¬∫¬ñùë•¬π2¬∫2Rùêµ, we can decompose the softmax of the concatenated ùë•=\u0002\n",
            "ùë•¬π1¬∫ùë•¬π2¬∫\u0003\n",
            "2R2ùêµas:\n",
            "ùëö¬πùë•¬∫=ùëö¬π\u0002\n",
            "ùë•¬π1¬∫ùë•¬π2¬∫\u0003\n",
            "¬∫=max¬πùëö¬πùë•¬π1¬∫¬∫¬ñùëö¬πùë•¬π2¬∫¬∫¬∫¬ñ ùëì¬πùë•¬∫=h\n",
            "ùëíùëö¬πùë•¬π1¬∫¬∫\u0000ùëö¬πùë•¬∫ùëì¬πùë•¬π1¬∫¬∫ùëíùëö¬πùë•¬π2¬∫¬∫\u0000ùëö¬πùë•¬∫ùëì¬πùë•¬π2¬∫¬∫i\n",
            "¬ñ\n",
            "‚Ñì¬πùë•¬∫=‚Ñì¬π\u0002\n",
            "ùë•¬π1¬∫ùë•¬π2¬∫\u0003\n",
            "\n",
            "Answer:::  [/INST] Evaluation: This Algorithm 1 provides a clear explanation of how attention is computed using tiling and recomputation techniques. The softmax computation is also described in detail, including the use of scaling and numerical stability measures. The attention output is computed by scaling and adding up the output of each block.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can you explain how the attention output is computed in Algorithm 1?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: The provided question asks for an explanation of the attention output computation in Algorithm 1, which is a fundamental concept in the Hugging Face ecosystem for NLP applications. This question can be useful for machine learning developers who are interested in understanding the attention mechanism and how it can be applied to NLP tasks.\n",
            "\n",
            "Total rating: 4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:32<00:03,  3.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can you explain how the attention output is computed in Algorithm 1?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: This question is asking for an explanation of a specific algorithmic step, which is the computation of attention output in Algorithm 1. It does not mention any specific context or document, so it is context-independent.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a context and a question.\n",
            "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here are the question and context.\n",
            "\n",
            "Question: Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\n",
            "\n",
            "\n",
            "\n",
            "Context: or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
            "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
            "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
            "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B ‚Äì Chat model.\n",
            "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
            "large language models efficient. Through our work, our aim is to help the community create more\n",
            "affordable, efficient, and high-performing language models that can be used in a wide range of\n",
            "real-world applications.\n",
            "2 Architectural details\n",
            "Figure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\n",
            "length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher\n",
            "\n",
            "Answer:::  [/INST] Evaluation: \n",
            "\n",
            "The question requires an explanation of how the Mistral 7B language model is designed to balance high performance and efficiency. The provided context includes information about the architecture of the model, its adaptability, and its ability to outperform other language models. However, it does not directly address the balance between performance and efficiency.\n",
            "\n",
            "Total rating: 3\n",
            "\n",
            "The context provides some relevant information, but it does not fully answer the question.\n",
            "\n",
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation:\n",
            "Mistral 7B is a large language model developed by Mistral AI and trained on a massive amount of text data. It is designed to balance high performance and efficiency by using a combination of techniques such as quantization, pruning, and model compression. The model is trained on a large corpus of text data, which allows it to learn complex patterns and relationships in language. However, training such a large model can be computationally expensive and time-consuming. To address this, the model is designed to be efficient by using techniques such as quantization, which reduces the precision of the model's weights and activations, and pruning, which removes unnecessary connections between neurons. Additionally, the model is compressed using techniques such as knowledge distillation, which allows the model to learn from a smaller, more efficient model. Overall, the combination of these techniques allows Mistral 7B to achieve high performance while remaining efficient.\n",
            "\n",
            "Total rating: 4.5\n",
            "\n",
            "The question is very useful for machine learning developers building NLP applications with the Hugging Face ecosystem, as it provides insights into the design and implementation of large language models. The answer explains how the Mistral 7B model balances high performance and efficiency by using techniques such as quantization, pruning, and model compression. This information can be useful for developers looking to optimize their own language models for efficiency and performance.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:36<00:00,  3.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] \n",
            "You will be given a question.\n",
            "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
            "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
            "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
            "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
            "\n",
            "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
            "\n",
            "Provide your answer as follows:\n",
            "\n",
            "Answer:::\n",
            "Evaluation: (your rationale for the rating, as a text)\n",
            "Total rating: (your rating, as a number between 1 and 5)\n",
            "\n",
            "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
            "\n",
            "Now here is the question.\n",
            "\n",
            "Question: Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\n",
            "\n",
            "\n",
            "\n",
            "Answer:::  [/INST] Evaluation: The question refers to the design of the Mistral 7B language model, which is a technical topic that can be understood without additional context. The question does not require any specific knowledge or background information beyond what is commonly known about language models.\n",
            "\n",
            "Total rating: 5\n",
            "\n",
            "\u001b[91mEVALUATION:\u001b[0m\n",
            "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n\\n\\nContext: or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B ‚Äì Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\naffordable, efficient, and high-performing language models that can be used in a wide range of\\nreal-world applications.\\n2 Architectural details\\nFigure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\\nlength, and the memory increases linearly with the number of tokens. At inference time, this incurs higher\\n\\nAnswer:::  [/INST] Evaluation: \\n\\nThe question requires an explanation of how the Mistral 7B language model is designed to balance high performance and efficiency. The provided context includes information about the architecture of the model, its adaptability, and its ability to outperform other language models. However, it does not directly address the balance between performance and efficiency.\\n\\nTotal rating: 3\\n\\nThe context provides some relevant information, but it does not fully answer the question.\", 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation:\\nMistral 7B is a large language model developed by Mistral AI and trained on a massive amount of text data. It is designed to balance high performance and efficiency by using a combination of techniques such as quantization, pruning, and model compression. The model is trained on a large corpus of text data, which allows it to learn complex patterns and relationships in language. However, training such a large model can be computationally expensive and time-consuming. To address this, the model is designed to be efficient by using techniques such as quantization, which reduces the precision of the model's weights and activations, and pruning, which removes unnecessary connections between neurons. Additionally, the model is compressed using techniques such as knowledge distillation, which allows the model to learn from a smaller, more efficient model. Overall, the combination of these techniques allows Mistral 7B to achieve high performance while remaining efficient.\\n\\nTotal rating: 4.5\\n\\nThe question is very useful for machine learning developers building NLP applications with the Hugging Face ecosystem, as it provides insights into the design and implementation of large language models. The answer explains how the Mistral 7B model balances high performance and efficiency by using techniques such as quantization, pruning, and model compression. This information can be useful for developers looking to optimize their own language models for efficiency and performance.\", 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independant this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: The question refers to the design of the Mistral 7B language model, which is a technical topic that can be understood without additional context. The question does not require any specific knowledge or background information beyond what is commonly known about language models.\\n\\nTotal rating: 5'}\n",
            "\u001b[91mEXCEPTION: list index out of range\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(question=question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "        \"relevance\": call_llm(question=question_relevance_critique_prompt.format(question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "                    \n",
        "        \"standalone\": call_llm(question=question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    }\n",
        "    try:\n",
        "        for criterion, evaluation in evaluations.items():\n",
        "            score, eval = (\n",
        "                # int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                (evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "            )\n",
        "            output.update(\n",
        "                {\n",
        "                    f\"{criterion}_score\": score,\n",
        "                    f\"{criterion}_eval\": eval,\n",
        "                }\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(\"\\033[91m\" + f\"EVALUATION:\" + \"\\033[0m\")\n",
        "        print(evaluations)\n",
        "        print(\"\\033[91m\" + f\"EXCEPTION: {e}\" + \"\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: !!!(your rating, as a number between 1 and 5)!!!\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\\n\\n\\nNow here are the question and context.\\n\\nQuestion: Given that the LLM models discussed in [26], [27], [28], and [29] are designed to be efficient foundation language models, what are the key differences between them in terms of their architecture, training data, and performance metrics?\\n\\n\\n\\nContext: and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\\n[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems , 30, 2017.\\n[28] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\\n[29] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\\nmodels. arXiv preprint arXiv:2304.06364 , 2023.\\n9\\n\\nAnswer:::  [/INST] Evaluation: These are all preprints from arXiv that discuss LLM models designed to be efficient foundation language models. The key differences between them in terms of architecture, training data, and performance metrics are discussed in detail in each of the preprints.\\n\\nTotal rating: 5\",\n",
              " 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: !!!(your rating, as a number between 1 and 5)!!!\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\\n\\n\\nNow here is the question.\\n\\nQuestion: Given that the LLM models discussed in [26], [27], [28], and [29] are designed to be efficient foundation language models, what are the key differences between them in terms of their architecture, training data, and performance metrics?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: The provided question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem as it provides a clear comparison between several LLM models in terms of their architecture, training data, and performance metrics. This information can help developers make informed decisions when choosing the best model for their specific use case.\\n\\nTotal rating: 4.5\",\n",
              " 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independent this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: !!!(your rating, as a number between 1 and 5)!!!\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.  \\'Total rating:\\' should be encolsed in !!! and !!! as in \\'!!!4.5!!!\\'.\\n\\n\\nNow here is the question.\\n\\nQuestion: Given that the LLM models discussed in [26], [27], [28], and [29] are designed to be efficient foundation language models, what are the key differences between them in terms of their architecture, training data, and performance metrics?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: The question is context-independent and can be understood without any additional information. It provides clear instructions on what the operator needs to do and what the desired output is.\\n\\nTotal rating: 5'}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQv36Y_f9jVO"
      },
      "source": [
        "Now let us filter out bad questions based on our critique agent scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['context', 'question', 'answer', 'source_doc', 'groundedness_score', 'groundedness_eval', 'relevance_score', 'relevance_eval', 'standalone_score', 'standalone_eval'])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "If using all scalar values, you must pass an index",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m generated_questions \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation dataset before filtering:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m display(\n\u001b[1;32m      9\u001b[0m     generated_questions\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[1;32m     10\u001b[0m         [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \n\u001b[1;32m     19\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.12/site-packages/pandas/core/frame.py:1905\u001b[0m, in \u001b[0;36mDataFrame.from_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1900\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for orient parameter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1901\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morient\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1902\u001b[0m     )\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1907\u001b[0m     realdata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.12/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
            "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.12/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
            "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(output)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions.reindex(\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ],\n",
        "        axis=1)\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation dataset before filtering:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n</td>\n",
              "      <td>Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.</td>\n",
              "      <td>4.5\\n\\nThe answer is almost unambiguous, but there is a slight ambiguity in the question regarding the specific requirements for the IO-aware implementation. However, the context provides enough information to clarify that the IO-aware implementation should be optimal for computing attention on a single GPU and potentially parallelizable for multi-GPU scenarios.</td>\n",
              "      <td>4. This question is highly useful for developers who need to understand the capabilities of PyTorch and CUDA for implementing attention algorithms in NLP applications.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n</td>\n",
              "      <td>Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n</td>\n",
              "      <td>Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22</td>\n",
              "      <td>(your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK&gt;¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention\\n\\nOutput::: [/INST] Deep question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n</td>\n",
              "      <td>The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n</td>\n",
              "      <td>Paged attention can improve the performance of a gauge model serving in a production environment by reducing the memory footprint and speeding up the inference process.</td>\n",
              "      <td>3.5\\n\\nThe question is clearly answerable with the given context, but the specific impact of paged attention on the performance of a gauge model serving in a production environment may depend on the specific implementation and use case, making it a bit less unambiguous.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n</td>\n",
              "      <td>The purpose of chunking in the context of the pre-fill process in Mistral 7B is to limit memory usage by processing long sequences in smaller, more manageable chunks.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\\n\\n</td>\n",
              "      <td>The grouped-query attention (GQA) mechanism in Mistral 7B groups multiple queries together and processes them simultaneously, reducing the inference cost. The sliding window attention (SWA) mechanism enables the model to effectively handle sequences of arbitrary length by processing the sequence in smaller windows, further reducing the inference cost. These mechanisms, combined with other optimizations, enable Mistral 7B to outperform other language models in terms of performance and efficiency.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Can you explain how the attention output is computed in Algorithm 1?\\n\\n</td>\n",
              "      <td>The attention output is computed by splitting the inputs Q-K-V into blocks, loading them from slow HBM to fast SRAM, and then computing the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, the correct result is obtained at the end. The softmax is decomposed using scaling for numerical stability. For vectors ùë•¬π1¬π-ùë•¬π2¬≤Rùêµ, the softmax of the concatenated ùë•=ùë•¬π1¬πùë•¬π2¬≤Rùêµ is decomposed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas and ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas. ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas is computed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬π</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n</td>\n",
              "      <td>The Mistral 7B language model is designed to balance high performance and efficiency by using a sliding window attention mechanism. This mechanism allows the model to attend to a smaller portion of the input sequence at a time, reducing the number of operations and memory usage required for inference. Additionally, the model is crafted for ease of fine-tuning across a wide range of tasks, making it adaptable and efficient for various real-world applications.</td>\n",
              "      <td>3\\n\\nThe context provides some relevant information, but it does not fully answer the question.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                question  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n   \n",
              "3  Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\\n\\n   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Can you explain how the attention output is computed in Algorithm 1?\\n\\n   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.   \n",
              "3  (your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention\\n\\nOutput::: [/INST] Deep question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Paged attention can improve the performance of a gauge model serving in a production environment by reducing the memory footprint and speeding up the inference process.   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The purpose of chunking in the context of the pre-fill process in Mistral 7B is to limit memory usage by processing long sequences in smaller, more manageable chunks.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The grouped-query attention (GQA) mechanism in Mistral 7B groups multiple queries together and processes them simultaneously, reducing the inference cost. The sliding window attention (SWA) mechanism enables the model to effectively handle sequences of arbitrary length by processing the sequence in smaller windows, further reducing the inference cost. These mechanisms, combined with other optimizations, enable Mistral 7B to outperform other language models in terms of performance and efficiency.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The attention output is computed by splitting the inputs Q-K-V into blocks, loading them from slow HBM to fast SRAM, and then computing the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, the correct result is obtained at the end. The softmax is decomposed using scaling for numerical stability. For vectors ùë•¬π1¬π-ùë•¬π2¬≤Rùêµ, the softmax of the concatenated ùë•=ùë•¬π1¬πùë•¬π2¬≤Rùêµ is decomposed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas and ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas. ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas is computed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬π   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The Mistral 7B language model is designed to balance high performance and efficiency by using a sliding window attention mechanism. This mechanism allows the model to attend to a smaller portion of the input sequence at a time, reducing the number of operations and memory usage required for inference. Additionally, the model is crafted for ease of fine-tuning across a wide range of tasks, making it adaptable and efficient for various real-world applications.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                             groundedness_score  \\\n",
              "0  4.5\\n\\nThe answer is almost unambiguous, but there is a slight ambiguity in the question regarding the specific requirements for the IO-aware implementation. However, the context provides enough information to clarify that the IO-aware implementation should be optimal for computing attention on a single GPU and potentially parallelizable for multi-GPU scenarios.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                           4.5   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
              "5                                                                                                3.5\\n\\nThe question is clearly answerable with the given context, but the specific impact of paged attention on the performance of a gauge model serving in a production environment may depend on the specific implementation and use case, making it a bit less unambiguous.   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                           4.5   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                             5   \n",
              "9                                                                                                                                                                                                                                                                               3\\n\\nThe context provides some relevant information, but it does not fully answer the question.   \n",
              "\n",
              "                                                                                                                                                           relevance_score  \\\n",
              "0  4. This question is highly useful for developers who need to understand the capabilities of PyTorch and CUDA for implementing attention algorithms in NLP applications.   \n",
              "1                                                                                                                                                                      NaN   \n",
              "2                                                                                                                                                                        4   \n",
              "3                                                                                                                                                                      NaN   \n",
              "4                                                                                                                                                                      NaN   \n",
              "5                                                                                                                                                                      NaN   \n",
              "6                                                                                                                                                                      NaN   \n",
              "7                                                                                                                                                                        4   \n",
              "8                                                                                                                                                                        4   \n",
              "9                                                                                                                                                                      NaN   \n",
              "\n",
              "  standalone_score  \n",
              "0                5  \n",
              "1              NaN  \n",
              "2                4  \n",
              "3              NaN  \n",
              "4              NaN  \n",
              "5              NaN  \n",
              "6              NaN  \n",
              "7                5  \n",
              "8                5  \n",
              "9              NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_questions['groundedness_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['relevance_score']=generated_questions['relevance_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['standalone_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n</td>\n",
              "      <td>Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n</td>\n",
              "      <td>Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n</td>\n",
              "      <td>Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22</td>\n",
              "      <td>(your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK&gt;¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention\\n\\nOutput::: [/INST] Deep question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n</td>\n",
              "      <td>The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n</td>\n",
              "      <td>Paged attention can improve the performance of a gauge model serving in a production environment by reducing the memory footprint and speeding up the inference process.</td>\n",
              "      <td>3.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n</td>\n",
              "      <td>The purpose of chunking in the context of the pre-fill process in Mistral 7B is to limit memory usage by processing long sequences in smaller, more manageable chunks.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\\n\\n</td>\n",
              "      <td>The grouped-query attention (GQA) mechanism in Mistral 7B groups multiple queries together and processes them simultaneously, reducing the inference cost. The sliding window attention (SWA) mechanism enables the model to effectively handle sequences of arbitrary length by processing the sequence in smaller windows, further reducing the inference cost. These mechanisms, combined with other optimizations, enable Mistral 7B to outperform other language models in terms of performance and efficiency.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Can you explain how the attention output is computed in Algorithm 1?\\n\\n</td>\n",
              "      <td>The attention output is computed by splitting the inputs Q-K-V into blocks, loading them from slow HBM to fast SRAM, and then computing the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, the correct result is obtained at the end. The softmax is decomposed using scaling for numerical stability. For vectors ùë•¬π1¬π-ùë•¬π2¬≤Rùêµ, the softmax of the concatenated ùë•=ùë•¬π1¬πùë•¬π2¬≤Rùêµ is decomposed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas and ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas. ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas is computed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬π</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n</td>\n",
              "      <td>The Mistral 7B language model is designed to balance high performance and efficiency by using a sliding window attention mechanism. This mechanism allows the model to attend to a smaller portion of the input sequence at a time, reducing the number of operations and memory usage required for inference. Additionally, the model is crafted for ease of fine-tuning across a wide range of tasks, making it adaptable and efficient for various real-world applications.</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                question  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n   \n",
              "3  Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              How does paged attention affect the performance of a gauge model serving in a production environment?\\n\\n   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              What is the purpose of chunking in the context of the pre-fill process in Mistral 7B?\\n\\n   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Can you explain how the grouped-query attention (GQA) and sliding window attention (SWA) mechanisms in Mistral 7B enable faster and more efficient inference compared to other language models?\\n\\n   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Can you explain how the attention output is computed in Algorithm 1?\\n\\n   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Can you explain how the Mistral 7B language model is designed to balance high performance and efficiency?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.   \n",
              "3  (your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention\\n\\nOutput::: [/INST] Deep question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Paged attention can improve the performance of a gauge model serving in a production environment by reducing the memory footprint and speeding up the inference process.   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The purpose of chunking in the context of the pre-fill process in Mistral 7B is to limit memory usage by processing long sequences in smaller, more manageable chunks.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The grouped-query attention (GQA) mechanism in Mistral 7B groups multiple queries together and processes them simultaneously, reducing the inference cost. The sliding window attention (SWA) mechanism enables the model to effectively handle sequences of arbitrary length by processing the sequence in smaller windows, further reducing the inference cost. These mechanisms, combined with other optimizations, enable Mistral 7B to outperform other language models in terms of performance and efficiency.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The attention output is computed by splitting the inputs Q-K-V into blocks, loading them from slow HBM to fast SRAM, and then computing the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, the correct result is obtained at the end. The softmax is decomposed using scaling for numerical stability. For vectors ùë•¬π1¬π-ùë•¬π2¬≤Rùêµ, the softmax of the concatenated ùë•=ùë•¬π1¬πùë•¬π2¬≤Rùêµ is decomposed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas and ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas. ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas is computed as: ùëö¬πùë•¬∫=ùëö¬π\u0002ùë•¬π1¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬πùë•¬π2¬≤Rùêµas = max¬πùëö¬πùë•¬π1¬π¬∫-ùëì¬πùë•¬∫=hùëíùëö¬πùë•¬π1¬π¬∫-ùëö¬π   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The Mistral 7B language model is designed to balance high performance and efficiency by using a sliding window attention mechanism. This mechanism allows the model to attend to a smaller portion of the input sequence at a time, reducing the number of operations and memory usage required for inference. Additionally, the model is crafted for ease of fine-tuning across a wide range of tasks, making it adaptable and efficient for various real-world applications.   \n",
              "\n",
              "   groundedness_score  relevance_score  standalone_score  \n",
              "0                 4.5              4.0               4.5  \n",
              "1                 NaN              NaN               NaN  \n",
              "2                 4.5              4.0               4.5  \n",
              "3                 NaN              NaN               NaN  \n",
              "4                 NaN              NaN               NaN  \n",
              "5                 3.5              NaN               3.5  \n",
              "6                 NaN              NaN               NaN  \n",
              "7                 4.5              4.0               4.5  \n",
              "8                 5.0              4.0               5.0  \n",
              "9                 3.0              NaN               3.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "oBWuOu1b9jVO",
        "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation dataset before filtering:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the number of HBM accesses required by the backward pass of FlashAttention for a given sequence length, head dimension, and SRAM size?\\n\\n</td>\n",
              "      <td>The backward pass of FlashAttention requires Œò¬πùëÅ2ùëë2ùëÄ¬πHBM accesses.</td>\n",
              "      <td>!!!2.5!!!</td>\n",
              "      <td>!!!3.5!!!</td>\n",
              "      <td>!!!4.5!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Can the FlashAttention algorithm achieve wall-clock speedup compared to approximate attention methods, even with the added constraint of being IO-aware?\\n\\n</td>\n",
              "      <td>Yes, the FlashAttention algorithm can achieve wall-clock speedup compared to approximate attention methods, even with the added constraint of being IO-aware. The use of tiling in FlashAttention reduces the number of memory reads/writes, which results in faster processing times and lower memory usage. Additionally, the IO-awareness of the algorithm ensures that it is optimized for GPU memory usage, further improving its performance.</td>\n",
              "      <td>!!!4.0!!!</td>\n",
              "      <td>!!!4.0!!!\\n\\nThe total rating of 4.0 indicates that this question is moderately useful for machine learning developers building NLP applications with the Hugging Face ecosystem. While the question provides valuable insights into the performance characteristics of different attention mechanisms, it may not be as relevant or informative as other questions related to specific NLP tasks or applications.</td>\n",
              "      <td>!!!5!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Can you explain the difference between the instruction fine-tuning process used for the Mistral 7B ‚Äì Instruct model and the training tricks used for the other 7B models on MT-Bench?\\n\\n</td>\n",
              "      <td>The instruction fine-tuning process used for the Mistral 7B ‚Äì Instruct model involved fine-tuning the base model on publicly available instruction datasets on the Hugging Face repository, without using any proprietary data or training tricks. This was done to demonstrate the generalization capabilities of the base model. On the other hand, the other 7B models on MT-Bench were trained using proprietary data and training tricks, which may have contributed to their superior performance.</td>\n",
              "      <td>!!!2.5!!!</td>\n",
              "      <td>!!!4.0!!! \\n\\nThe question is moderately useful as it provides insights into the specific fine-tuning process used for the Mistral 7B ‚Äì Instruct model and the training tricks used for other 7B models on MT-Bench. However, it could be further improved by providing more context and specificity to the question, such as comparing the performance of the two models or discussing the specific training tricks used for each model.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the relative performance of GPT-2 with FlashAttention compared to Megatron-LM in terms of perplexity and training time speedup, when the context length is increased from 1K to 4K?\\n\\n</td>\n",
              "      <td>GPT-2 with FlashAttention achieved 0.7 better perplexity and 30% faster training time compared to Megatron-LM when the context length was increased from 1K to 4K. Specifically, GPT-2 small with FlashAttention 4k achieved a perplexity of 17.5 and a training time speedup of 1.3 compared to Megatron-LM small with a context length of 1K.</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4\\n\\nThe question is quite useful, but it could be improved by providing more specific details about the datasets used for training and testing, as well as any other relevant metrics that could be considered. Additionally, it would be helpful to know if there are any other factors that could impact the performance of these models, such as the size of the training corpus or the specific hardware used for training.</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In the context of model training, how can the effectiveness of naive kernel fusion be increased in the standard attention implementation?\\n\\n</td>\n",
              "      <td>In the standard attention implementation, the intermediate values still need to be written to HBM to save for the backward pass, reducing the effectiveness of naive kernel fusion. To increase the effectiveness of naive kernel fusion, one approach is to use techniques such as weight pruning or quantization to reduce the memory requirements of the intermediate values, allowing them to be stored in the HBM without taking up too much memory. Another approach is to use techniques such as batch normalization or layer normalization to stabilize the training process and reduce the need for intermediate values to be written to HBM during the forward pass.</td>\n",
              "      <td>4. While the question is somewhat ambiguous regarding the specifics of how to increase the effectiveness of naive kernel fusion, the provided context provides enough information to give a general understanding of the problem and potential solutions.</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Given the table data, what is the training time speedup achieved by FlashAttention in comparison to the Huggingface implementation for GPT-2 small and medium models?\\n\\n</td>\n",
              "      <td>The table shows that for GPT-2 small and medium models, FlashAttention achieved a training time speedup of 3.5 and 3.0, respectively, compared to the Huggingface implementation.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the use of kernel fusion and tiling in the implementation of the FlashAttention algorithm enable faster computation and reduced HBM accesses?\\n\\n</td>\n",
              "      <td>The use of kernel fusion and tiling in the implementation of the FlashAttention algorithm allows for faster computation and reduced HBM accesses by enabling all computation steps to be performed in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then writing the result back to HBM. This avoids repeatedly reading and writing of inputs and outputs from and to HBM, which can slow down computation and increase memory usage. By minimizing the number of blocks required and reducing the amount of data that needs to be transferred between HBM and on-chip SRAM, the implementation of FlashAttention is able to achieve faster computation and reduced HBM accesses.</td>\n",
              "      <td>4.5\\n\\nThe use of kernel fusion and tiling in the implementation of the FlashAttention algorithm is described in detail in the context, and the algorithm itself is also provided. The context also explains how the use of these techniques enables faster computation and reduced HBM accesses. Therefore, the question is clearly and unambiguously answerable with the given context.</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the independent human evaluation of the leaderboard on &lt;https://llmboxing.com/leaderboard&gt; compare the performance of Mistral 7B and Llama 2 13B models in terms of preferred responses?\\n\\n</td>\n",
              "      <td>According to the evaluation, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143 times for Llama 2 13B.</td>\n",
              "      <td>5.0\\n\\nThe question is clearly and unambiguously answerable with the given context, as the context provides all the necessary information to answer the question. The answer is based on the results of an independent human evaluation conducted on the leaderboard of &lt;https://llmboxing.com/leaderboard&gt; and provides a clear comparison of the performance of the two models in terms of preferred responses.</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Can you explain the difference between Mistral 7B ‚Äì Instruct and Llama 2 Chat 13B in terms of their ability to classify user prompts and their generated answers as acceptable or falling into categories related to illegal activities or hateful harassment?\\n\\n</td>\n",
              "      <td>The difference between Mistral 7B ‚Äì Instruct and Llama 2 Chat 13B lies in their ability to classify user prompts and their generated answers as acceptable or falling into categories related to illegal activities or hateful harassment. Mistral 7B ‚Äì Instruct is able to accurately classify user prompts or their generated answers as either acceptable or falling into one of the following categories: Illegal activities such as terrorism, child abuse or fraud; Hateful, harassing or violent content. On the other hand, Llama 2 Chat 13B declines to answer harmful questions even when system prompts are activated and does not provide a correct response for the question \"How to kill a linux process with system prompts activated\", but answers correctly when system prompts are deactivated.</td>\n",
              "      <td>4.5\\n\\nThe total rating is 4.5 because while Mistral 7B ‚Äì Instruct has a better ability to classify harmful questions, the context is limited to a specific example and does not provide a comprehensive analysis of the models' performance in this regard.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the rolling buffer cache work in the context of the FlashAttention and xFormers models?\\n\\n</td>\n",
              "      <td>The rolling buffer cache works by limiting the cache size to a fixed value of W. The keys and values for the timesteps i are stored in position imodW of the cache. When the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. This allows for efficient use of memory and can improve the speed of the models.</td>\n",
              "      <td>3.5. While the context provides a good overview of the models and their attention mechanisms, the question asks for a more specific explanation of the rolling buffer cache, which may not be fully covered in the context.</td>\n",
              "      <td>4.\\n\\nThe rolling buffer cache is an important aspect of the Hugging Face models, and understanding how it works can help developers optimize their models for better performance. However, the question could be improved by providing more context about the FlashAttention and xFormers models, as well as examples of how the rolling buffer cache is used in practice.</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                             question  \\\n",
              "0                                                                                                                  What is the number of HBM accesses required by the backward pass of FlashAttention for a given sequence length, head dimension, and SRAM size?\\n\\n   \n",
              "1                                                                                                        Can the FlashAttention algorithm achieve wall-clock speedup compared to approximate attention methods, even with the added constraint of being IO-aware?\\n\\n   \n",
              "2                                                                           Can you explain the difference between the instruction fine-tuning process used for the Mistral 7B ‚Äì Instruct model and the training tricks used for the other 7B models on MT-Bench?\\n\\n   \n",
              "3                                                                     What is the relative performance of GPT-2 with FlashAttention compared to Megatron-LM in terms of perplexity and training time speedup, when the context length is increased from 1K to 4K?\\n\\n   \n",
              "4                                                                                                                       In the context of model training, how can the effectiveness of naive kernel fusion be increased in the standard attention implementation?\\n\\n   \n",
              "5                                                                                           Given the table data, what is the training time speedup achieved by FlashAttention in comparison to the Huggingface implementation for GPT-2 small and medium models?\\n\\n   \n",
              "6                                                                                                          How does the use of kernel fusion and tiling in the implementation of the FlashAttention algorithm enable faster computation and reduced HBM accesses?\\n\\n   \n",
              "7                                                               How does the independent human evaluation of the leaderboard on <https://llmboxing.com/leaderboard> compare the performance of Mistral 7B and Llama 2 13B models in terms of preferred responses?\\n\\n   \n",
              "8  Can you explain the difference between Mistral 7B ‚Äì Instruct and Llama 2 Chat 13B in terms of their ability to classify user prompts and their generated answers as acceptable or falling into categories related to illegal activities or hateful harassment?\\n\\n   \n",
              "9                                                                                                                                                                How does the rolling buffer cache work in the context of the FlashAttention and xFormers models?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The backward pass of FlashAttention requires Œò¬πùëÅ2ùëë2ùëÄ¬πHBM accesses.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                Yes, the FlashAttention algorithm can achieve wall-clock speedup compared to approximate attention methods, even with the added constraint of being IO-aware. The use of tiling in FlashAttention reduces the number of memory reads/writes, which results in faster processing times and lower memory usage. Additionally, the IO-awareness of the algorithm ensures that it is optimized for GPU memory usage, further improving its performance.   \n",
              "2                                                                                                                                                                                                                                                                                                           The instruction fine-tuning process used for the Mistral 7B ‚Äì Instruct model involved fine-tuning the base model on publicly available instruction datasets on the Hugging Face repository, without using any proprietary data or training tricks. This was done to demonstrate the generalization capabilities of the base model. On the other hand, the other 7B models on MT-Bench were trained using proprietary data and training tricks, which may have contributed to their superior performance.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                    GPT-2 with FlashAttention achieved 0.7 better perplexity and 30% faster training time compared to Megatron-LM when the context length was increased from 1K to 4K. Specifically, GPT-2 small with FlashAttention 4k achieved a perplexity of 17.5 and a training time speedup of 1.3 compared to Megatron-LM small with a context length of 1K.   \n",
              "4                                                                                                                                     In the standard attention implementation, the intermediate values still need to be written to HBM to save for the backward pass, reducing the effectiveness of naive kernel fusion. To increase the effectiveness of naive kernel fusion, one approach is to use techniques such as weight pruning or quantization to reduce the memory requirements of the intermediate values, allowing them to be stored in the HBM without taking up too much memory. Another approach is to use techniques such as batch normalization or layer normalization to stabilize the training process and reduce the need for intermediate values to be written to HBM during the forward pass.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The table shows that for GPT-2 small and medium models, FlashAttention achieved a training time speedup of 3.5 and 3.0, respectively, compared to the Huggingface implementation.   \n",
              "6                     The use of kernel fusion and tiling in the implementation of the FlashAttention algorithm allows for faster computation and reduced HBM accesses by enabling all computation steps to be performed in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then writing the result back to HBM. This avoids repeatedly reading and writing of inputs and outputs from and to HBM, which can slow down computation and increase memory usage. By minimizing the number of blocks required and reducing the amount of data that needs to be transferred between HBM and on-chip SRAM, the implementation of FlashAttention is able to achieve faster computation and reduced HBM accesses.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                According to the evaluation, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143 times for Llama 2 13B.   \n",
              "8  The difference between Mistral 7B ‚Äì Instruct and Llama 2 Chat 13B lies in their ability to classify user prompts and their generated answers as acceptable or falling into categories related to illegal activities or hateful harassment. Mistral 7B ‚Äì Instruct is able to accurately classify user prompts or their generated answers as either acceptable or falling into one of the following categories: Illegal activities such as terrorism, child abuse or fraud; Hateful, harassing or violent content. On the other hand, Llama 2 Chat 13B declines to answer harmful questions even when system prompts are activated and does not provide a correct response for the question \"How to kill a linux process with system prompts activated\", but answers correctly when system prompts are deactivated.   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                   The rolling buffer cache works by limiting the cache size to a fixed value of W. The keys and values for the timesteps i are stored in position imodW of the cache. When the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. This allows for efficient use of memory and can improve the speed of the models.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                  groundedness_score  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                          !!!2.5!!!   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                          !!!4.0!!!   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                          !!!2.5!!!   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                3.5   \n",
              "4                                                                                                                                                          4. While the question is somewhat ambiguous regarding the specifics of how to increase the effectiveness of naive kernel fusion, the provided context provides enough information to give a general understanding of the problem and potential solutions.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                4.5   \n",
              "6                          4.5\\n\\nThe use of kernel fusion and tiling in the implementation of the FlashAttention algorithm is described in detail in the context, and the algorithm itself is also provided. The context also explains how the use of these techniques enables faster computation and reduced HBM accesses. Therefore, the question is clearly and unambiguously answerable with the given context.   \n",
              "7  5.0\\n\\nThe question is clearly and unambiguously answerable with the given context, as the context provides all the necessary information to answer the question. The answer is based on the results of an independent human evaluation conducted on the leaderboard of <https://llmboxing.com/leaderboard> and provides a clear comparison of the performance of the two models in terms of preferred responses.   \n",
              "8                                                                                                                                                       4.5\\n\\nThe total rating is 4.5 because while Mistral 7B ‚Äì Instruct has a better ability to classify harmful questions, the context is limited to a specific example and does not provide a comprehensive analysis of the models' performance in this regard.   \n",
              "9                                                                                                                                                                                        3.5. While the context provides a good overview of the models and their attention mechanisms, the question asks for a more specific explanation of the rolling buffer cache, which may not be fully covered in the context.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                             relevance_score  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                  !!!3.5!!!   \n",
              "1                         !!!4.0!!!\\n\\nThe total rating of 4.0 indicates that this question is moderately useful for machine learning developers building NLP applications with the Hugging Face ecosystem. While the question provides valuable insights into the performance characteristics of different attention mechanisms, it may not be as relevant or informative as other questions related to specific NLP tasks or applications.   \n",
              "2  !!!4.0!!! \\n\\nThe question is moderately useful as it provides insights into the specific fine-tuning process used for the Mistral 7B ‚Äì Instruct model and the training tricks used for other 7B models on MT-Bench. However, it could be further improved by providing more context and specificity to the question, such as comparing the performance of the two models or discussing the specific training tricks used for each model.   \n",
              "3           4\\n\\nThe question is quite useful, but it could be improved by providing more specific details about the datasets used for training and testing, as well as any other relevant metrics that could be considered. Additionally, it would be helpful to know if there are any other factors that could impact the performance of these models, such as the size of the training corpus or the specific hardware used for training.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                          4   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                        NaN   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                          4   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                          4   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                        NaN   \n",
              "9                                                                4.\\n\\nThe rolling buffer cache is an important aspect of the Hugging Face models, and understanding how it works can help developers optimize their models for better performance. However, the question could be improved by providing more context about the FlashAttention and xFormers models, as well as examples of how the rolling buffer cache is used in practice.   \n",
              "\n",
              "  standalone_score  \n",
              "0        !!!4.5!!!  \n",
              "1          !!!5!!!  \n",
              "2              NaN  \n",
              "3                4  \n",
              "4                3  \n",
              "5              NaN  \n",
              "6              NaN  \n",
              "7              NaN  \n",
              "8              NaN  \n",
              "9                3  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "# generated_questions = generated_questions.loc[\n",
        "#     (generated_questions[\"groundedness_score\"] >= 4)\n",
        "#     & (generated_questions[\"relevance_score\"] >= 4)\n",
        "#     & (generated_questions[\"standalone_score\"] >= 4)\n",
        "# ]\n",
        "# print(\"============================================\")\n",
        "# print(\"Final evaluation dataset:\")\n",
        "# display(\n",
        "#     generated_questions[\n",
        "#         [\n",
        "#             \"question\",\n",
        "#             \"answer\",\n",
        "#             \"groundedness_score\",\n",
        "#             \"relevance_score\",\n",
        "#             \"standalone_score\",\n",
        "#         ]\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# eval_dataset = datasets.Dataset.from_pandas(\n",
        "#     generated_questions, split=\"train\", preserve_index=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>groundedness_eval</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>relevance_eval</th>\n",
              "      <th>standalone_score</th>\n",
              "      <th>standalone_eval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort.\\nImplementations may also not be transferrable across GPU architectures. These limitations suggest the\\nneed for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\\ncompiling to IO-aware implementations in CUDA‚Äîsimilar to eÔ¨Äorts such as Halide in image processing [ 70].\\nIO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\\nAttention is the most memory-intensive computation in Transformers, but every layer in a deep network\\ntouches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\\nthese potential extensions in Appendix D.\\nMulti-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-\\nstants for computing attention on a single GPU. However, the attention computation may be parallelizable</td>\n",
              "      <td>Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n</td>\n",
              "      <td>Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.</td>\n",
              "      <td>FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf</td>\n",
              "      <td>4.5</td>\n",
              "      <td>This question is asking about the possibility of using PyTorch to write attention algorithms with IO-aware implementations in CUDA. The context provides some background information on the limitations of writing attention algorithms in lower-level languages and the need for a high-level language approach. It also discusses the benefits of IO-aware implementations and their potential extensions to other modules. The question is specific to PyTorch and CUDA, and the context provides relevant information to answer the question.\\n\\n</td>\n",
              "      <td>4.0</td>\n",
              "      <td>The question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the possibility of using a high-level language such as PyTorch to write attention algorithms with IO-aware implementations in CUDA. This is an important consideration for developers who want to optimize their NLP models for efficient execution on GPUs.\\n\\n</td>\n",
              "      <td>4.5</td>\n",
              "      <td>\\nThe question is context-independent and does not require any additional information to be understood. It refers to the technical capabilities of PyTorch in implementing attention algorithms with IO-aware implementations in CUDA.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n5.1 System prompt to enforce guardrails\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified\\nguardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\\nPareto front of model utility / guardrails enforcement, as indicated in Table 4.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\\nunethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\nGuardrails MT Bench\\nNo system prompt 6.84 ¬±0.07\\nLlama 2 system prompt 6.38 ¬±0.07\\nMistral system prompt 6.58 ¬±0.05\\nTable 4: System prompts. Mean official\\nMT Bench score over 10 iterations with</td>\n",
              "      <td>Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n</td>\n",
              "      <td>Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).</td>\n",
              "      <td>Mistral7B.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,</td>\n",
              "      <td>Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n</td>\n",
              "      <td>Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.</td>\n",
              "      <td>Mistral7B.pdf</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Mistral 7B is a large language model that uses grouped-query attention (GQA) and sliding window attention (SWA) as attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.\\n\\n</td>\n",
              "      <td>4.0</td>\n",
              "      <td>The given question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the attention mechanisms used by Mistral 7B and how they contribute to its enhanced performance and efficiency. Attention mechanisms are an important aspect of NLP models, and understanding how they work can help developers improve the performance of their own models. Additionally, Mistral 7B is a state-of-the-art NLP model that has achieved impressive results on various benchmarks, so exploring its attention mechanisms can provide valuable insights into best practices for building high-performing NLP models.\\n\\n</td>\n",
              "      <td>4.5</td>\n",
              "      <td>\\nThe question is asking for information about the attention mechanisms used by Mistral 7B and how they contribute to its performance and efficiency. It is not explicitly stated what context is being referred to, so it is likely that the question is context-independent. \\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK&gt;¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention</td>\n",
              "      <td>Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22</td>\n",
              "      <td>(your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK&gt;¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention\\n\\nOutput::: [/INST] Deep question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22</td>\n",
              "      <td>FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\\nunit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\\n3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\\nbetter than as reported in the original comparison [80].\\n8Attention Memory Usage\\nSequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\\nSequence LengthRuntime (ms)\\nMemory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\\n1020\\nFlashAttention\\nBlock-Sparse FlashAttentionPyTorch Attention\\nMegatron AttentionLinformer Attention\\nOpenAI Sparse Attention8192100Crossover Points\\n20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.\\nEuropean Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights</td>\n",
              "      <td>Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n</td>\n",
              "      <td>The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.</td>\n",
              "      <td>FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                context  \\\n",
              "0                                                   algorithm in a considerably lower-level language than PyTorch, and requires signiÔ¨Åcant engineering eÔ¨Äort.\\nImplementations may also not be transferrable across GPU architectures. These limitations suggest the\\nneed for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\\ncompiling to IO-aware implementations in CUDA‚Äîsimilar to eÔ¨Äorts such as Halide in image processing [ 70].\\nIO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\\nAttention is the most memory-intensive computation in Transformers, but every layer in a deep network\\ntouches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\\nthese potential extensions in Appendix D.\\nMulti-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-\\nstants for computing attention on a single GPU. However, the attention computation may be parallelizable   \n",
              "1  cations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n5.1 System prompt to enforce guardrails\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified\\nguardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\\nPareto front of model utility / guardrails enforcement, as indicated in Table 4.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\\nunethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\nGuardrails MT Bench\\nNo system prompt 6.84 ¬±0.07\\nLlama 2 system prompt 6.38 ¬±0.07\\nMistral system prompt 6.58 ¬±0.05\\nTable 4: System prompts. Mean official\\nMT Bench score over 10 iterations with   \n",
              "2                                         generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,   \n",
              "3                                          ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention   \n",
              "4                                                                                          improves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\\nunit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\\n3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\\nbetter than as reported in the original comparison [80].\\n8Attention Memory Usage\\nSequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\\nSequence LengthRuntime (ms)\\nMemory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\\n1020\\nFlashAttention\\nBlock-Sparse FlashAttentionPyTorch Attention\\nMegatron AttentionLinformer Attention\\nOpenAI Sparse Attention8192100Crossover Points\\n20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.\\nEuropean Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                question  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Can a high-level language such as PyTorch be used to write attention algorithms with IO-aware implementations in CUDA?\\n\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Can Mistral 7B perform fine-grained content moderation using a system prompt to enforce guardrails?\\n\\n   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Given the context, what are the attention mechanisms used by Mistral 7B and how do they contribute to its enhanced performance and efficiency?\\n\\n   \n",
              "3  Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Given the context, what is the relationship between the attention memory usage and sequence length in the MIMIC-III and ECtHR datasets?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Yes, a high-level language such as PyTorch can be used to write attention algorithms with IO-aware implementations in CUDA. This is because the IO-aware approach can extend beyond attention and can be applied to other modules in a deep network. The IO-aware implementation of attention is optimal within constants for computing attention on a single GPU, but it may not be parallelizable across multiple GPUs.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Yes, according to the context, Mistral 7B can perform fine-grained content moderation using a system prompt to enforce guardrails. This is indicated in Table 4, where the mean official MT Bench score for Mistral system prompt is 6.58 ¬±0.05, which is higher than the score for no system prompt (6.84 ¬±0.07) and Llama 2 system prompt (6.38 ¬±0.07).   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Mistral 7B uses grouped-query attention (GQA) and sliding window attention (SWA) as its attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput, which is crucial for real-time applications. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.   \n",
              "3  (your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: ùëÅ\\nùêµùëêm\\nblocks\\nK1¬ñ¬ï¬ï¬ï¬ñKùëáùëêandV1¬ñ¬ï¬ï¬ï¬ñVùëáùëê, of sizeùêµùëê\u0002ùëëeach.\\n4:Divide Ointoùëáùëüblocks Oùëñ¬ñ¬ï¬ï¬ï¬ñOùëáùëüof sizeùêµùëü\u0002ùëëeach, divide ‚Ñìintoùëáùëüblocks‚Ñìùëñ¬ñ¬ï¬ï¬ï¬ñ‚Ñìùëáùëüof sizeùêµùëüeach,\\ndivideùëöintoùëáùëüblocksùëö1¬ñ¬ï¬ï¬ï¬ñùëöùëáùëüof sizeùêµùëüeach.\\n5:for1\u0014ùëó\u0014ùëáùëêdo\\n6:Load Kùëó¬ñVùëófrom HBM to on-chip SRAM.\\n7:for1\u0014ùëñ\u0014ùëáùëüdo\\n8:Load Qùëñ¬ñOùëñ¬ñ‚Ñìùëñ¬ñùëöùëñfrom HBM to on-chip SRAM.\\n9:On chip, compute Sùëñùëó=QùëñKùëá\\nùëó2Rùêµùëü\u0002ùêµùëê.\\n10:On chip, compute ~ùëöùëñùëó=rowmax¬πSùëñùëó¬∫ 2Rùêµùëü,~Pùëñùëó=exp¬πSùëñùëó\u0000~ùëöùëñùëó¬∫ 2Rùêµùëü\u0002ùêµùëê(pointwise), ~‚Ñìùëñùëó=\\nrowsum¬π~Pùëñùëó¬∫2Rùêµùëü.\\n11:On chip, compute ùëönew\\nùëñ=max¬πùëöùëñ¬ñ~ùëöùëñùëó¬∫2Rùêµùëü,‚Ñìnew\\nùëñ=ùëíùëöùëñ\u0000ùëönew\\nùëñ‚Ñìùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~‚Ñìùëñùëó2Rùêµùëü.\\n12:Write Oùëñ diag¬π‚Ñìnew\\nùëñ¬∫\u00001¬πdiag¬π‚Ñìùëñ¬∫ùëíùëöùëñ\u0000ùëönew\\nùëñOùëñ¬∏ùëí~ùëöùëñùëó\u0000ùëönew\\nùëñ~PùëñùëóVùëó¬∫to HBM.\\n13:Write‚Ñìùëñ ‚Ñìnew\\nùëñ,ùëöùëñ ùëönew\\nùëñto HBM.\\n14:end for\\n15:end for\\n16:Return O.\\nWe show FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C).\\nTheorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional\\nmemory beyond inputs and output.\\n3.2 Analysis: IO Complexity of FlashAttention\\n\\nOutput::: [/INST] Deep question: Suppose K1, K2, K3, K4, K5, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K20, K21, K22, K23, K24, K25, K26, K27, K28, K29, K30, K31, K32, K33, K34, K35, K36, K37, K38, K39, K40, K41, K42, K43, K44, K45, K46, K47, K48, K49, K50, K51, K52, K53, K54, K55, K56, K57, K58, K59, K60, K61, K62, K63, K64, K65, K66, K67, K68, K69, K70, K71, K72, K73, K74, K75, K76, K77, K78, K79, K80, K81, K82, K83, K84, K85, K86, K87, K88, K89, K90, K91, K92, K93, K94, K95, K96, K97, K98, K99, K100, K101, K102, K103, K104, K105, K106, K107, K108, K109, K110, K111, K112, K113, K114, K115, K116, K117, K118, K119, K120, K121, K122, K123, K124, K125, K126, K127, K128, K129, K130, K131, K132, K133, K134, K135, K136, K137, K138, K139, K140, K141, K142, K143, K144, K145, K146, K147, K148, K149, K150, K151, K152, K153, K154, K155, K156, K157, K158, K159, K160, K161, K162, K163, K164, K165, K166, K167, K168, K169, K170, K171, K172, K173, K174, K175, K176, K177, K178, K179, K180, K181, K182, K183, K184, K185, K186, K187, K188, K189, K190, K191, K192, K193, K194, K195, K196, K197, K198, K199, K200, K201, K202, K203, K204, K205, K206, K207, K208, K209, K210, K211, K212, K213, K214, K215, K216, K217, K218, K219, K220, K221, K222, K223, K224, K225, K22   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The attention memory usage and sequence length are positively correlated in both the MIMIC-III and ECtHR datasets. As the sequence length increases, the attention memory usage also increases. This is evident from the graph in Figure 3, which shows that the attention memory usage increases as the sequence length increases.   \n",
              "\n",
              "                                                                         source_doc  \\\n",
              "0  FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf   \n",
              "1                                                                     Mistral7B.pdf   \n",
              "2                                                                     Mistral7B.pdf   \n",
              "3  FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf   \n",
              "4  FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf   \n",
              "\n",
              "   groundedness_score  \\\n",
              "0                 4.5   \n",
              "1                 NaN   \n",
              "2                 4.5   \n",
              "3                 NaN   \n",
              "4                 NaN   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      groundedness_eval  \\\n",
              "0                 This question is asking about the possibility of using PyTorch to write attention algorithms with IO-aware implementations in CUDA. The context provides some background information on the limitations of writing attention algorithms in lower-level languages and the need for a high-level language approach. It also discusses the benefits of IO-aware implementations and their potential extensions to other modules. The question is specific to PyTorch and CUDA, and the context provides relevant information to answer the question.\\n\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "2  Mistral 7B is a large language model that uses grouped-query attention (GQA) and sliding window attention (SWA) as attention mechanisms. GQA significantly accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and higher throughput. SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.\\n\\n   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "\n",
              "   relevance_score  \\\n",
              "0              4.0   \n",
              "1              NaN   \n",
              "2              4.0   \n",
              "3              NaN   \n",
              "4              NaN   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        relevance_eval  \\\n",
              "0                                                                                                                                                                                                                                                                              The question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the possibility of using a high-level language such as PyTorch to write attention algorithms with IO-aware implementations in CUDA. This is an important consideration for developers who want to optimize their NLP models for efficient execution on GPUs.\\n\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  NaN   \n",
              "2  The given question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem because it asks about the attention mechanisms used by Mistral 7B and how they contribute to its enhanced performance and efficiency. Attention mechanisms are an important aspect of NLP models, and understanding how they work can help developers improve the performance of their own models. Additionally, Mistral 7B is a state-of-the-art NLP model that has achieved impressive results on various benchmarks, so exploring its attention mechanisms can provide valuable insights into best practices for building high-performing NLP models.\\n\\n   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  NaN   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  NaN   \n",
              "\n",
              "   standalone_score  \\\n",
              "0               4.5   \n",
              "1               NaN   \n",
              "2               4.5   \n",
              "3               NaN   \n",
              "4               NaN   \n",
              "\n",
              "                                                                                                                                                                                                                                                                       standalone_eval  \n",
              "0                                           \\nThe question is context-independent and does not require any additional information to be understood. It refers to the technical capabilities of PyTorch in implementing attention algorithms with IO-aware implementations in CUDA.\\n\\n  \n",
              "1                                                                                                                                                                                                                                                                                  NaN  \n",
              "2  \\nThe question is asking for information about the attention mechanisms used by Mistral 7B and how they contribute to its performance and efficiency. It is not explicitly stated what context is being referred to, so it is likely that the question is context-independent. \\n\\n  \n",
              "3                                                                                                                                                                                                                                                                                  NaN  \n",
              "4                                                                                                                                                                                                                                                                                  NaN  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_questions.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaOMZyu69jVO"
      },
      "source": [
        "Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
        "\n",
        "We have generated only a few QA couples here to reduce time and cost. But let's kick start the next part by loading a pre-generated dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q3RRz4W79jVO"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3efe40dfdd5a468fa9ba6b94c920e12a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/915 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 297k/297k [00:00<00:00, 990kB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a49df22745f74fdf8c05c3f8ee3c76f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/67 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5s19uTd9jVO"
      },
      "source": [
        "# 2. Build our RAG System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mET8Dy9jVO"
      },
      "source": [
        "### 2.1. Preprocessing documents to build our vector database\n",
        "\n",
        "- In this part, __we split the documents from our knowledge base into smaller chunks__: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
        "- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
        "\n",
        "Many options exist for text splitting:\n",
        "- split every `n` words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
        "- split after `n` words / character, but only on sentence boundaries\n",
        "- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
        "\n",
        "To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
        "\n",
        "[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
        "\n",
        "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "üí° _To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H4fhm55Q9jVO"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b324ffff4808427193aec778db49128a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sz9Jw2_q9jVO"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: str,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzBYfNG79jVO"
      },
      "source": [
        "### 2.2. Retriever - embeddings üóÇÔ∏è\n",
        "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
        "\n",
        "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
        "\n",
        "üõ†Ô∏è __Options included:__\n",
        "\n",
        "- Tune the chunking method:\n",
        "    - Size of the chunks\n",
        "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
        "- Change the embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LqJlIDZR9jVO"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "import os\n",
        "\n",
        "\n",
        "def load_embeddings(\n",
        "    langchain_docs: List[LangchainDocument],\n",
        "    chunk_size: int,\n",
        "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
        "\n",
        "    Args:\n",
        "        langchain_docs: list of documents\n",
        "        chunk_size: size of the chunks to split the documents into\n",
        "        embedding_model_name: name of the embedding model to use\n",
        "\n",
        "    Returns:\n",
        "        FAISS index\n",
        "    \"\"\"\n",
        "    # load embedding_model\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=embedding_model_name,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\n",
        "            \"normalize_embeddings\": True\n",
        "        },  # set True to compute cosine similarity\n",
        "    )\n",
        "\n",
        "    # Check if embeddings already exist on disk\n",
        "    index_name = (\n",
        "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
        "    )\n",
        "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
        "    if os.path.isdir(index_folder_path):\n",
        "        return FAISS.load_local(\n",
        "            index_folder_path,\n",
        "            embedding_model,\n",
        "            distance_strategy=DistanceStrategy.COSINE,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        print(\"Index not found, generating it...\")\n",
        "        docs_processed = split_documents(\n",
        "            chunk_size,\n",
        "            langchain_docs,\n",
        "            embedding_model_name,\n",
        "        )\n",
        "        knowledge_index = FAISS.from_documents(\n",
        "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "        )\n",
        "        knowledge_index.save_local(index_folder_path)\n",
        "        return knowledge_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6y1mQJX9jVO"
      },
      "source": [
        "### 2.3. Reader - LLM üí¨\n",
        "\n",
        "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
        "\n",
        "üõ†Ô∏è Here we tried the following options to improve results:\n",
        "- Switch reranking on/off\n",
        "- Change the reader model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9PdpuWyP9jVP"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9SDqenld9jVP"
      },
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for HuggingFaceHub\n__root__\n  Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceH4/zephyr-7b-beta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m READER_MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzephyr-7b-beta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m READER_LLM \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceHub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepetition_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.03\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     emit_warning()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.12/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
            "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n__root__\n  Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
        "\n",
        "READER_LLM = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 30,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ62CbcZ9jVP"
      },
      "outputs": [],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_core.language_models.llms import LLM\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: LLM,\n",
        "    knowledge_index: VectorStore,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 7,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
        "    # Gather documents with retriever\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    answer = llm(final_prompt)\n",
        "\n",
        "    return answer, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiygbqfT9jVP"
      },
      "source": [
        "# 3. Benchmarking the RAG system\n",
        "\n",
        "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
        "\n",
        "To this end, __we setup a judge agent__. ‚öñÔ∏èü§ñ\n",
        "\n",
        "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
        "\n",
        "> We use GPT4 as a judge for its empirically good performance, but you could try with other models such as [kaist-ai/prometheus-13b-v1.0](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) or [BAAI/JudgeLM-33B-v1.0](https://huggingface.co/BAAI/JudgeLM-33B-v1.0).\n",
        "\n",
        "üí° _In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples._\n",
        "\n",
        "üí° _Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrlMh_ZI9jVP"
      },
      "outputs": [],
      "source": [
        "def run_rag_tests(\n",
        "    eval_dataset: datasets.Dataset,\n",
        "    llm: BaseChatModel,\n",
        "    knowledge_index: VectorStore,\n",
        "    output_file: str,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    verbose: Optional[bool] = True,\n",
        "    test_settings: Optional[str] = None,  # To document the test settings used\n",
        "):\n",
        "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
        "    try:  # load previous generations if they exist\n",
        "        with open(output_file, \"r\") as f:\n",
        "            outputs = json.load(f)\n",
        "    except:\n",
        "        outputs = []\n",
        "\n",
        "    for example in tqdm(eval_dataset):\n",
        "        question = example[\"question\"]\n",
        "        if question in [output[\"question\"] for output in outputs]:\n",
        "            continue\n",
        "\n",
        "        answer, relevant_docs = answer_with_rag(\n",
        "            question, llm, knowledge_index, reranker=reranker\n",
        "        )\n",
        "        if verbose:\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'True answer: {example[\"answer\"]}')\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"true_answer\": example[\"answer\"],\n",
        "            \"source_doc\": example[\"source_doc\"],\n",
        "            \"generated_answer\": answer,\n",
        "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
        "        }\n",
        "        if test_settings:\n",
        "            result[\"test_settings\"] = test_settings\n",
        "        outputs.append(result)\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(outputs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae-3KWzK9jVP"
      },
      "outputs": [],
      "source": [
        "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
        "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "\n",
        "###The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "###Response to evaluate:\n",
        "{response}\n",
        "\n",
        "###Reference Answer (Score 5):\n",
        "{reference_answer}\n",
        "\n",
        "###Score Rubrics:\n",
        "[Is the response correct, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\n",
        "###Feedback:\"\"\"\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "\n",
        "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
        "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia9Mvn859jVP"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
        "evaluator_name = \"GPT4\"\n",
        "\n",
        "\n",
        "def evaluate_answers(\n",
        "    answer_path: str,\n",
        "    eval_chat_model: BaseChatModel,\n",
        "    evaluator_name: str,\n",
        "    evaluation_prompt_template: ChatPromptTemplate,\n",
        ") -> None:\n",
        "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
        "    answers = []\n",
        "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
        "        answers = json.load(open(answer_path, \"r\"))\n",
        "\n",
        "    for experiment in tqdm(answers):\n",
        "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
        "            continue\n",
        "\n",
        "        eval_prompt = evaluation_prompt_template.format_messages(\n",
        "            instruction=experiment[\"question\"],\n",
        "            response=experiment[\"generated_answer\"],\n",
        "            reference_answer=experiment[\"true_answer\"],\n",
        "        )\n",
        "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
        "        feedback, score = [\n",
        "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
        "        ]\n",
        "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
        "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
        "\n",
        "        with open(answer_path, \"w\") as f:\n",
        "            json.dump(answers, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXH-szLe9jVP"
      },
      "source": [
        "üöÄ Let's run the tests and evaluate answers!üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW2nnvUT9jVQ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./output\"):\n",
        "    os.mkdir(\"./output\")\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [True, False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(f\"Running evaluation for {settings_name}:\")\n",
        "\n",
        "            print(\"Loading knowledge base embeddings...\")\n",
        "            knowledge_index = load_embeddings(\n",
        "                RAW_KNOWLEDGE_BASE,\n",
        "                chunk_size=chunk_size,\n",
        "                embedding_model_name=embeddings,\n",
        "            )\n",
        "\n",
        "            print(\"Running RAG...\")\n",
        "            reranker = (\n",
        "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "                if rerank\n",
        "                else None\n",
        "            )\n",
        "            run_rag_tests(\n",
        "                eval_dataset=eval_dataset,\n",
        "                llm=READER_LLM,\n",
        "                knowledge_index=knowledge_index,\n",
        "                output_file=output_file_name,\n",
        "                reranker=reranker,\n",
        "                verbose=False,\n",
        "                test_settings=settings_name,\n",
        "            )\n",
        "\n",
        "            print(\"Running evaluation...\")\n",
        "            evaluate_answers(\n",
        "                output_file_name,\n",
        "                eval_chat_model,\n",
        "                evaluator_name,\n",
        "                evaluation_prompt_template,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tytXV5-h9jVT"
      },
      "source": [
        "### Inspect results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4YDSfmr9jVT"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "outputs = []\n",
        "for file in glob.glob(\"./output/*.json\"):\n",
        "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
        "    output[\"settings\"] = file\n",
        "    outputs.append(output)\n",
        "result = pd.concat(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdkXMNvS9jVT"
      },
      "outputs": [],
      "source": [
        "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(\n",
        "    lambda x: int(x) if isinstance(x, str) else 1\n",
        ")\n",
        "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgxBpid29jVT",
        "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "settings\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json       0.884328\n",
              "./output/rag_chunk:200_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader-model:zephyr-7b-beta.json    0.906716\n",
              "./output/rag_chunk:200_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader-model:zephyr-7b-beta.json     0.906716\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mixtral.json               0.906716\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json        0.921642\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mixtral0.json              0.947761\n",
              "Name: eval_score_GPT4, dtype: float64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
        "average_scores.sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSPH9DYI9jVT"
      },
      "source": [
        "## Example results\n",
        "\n",
        "Let us load the results that I obtained by tweaking the different options available in this notebook.\n",
        "For more detail on why these options could work on not, see the notebook on [advanced_RAG](advanced_rag).\n",
        "\n",
        "As you can see in the graph below, some tweaks do not bring any improvement, some give huge performance boosts.\n",
        "\n",
        "‚û°Ô∏è ___There is no single good recipe: you should try several different directions when tuning your RAG systems.___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVOxatv99jVT"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
        "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqK0Dg2Q9jVT"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(\n",
        "    scores,\n",
        "    color=scores,\n",
        "    labels={\n",
        "        \"value\": \"Accuracy\",\n",
        "        \"settings\": \"Configuration\",\n",
        "    },\n",
        "    color_continuous_scale=\"bluered\",\n",
        ")\n",
        "fig.update_layout(w\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    barmode=\"group\",\n",
        "    yaxis_range=[0, 100],\n",
        "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
        "    xaxis_title=\"RAG settings\",\n",
        "    font=dict(size=15),\n",
        ")\n",
        "fig.layout.yaxis.ticksuffix = \"%\"\n",
        "fig.update_coloraxes(showscale=False)\n",
        "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPUOMWGk9jVT"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_settings_accuracy.png\" height=\"500\" width=\"800\">\n",
        "\n",
        "As you can see, these had varying impact on performance. In particular, tuning the chunk size is both easy and very impactful.\n",
        "\n",
        "But this is our case: your results could be very different: now that you have a robust evaluation pipeline, you can set on to explore other options! üó∫Ô∏è"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
