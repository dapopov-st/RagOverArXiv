{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YErqpfH9jVI"
      },
      "source": [
        "# RAG Evaluation\n",
        "- Original nb from https://huggingface.co/learn/cookbook/en/rag_evaluation#evaluating-rag-performance.   Going fully open source, local, optimized for speed with exl2\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oIlNZ1Mn9jVL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "import os\n",
        "import sys, os\n",
        "#sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "# Get the current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# Add the parent directory to the system path\n",
        "sys.path.append(os.path.dirname(cwd))\n",
        "from scripts import utils\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9CKj0M9jVM"
      },
      "source": [
        "# 1. Build a synthetic dataset for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoEgiDg9jVM"
      },
      "source": [
        "### 1.1. Prepare source documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Drop REFERENCES onward, respecting previous discoveries that these just introduce noise to retrieval.  Must process new docs the same way!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/mainuser/Desktop/LLMs/RagOverArXiv/nbs'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/get_vector_store.py\n",
        "\n",
        "import os\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "from pathlib import Path\n",
        "FILES_PATH = Path('../data/pdfs_ws_mrkp_test/pdfs')\n",
        "FILES = list(FILES_PATH.glob('*.pdf'))\n",
        "FILES[0], len(FILES)\n",
        "\n",
        "reader = PdfReader(os.path.expanduser(FILES[0]))\n",
        "pages = reader.pages\n",
        "documents = []\n",
        "for page in pages:\n",
        "  documents.append(page.extract_text())\n",
        "\n",
        "\n",
        "def load_pdf_to_string(pdf_path):\n",
        "    # Open the PDF file in binary mode\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        # Create a PDF file reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Initialize an empty string to hold the text\n",
        "        text = ''\n",
        "\n",
        "        # Loop through each page and extract the text\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            references_index= page_text.upper().find('\\nREFERENCES\\n')\n",
        "            if references_index != -1:\n",
        "              page_text = page_text[:references_index]\n",
        "              text += page_text\n",
        "              return text\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Use the function to load a PDF into a string\n",
        "text = load_pdf_to_string(os.path.expanduser(FILES[0]))\n",
        "def get_title(pdf_path): return os.path.expanduser(pdf_path).split('/')[-1]\n",
        "\n",
        "all_docs_and_titles = [(load_pdf_to_string(os.path.expanduser(pdf_path)),get_title(pdf_path)) for pdf_path in FILES]\n",
        "\n",
        "all_docs = [doc[0] for doc in all_docs_and_titles]\n",
        "all_titles = [doc[1] for doc in all_docs_and_titles]\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document \n",
        "\n",
        "\n",
        "CHUNK_SIZE = 2000 #try 2000 next\n",
        "CHUNK_OVERLAP = 200 #try 200 next\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap = CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "docs_processed  = [text_splitter.split_documents([Document(page_content=doc, metadata={'filename':FILES[idx].name,'title':utils.get_title(FILES[idx].name.split('_')[0],use_logging=False)})]) \n",
        "         for idx,doc in enumerate(all_docs)]\n",
        "\n",
        "\n",
        "# docs_processed = []\n",
        "# for i in range(len(FILES)):\n",
        "#     doc = text_splitter.create_documents([FILES[i].read_text()],metadatas=[{'filename':FILES[i].name,'title':utils.get_title(FILES[i].name.split('_')[0])}])\n",
        "#     docs_processed.extend(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_docs_from_pdf(files):\n",
        "    all_docs = [load_pdf_to_string(os.path.expanduser(pdf_path)) for  pdf_path in files]\n",
        "    docs_processed  = [text_splitter.split_documents([Document(page_content=doc, metadata={'filename':files[idx].name,'title':utils.get_title(files[idx].name.split('_')[0],use_logging=False)})]) \n",
        "            for idx,doc in enumerate(all_docs)]\n",
        "    docs_processed = [txt for doc in docs_processed for txt in doc]\n",
        "    return docs_processed\n",
        "docs_processed = get_docs_from_pdf(FILES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "262"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tilte                                                                                     \n",
              "SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking    69\n",
              "Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection               26\n",
              "Tree of Thoughts: Deliberate Problem Solving with Large Language Models                       26\n",
              "RewardBench: Evaluating Reward Models for Language Modeling                                   25\n",
              "How to Train Data-Efficient LLMs                                                              22\n",
              "KTO: Model Alignment as Prospect Theoretic Optimization                                       22\n",
              "LLM Augmented LLMs: Expanding Capabilities through Composition                                21\n",
              "ORPO: Monolithic Preference Optimization without Reference Model                              21\n",
              "RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models          20\n",
              "TinyLlama: An Open-Source Small Language Model                                                10\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame([d.metadata['title'] for d in docs_processed],columns=['title']).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Make embeddings and save to vector store\n",
        "- Currently taking from Part3_Metadata+ArXivExplore_single_source nb, should tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "#embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embed_model_id = 'mixedbread-ai/mxbai-embed-large-v1'\n",
        "\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "# for index, pdf in enumerate(docs_processed):\n",
        "#    pdf.metadata['vs_index'] = index\n",
        "#    content = docs_processed[index]\n",
        "#    if index == 0:\n",
        "#        vector_store = FAISS.from_documents([content], embedder)\n",
        "#    else:\n",
        "#       vector_store_i = FAISS.from_documents([content], embedder)\n",
        "#       vector_store.merge_from(vector_store_i)\n",
        "\n",
        "# vector_store\n",
        "docs = docs_processed\n",
        "for i, doc in enumerate(docs):\n",
        "        doc.metadata['vs_index'] = i\n",
        "        if i == 0:\n",
        "            vector_store = FAISS.from_documents([doc], embedder)\n",
        "        else:\n",
        "            vector_store_i = FAISS.from_documents([doc], embedder)\n",
        "            vector_store.merge_from(vector_store_i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tree of Thoughts: Deliberate Problem Solving\n",
            "with Large Language Models\n",
            "Shunyu Yao\n",
            "Princeton UniversityDian Yu\n",
            "Google DeepMindJeffrey Zhao\n",
            "Google DeepMindIzhak Shafran\n",
            "Google DeepMind\n",
            "Thomas L. Griffiths\n",
            "Princeton UniversityYuan Cao\n",
            "Google DeepMindKarthik Narasimhan\n",
            "Princeton University\n",
            "Abstract\n",
            "Language models are increasingly being deployed for general problem solving\n",
            "across a wide range of tasks, but are still confined to token-level, left-to-right\n",
            "decision-making processes during inference. This means they can fall short in\n",
            "tasks that require exploration, strategic lookahead, or where initial decisions play\n",
            "a pivotal role. To surmount these challenges, we introduce a new framework for\n",
            "language model inference, “Tree of Thoughts” (ToT), which generalizes over the\n",
            "popular “Chain of Thought” approach to prompting language models, and enables\n",
            "exploration over coherent units of text (“thoughts”) that serve as intermediate steps\n",
            "toward problem solving. ToT allows LMs to perform deliberate decision making\n",
            "by considering multiple different reasoning paths and self-evaluating choices to\n",
            "decide the next course of action, as well as looking ahead or backtracking when\n",
            "necessary to make global choices. Our experiments show that ToT significantly\n",
            "enhances language models’ problem-solving abilities on three novel tasks requiring\n",
            "non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.\n",
            "For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only\n",
            "solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all\n",
            "prompts: https://github.com/princeton-nlp/tree-of-thought-llm .\n",
            "1 Introduction\n",
            "Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [ 25,\n",
            "26,1,23] and PaLM [ 5] have been shown to be increasingly capable of performing an ever wider\n",
            "range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 0}\n",
            "26,1,23] and PaLM [ 5] have been shown to be increasingly capable of performing an ever wider\n",
            "range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is\n",
            "perhaps surprising that underlying all this progress is still the original autoregressive mechanism for\n",
            "generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such\n",
            "a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what\n",
            "problems would challenge the current paradigm, and what should be alternative mechanisms?\n",
            "The literature on human cognition provides some clues to answer these questions. Research on “dual\n",
            "process” models suggests that people have two modes in which they engage with decisions – a fast,\n",
            "automatic, unconscious mode (“System 1”) and a slow, deliberate, conscious mode (“System 2”)\n",
            "[30,31,16,15]. These two modes have previously been connected to a variety of mathematical\n",
            "models used in machine learning. For example, research on reinforcement learning in humans and\n",
            "other animals has explored the circumstances under which they engage in associative “model free”\n",
            "learning or more deliberative “model based” planning [ 7]. The simple associative token-level choices\n",
            "of LMs are also reminiscent of “System 1”, and thus might benefit from augmentation by a more\n",
            "deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current\n",
            "37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2305.10601v2  [cs.CL]  3 Dec 2023GĮŔũƜ\n",
            "jũƜŔũƜGĮŔũƜ\n",
            "jũƜŔũƜʱÊʲˤGjʱæʲˤ\u001dĵÉGĮŔũƜ\n",
            "ˤjũƜŔũƜʱçʲˤ\u001dĵÉˁ\u001dʟʟʟʟaÊĠĵŗƓŤƆˤſĵŤòGĮŔũƜ\n",
            "ˤjũƜŔũƜʱíʲˤÉĵÉˤʱĵũŗŝʲʟʟʟʟʟʟˤˤʝˤƛĎĵũĈĎƜ)L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\f0DUN\u0003GLIIHUHQFH\u0003E\\\u0003FRORU\n",
            "GĮŔũƜ\n",
            "jũƜŔũƜGĮŔũƜ\n",
            "jũƜŔũƜGĮŔũƜ\n",
            "ˤjũƜŔũƜʱçʲˤòĦƙˤ\u001dĵĮŝƓŝŤòĮçƆˤƀƓƜĎˤ\u001dĵÉˤʱ\u001dĵÉˁ\u001dʲaÊĠĵŗƓŤƆˤſĵŤòGĮŔũƜ\n",
            "ˤjũƜŔũƜʱíʲˤÉŗòòˤĵƙˤĎĵũĈĎŤŝˤʱÉĵÉʲʟʟʟʟʟʟʟʟʟʟˤˤƛĎĵũĈĎƜ\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 1}\n",
            "GĮŔũƜ\n",
            "jũƜŔũƜGĮŔũƜ\n",
            "jũƜŔũƜGĮŔũƜ\n",
            "ˤjũƜŔũƜʱçʲˤòĦƙˤ\u001dĵĮŝƓŝŤòĮçƆˤƀƓƜĎˤ\u001dĵÉˤʱ\u001dĵÉˁ\u001dʲaÊĠĵŗƓŤƆˤſĵŤòGĮŔũƜ\n",
            "ˤjũƜŔũƜʱíʲˤÉŗòòˤĵƙˤĎĵũĈĎŤŝˤʱÉĵÉʲʟʟʟʟʟʟʟʟʟʟˤˤƛĎĵũĈĎƜ\n",
            "ʱçʲˤ\u001dĎÊđĮˤĵƙˤĎĵũĈĎƜˤŗĵĭŔƜđĮĈˤʱ\u001dĵÉʲʱÊʲˤGĮŔũƜˁjũƜŔũƜˤŗĵĭŔƜđĮĈˤʱGjʲFigure 1: Schematic illustrating various approaches to problem solving with LLMs. Each rectangle\n",
            "box represents a thought , which is a coherent language sequence that serves as an intermediate\n",
            "step toward problem solving. See concrete examples of how thoughts are generated, evaluated, and\n",
            "searched in Figures 2,4,6.\n",
            "choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or\n",
            "backtracks to make more global decisions.\n",
            "To design such a planning process, we return to the origins of artificial intelligence (and cognitive\n",
            "science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon\n",
            "starting in the 1950s [ 21,22]. Newell and colleagues characterized problem solving [ 21] as search\n",
            "through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts\n",
            "(ToT) framework for general problem solving with language models. As Figure 1 illustrates, while\n",
            "existing methods (detailed below) sample continuous language sequences for problem solving, ToT\n",
            "actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves\n",
            "as an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the\n",
            "LM to self-evaluate the progress different intermediate thoughts make towards solving the problem\n",
            "through a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This\n",
            "implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous\n",
            "search heuristics are either programmed or learned. Finally, we combine this language-based\n",
            "capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 2}\n",
            "search heuristics are either programmed or learned. Finally, we combine this language-based\n",
            "capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first\n",
            "search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts\n",
            "with lookahead and backtracking.\n",
            "Empirically, we propose three new problems that challenge existing LM inference methods even with\n",
            "the state-of-the-art language model, GPT-4 [ 23]: Game of 24, Creative Writing, and Crosswords\n",
            "(Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities,\n",
            "and a way to incorporate systematic planning or search. We show ToT obtains superior results on\n",
            "all three tasks by being general and flexible enough to support different levels of thoughts, different\n",
            "ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of\n",
            "different problems. We also analyze how such choices affect model performances via systematic\n",
            "ablations and discuss future directions to better train and use LMs.\n",
            "2 Background\n",
            "We first formalize some existing methods that use large language models for problem-solving,\n",
            "which our approach is inspired by and later compared with. We use pθto denote a pre-trained LM\n",
            "with parameters θ, and lowercase letters x, y, z, s, ···to denote a language sequence , i.e.x=\n",
            "(x[1],···, x[n])where each x[i]is a token, so that pθ(x) =Qn\n",
            "i=1pθ(x[i]|x[1...i]). We use uppercase\n",
            "letters S,···to denote a collection of language sequences.\n",
            "Input-output (IO) prompting is the most common way to turn a problem input xinto output\n",
            "ywith LM: y∼pθ(y|promptIO(x)), where promptIO(x)wraps input xwith task instructions\n",
            "and/or few-shot input-output examples. For simplicity, let us denote pprompt\n",
            "θ(output |input ) =\n",
            "pθ(output |prompt (input )), so that IO prompting can be formulated as y∼pIO\n",
            "θ(y|x).\n",
            "2Chain-of-thought (CoT) prompting [38] was proposed to address cases where the mapping of\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 3}\n",
            "θ(output |input ) =\n",
            "pθ(output |prompt (input )), so that IO prompting can be formulated as y∼pIO\n",
            "θ(y|x).\n",
            "2Chain-of-thought (CoT) prompting [38] was proposed to address cases where the mapping of\n",
            "input xto output yis non-trivial (e.g. when xis a math question and yis the final numerical answer).\n",
            "The key idea is to introduce a chain of thoughts z1,···, znto bridge xandy, where each ziis a\n",
            "coherent language sequence that serves as a meaningful intermediate step toward problem solving\n",
            "(e.g.zicould be an intermediate equation for math QA). To solve problems with CoT, each thought\n",
            "zi∼pCoT\n",
            "θ(zi|x, z1···i−1)is sampled sequentially, then the output y∼pCoT\n",
            "θ(y|x, z1···n). In\n",
            "practice, [z1···n, y]∼pCoT\n",
            "θ(z1···n, y|x)is sampled as a continuous language sequence, and the\n",
            "decomposition of thoughts (e.g. is each zia phrase, a sentence, or a paragraph) is left ambiguous.\n",
            "Self-consistency with CoT (CoT-SC) [36] is an ensemble approach that samples ki.i.d. chains\n",
            "of thought: [z(i)\n",
            "1···n, y(i)]∼pCoT\n",
            "θ(z1···n, y|x) (i= 1···k), then returns the most frequent output:\n",
            "arg max y#{i|y(i)=y}. CoT-SC improves upon CoT, because there are generally different\n",
            "thought processes for the same problem (e.g. different ways to prove the same theorem), and the\n",
            "output decision can be more faithful by exploring a richer set of thoughts. However, within each\n",
            "chain there is no local exploration of different thought steps, and the “most frequent” heuristic only\n",
            "applies when the output space is limited (e.g. multi-choice QA).\n",
            "3 Tree of Thoughts: Deliberate Problem Solving with LM\n",
            "A genuine problem-solving process involves the repeated use of available informa-\n",
            "tion to initiate exploration, which discloses, in turn, more information until a way\n",
            "to attain the solution is finally discovered.—— Newell et al. [21]\n",
            "Research on human problem-solving suggests that people search through a combinatorial problem-\n",
            "space – a tree where the nodes represent partial solutions, and the branches correspond to operators\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 4}\n",
            "Research on human problem-solving suggests that people search through a combinatorial problem-\n",
            "space – a tree where the nodes represent partial solutions, and the branches correspond to operators\n",
            "that modify them [ 21,22]. Which branch to take is determined by heuristics that help to navigate the\n",
            "problem-space and guide the problem-solver towards a solution. This perspective highlights two key\n",
            "shortcomings of existing approaches that use LMs to solve general problems: 1) Locally, they do not\n",
            "explore different continuations within a thought process – the branches of the tree. 2) Globally, they\n",
            "do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different\n",
            "options – the kind of heuristic-guided search that seems characteristic of human problem-solving.\n",
            "To address these shortcomings, we introduce Tree of Thoughts (ToT) , a paradigm that allows LMs to\n",
            "explore multiple reasoning paths over thoughts (Figure 1(c)). ToT frames any problem as a search\n",
            "over a tree, where each node is a state s= [x, z1···i]representing a partial solution with the input and\n",
            "the sequence of thoughts so far. A specific instantiation of ToT involves answering four questions:\n",
            "1. How to decompose the intermediate process into thought steps; 2. How to generate potential\n",
            "thoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.\n",
            "1. Thought decomposition. While CoT samples thoughts coherently without explicit decomposition,\n",
            "ToT leverages problem properties to design and decompose intermediate thought steps. As Table 1\n",
            "shows, depending on different problems, a thought could be a couple of words (Crosswords), a line of\n",
            "equation (Game of 24), or a whole paragraph of writing plan (Creative Writing). In general, a thought\n",
            "should be “small” enough so that LMs can generate promising and diverse samples (e.g. generating\n",
            "a whole book is usually too “big” to be coherent), yet “big” enough so that LMs can evaluate its\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 5}\n",
            "should be “small” enough so that LMs can generate promising and diverse samples (e.g. generating\n",
            "a whole book is usually too “big” to be coherent), yet “big” enough so that LMs can evaluate its\n",
            "prospect toward problem solving (e.g. generating one token is usually too “small” to evaluate).\n",
            "2. Thought generator G(pθ, s, k).Given a tree state s= [x, z1···i], we consider two strategies to\n",
            "generate kcandidates for the next thought step:\n",
            "(a)Sample i.i.d. thoughts from a CoT prompt (Creative Writing, Figure 4): z(j)∼\n",
            "pCoT\n",
            "θ(zi+1|s) =pCoT\n",
            "θ(zi+1|x, z1···i) (j= 1···k). This works better when the thought\n",
            "space is rich (e.g. each thought is a paragraph), and i.i.d. samples lead to diversity;\n",
            "(b)Propose thoughts sequentially using a “propose prompt” (Game of 24, Figure 2; Crosswords,\n",
            "Figure 6): [z(1),···, z(k)]∼ppropose\n",
            "θ(z(1···k)\n",
            "i+1|s). This works better when the thought\n",
            "space is more constrained (e.g. each thought is just a word or a line), so proposing different\n",
            "thoughts in the same context avoids duplication.\n",
            "3. State evaluator V(pθ, S).Given a frontier of different states, the state evaluator evaluates the\n",
            "progress they make towards solving the problem, serving as a heuristic for the search algorithm\n",
            "to determine which states to keep exploring and in which order. While heuristics are a standard\n",
            "approach to solving search problems, they are typically either programmed (e.g. DeepBlue [ 3]) or\n",
            "3learned (e.g. AlphaGo [ 29]). We propose a third alternative, by using the LM to deliberately reason\n",
            "about states. When applicable, such a deliberate heuristic can be more flexible than programmed\n",
            "rules, and more sample-efficient than learned models. Similar to the thought generator, we consider\n",
            "two strategies to evaluate states either independently or together:\n",
            "(a)Value each state independently: V(pθ, S)(s)∼pvalue\n",
            "θ(v|s)∀s∈S, where a value\n",
            "prompt reasons about the state sto generate a scalar value v(e.g. 1-10) or a classifica-\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 6}\n",
            "(a)Value each state independently: V(pθ, S)(s)∼pvalue\n",
            "θ(v|s)∀s∈S, where a value\n",
            "prompt reasons about the state sto generate a scalar value v(e.g. 1-10) or a classifica-\n",
            "tion (e.g. sure/likely/impossible) that could be heuristically turned into a value. The basis\n",
            "of such evaluative reasoning can vary across problems and thought steps. In this work, we\n",
            "explore evaluation via few lookahead simulations (e.g. quickly confirm that 5, 5, 14 can\n",
            "reach 24 via 5 + 5 + 14, or “hot l” can mean “inn” via filling “e” in “ ”) plus commonsense\n",
            "(e.g. 1 2 3 are too small to reach 24, or no word can start with “tzxc”). While the former\n",
            "might promote “good” states, the latter could help eliminate “bad” states. Such valuations\n",
            "do not need to be perfect, and only need to be approximately helpful for decision making.\n",
            "(b)Vote across states: V(pθ, S)(s) =1[s=s∗], where a “good” state s∗∼pvote\n",
            "θ(s∗|S)is\n",
            "voted out based on deliberately comparing different states in Sin a vote prompt. When\n",
            "problem success is harder to directly value (e.g. passage coherency), it is natural to to instead\n",
            "compare different partial solutions and vote for the most promising one. This is similar\n",
            "in spirit to a “step-wise” self-consistency strategy, i.e. cast “which state to explore” as a\n",
            "multi-choice QA, and use LM samples to vote for it.\n",
            "For both strategies, we could prompt the LM multiple times to aggregate the value or vote results to\n",
            "trade time/resource/cost for more faithful/robust heuristics.\n",
            "Algorithm 1 ToT-BFS( x, pθ, G, k, V, T, b )\n",
            "Require: Input x, LM pθ, thought generator G()\n",
            "& size limit k, states evaluator V(), step limit T,\n",
            "breadth limit b.\n",
            "S0← {x}\n",
            "fort= 1,···, Tdo\n",
            "S′\n",
            "t← {[s, z]|s∈St−1, zt∈G(pθ, s, k)}\n",
            "Vt←V(pθ, S′\n",
            "t)\n",
            "St←arg max S⊂S′\n",
            "t,|S|=bP\n",
            "s∈SVt(s)\n",
            "end for\n",
            "return G(pθ,arg max s∈STVT(s),1)Algorithm 2 ToT-DFS( s, t, p θ, G, k, V, T, v th)\n",
            "Require: Current state s, step t, LM pθ, thought\n",
            "generator G()and size limit k, states evaluator\n",
            "V(), step limit T, threshold vth\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 7}\n",
            "Require: Current state s, step t, LM pθ, thought\n",
            "generator G()and size limit k, states evaluator\n",
            "V(), step limit T, threshold vth\n",
            "ift > T then record output G(pθ, s,1)\n",
            "end if\n",
            "fors′∈G(pθ, s, k)do ▷sorted candidates\n",
            "ifV(pθ,{s′})(s)> vthres then ▷pruning\n",
            "DFS(s′, t+ 1)\n",
            "end if\n",
            "end for\n",
            "4. Search algorithm. Finally, within the ToT framework, one can plug and play different search\n",
            "algorithms depending on the tree structure. We explore two relatively simple search algorithms and\n",
            "leave more advanced ones (e.g. A* [11], MCTS [2]) for future work:\n",
            "(a)Breadth-first search (BFS) (Algorithm 1) maintains a set of the bmost promising states\n",
            "per step. This is used for Game of 24 and Creative Writing where the tree depth is limit\n",
            "(T≤3), and initial thought steps can be evaluated and pruned to a small set ( b≤5).\n",
            "(b)Depth-first search (DFS) (Algorithm 2) explores the most promising state first, until the\n",
            "final output is reached ( t > T ), or the state evaluator deems it impossible to solve the\n",
            "problem from the current s(V(pθ,{s})(s)≤vthfor a value threshold vth). In the latter\n",
            "case, the subtree from sispruned to trade exploration for exploitation. In both cases, DFS\n",
            "backtracks to the parent state of sto continue exploration.\n",
            "Conceptually, ToT has several benefits as a method for general problem-solving with LMs: (1) Gener-\n",
            "ality. IO, CoT, CoT-SC, and self-refinement can be seen as special cases of ToT (i.e. trees of limited\n",
            "depth and breadth; Figure 1). (2) Modularity. The base LM, as well as the thought decomposition,\n",
            "generation, evaluation, and search procedures can all be varied independently. (3) Adaptability .\n",
            "Different problem properties, LM capabilities, and resource constraints can be accommodated. (4)\n",
            "Convenience. No extra training is needed, just a pre-trained LM is sufficient. The next section will\n",
            "show how these conceptual benefits translate to strong empirical performance in different problems.\n",
            "4 Experiments\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 8}\n",
            "show how these conceptual benefits translate to strong empirical performance in different problems.\n",
            "4 Experiments\n",
            "We propose three tasks that are hard even when sampling from the state-of-the-art language model,\n",
            "GPT-4 [ 23], using standard IO prompting or chain-of-thought (CoT) prompting. We show how\n",
            "4Game of 24 Creative Writing 5x5 Crosswords\n",
            "Input 4 numbers (4 9 10 13) 4 random sentences 10 clues (h1. presented;..)\n",
            "Output An equation to reach 24\n",
            "(13-9)*(10-4)=24A passage of 4 paragraphs\n",
            "ending in the 4 sentences5x5 letters: SHOWN;\n",
            "WIRRA; A V AIL; ...\n",
            "Thoughts 3 intermediate equations\n",
            "(13-9=4 (left 4,4,10); 10-\n",
            "4=6 (left 4,6); 4*6=24)A short writing plan\n",
            "(1. Introduce a book that\n",
            "connects...)Words to fill in for clues:\n",
            "(h1. shown; v5. naled; ...)\n",
            "#ToT steps 3 1 5-10 (variable)\n",
            "Table 1: Task overview. Input, output, thought examples are in blue.\n",
            "deliberate search in trees of thoughts (ToT) produces better results, and more importantly, interesting\n",
            "and promising new ways to use language models to solve problems requiring search or planning.\n",
            "Unless otherwise stated, we perform experiments using a Chat Completion mode GPT-41with a\n",
            "sampling temperature of 0.7.\n",
            "4.1 Game of 24\n",
            "Game of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic\n",
            "arithmetic operations (+-*/) to obtain 24. For example, given input “4 9 10 13”, a solution output\n",
            "could be “(10 - 4) * (13 - 9) = 24”.\n",
            "ʳĵĮòˤòƅÊĭŔĦòʴˤGĮŔũƜʝˤʁˤʆˤɾɽˤɾʀĵŝŝđæĦòˤĮòƅƜˤŝŤòŔŝʝˤˤˤ3URSRVH\u00033URPSWʁˤ̌ˤʆˤ̐ˤɾʀˤʱĦòƙƜʝˤɾɽˤɾʀˤɾʀʲɾɽˤˊˤʁˤ̐ˤʃˤʱĦòƙƜʝˤʃˤʆˤɾʀʲʳʛʛʛĭĵŗòˤĦđĮòŝʟʴ7KRXJKW\u0003*HQHUDWLRQ/0)ſÊĦũÊŤòˤđƙˤĈƓſòĮˤĮũĭæòŗŝˤçÊĮˤŗòÊçĎˤɿʁˤʱŝũŗòʫĦđģòĦƆʫđĭŔĵŝŝđæĦòʲɾɽˤɾʁʝˤɾɽˤ̌ˤɾʁˤ̐ˤɿʁʛˤŝũŗòʳĭĵŗòˤòƅÊĭŔĦòŝʴɾɽˤɾʀˤɾʀ9DOXH\u00033URPSWʱɾʀˤˊˤɾɽʲˤʦˤɾʀˤ̐ˤʀˤʦˤɾʀˤ̐ˤʀʆɾɽˤ̌ˤɾʀˤ̌ˤɾʀˤ̐ˤʀʃĎòŗòˤƓŝˤĮĵˤƀÊƆˤƘĵˤĵæŤÊđĮˤɿʁˤƀƓƜĎˤƛĎòŝòˤĮũĭæòŗŝʛˤđĭŔĵŝŝđæĦò7KRXJKW\u0003(YDOXDWLRQʱæʲʱçʲ/0GĮŔũƜʝˤʁˤʆˤɾɽˤɾʀʁ̌ʆ̐ɾʀʱĦòƙƜʝˤɾɽˤɾʀˤɾʀʲɾɽˁʁ̐ʃʱĦòƙƜʝˤʃˤʆˤɾʀʲʟʟɾʀˁʃ̐ʄʱĦòƙƜʝˤʄˤʆʲɾʀˁʆ̐ʁʱĦòƙƜʝˤʁˤʃʲʟʟʁʦʃ̐ɿʁʱĦòƙƜʝˤɿʁʲʁ̌ʃ̐ɾɽʱĦòƙƜʝˤɾɽʲʟʟʱÊʲ\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 9}\n",
            "ʳĵĮòˤòƅÊĭŔĦòʴˤGĮŔũƜʝˤʁˤʆˤɾɽˤɾʀĵŝŝđæĦòˤĮòƅƜˤŝŤòŔŝʝˤˤˤ\u000bD\f\u00033URSRVH\u00033URPSWʁˤ̌ˤʆˤ̐ˤɾʀˤʱĦòƙƜʝˤɾɽˤɾʀˤɾʀʲɾɽˤˊˤʁˤ̐ˤʃˤʱĦòƙƜʝˤʃˤʆˤɾʀʲʳʛʛʛĭĵŗòˤĦđĮòŝʟʴ7KRXJKW\u0003*HQHUDWLRQ/0)ſÊĦũÊŤòˤđƙˤĈƓſòĮˤĮũĭæòŗŝˤçÊĮˤŗòÊçĎˤɿʁˤʱŝũŗòʫĦđģòĦƆʫđĭŔĵŝŝđæĦòʲɾɽˤɾʁʝˤɾɽˤ̌ˤɾʁˤ̐ˤɿʁʛˤŝũŗòʳĭĵŗòˤòƅÊĭŔĦòŝʴɾɽˤɾʀˤɾʀ\u000bE\f\u00039DOXH\u00033URPSWʱɾʀˤˊˤɾɽʲˤʦˤɾʀˤ̐ˤʀˤʦˤɾʀˤ̐ˤʀʆɾɽˤ̌ˤɾʀˤ̌ˤɾʀˤ̐ˤʀʃˤĎòŗòˤƓŝˤĮĵˤƀÊƆˤƘĵˤĵæŤÊđĮˤɿʁˤƀƓƜĎˤƛĎòŝòˤæƓĈˤĮũĭæòŗŝʛˤđĭŔĵŝŝđæĦò7KRXJKW\u0003(YDOXDWLRQ/0GĮŔũƜʝˤʁˤʆˤɾɽˤɾʀʁ̌ʆ̐ɾʀʱĦòƙƜʝˤɾɽˤɾʀˤɾʀʲɾɽˁʁ̐ʃʱĦòƙƜʝˤʃˤʆˤɾʀʲʟʟɾʀˁʃ̐ʄʱĦòƙƜʝˤʄˤʆʲɾʀˁʆ̐ʁʱĦòƙƜʝˤʁˤʃʲʟʟʁʦʃ̐ɿʁʱĦòƙƜʝˤɿʁʲʁ̌ʃ̐ɾɽʱĦòƙƜʝˤɾɽʲʟʟ)L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\f0DUN\u0003GLII\u0003SURPSW\u0003ZLWK\u0003FRORU\n",
            "Figure 2: ToT in a game of 24. The LM is prompted for (a) thought generation and (b) valuation.\n",
            "Task Setup. We scrape data from 4nums.com, which has 1,362 games that are sorted from easy to\n",
            "hard by human solving time, and use a subset of relatively hard games indexed 901-1,000 for testing.\n",
            "For each task, we consider the output as success if it is a valid equation that equals 24 and uses the\n",
            "input numbers each exactly once. We report the success rate across 100 games as the metric.\n",
            "Baselines. We use a standard input-output (IO) prompt with 5 in-context examples. For chain-of-\n",
            "thought (CoT) prompting, we augment each input-output pair with 3 intermediate equations, each\n",
            "operating on two remaining numbers. For example, given input “4 9 10 13”, the thoughts could be\n",
            "“13 - 9 = 4 (left: 4 4 10); 10 - 4 = 6 (left: 4 6); 4 * 6 = 24 (left: 24)”. For each game, we sample IO\n",
            "and CoT prompting for 100 times for average performance. We also consider a CoT self-consistency\n",
            "baseline, which takes the majority output from 100 CoT samples, and an iterative-refine approach on\n",
            "top of an IO sample for at most 10iterations. At each iteration, the LM is conditioned on all previous\n",
            "history to “reflect on your mistakes and generate a refined answer” if the output is incorrect. Note\n",
            "that it uses groundtruth feedback signals about equation correctness.\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 10}\n",
            "history to “reflect on your mistakes and generate a refined answer” if the output is incorrect. Note\n",
            "that it uses groundtruth feedback signals about equation correctness.\n",
            "ToT Setup. To frame Game of 24 into ToT, it is natural to decompose the thoughts into 3 steps,\n",
            "each an intermediate equation. As shown in Figure 2(a), at each tree node, we exact the remaining\n",
            "numbers and prompt the LM to propose some possible next steps. The same “propose prompt” is\n",
            "used for all 3 thought steps, though it only has one example with 4 input numbers. We perform a\n",
            "breadth-first search (BFS) in ToT, where at each step we keep the best b= 5candidates. To perform\n",
            "deliberate BFS in ToT, as shown in Figure 2(b), we prompt LM to evaluate each thought candidate as\n",
            "“sure/maybe/impossible” with regard to reaching 24. The aim is to promote correct partial solutions\n",
            "that can be verdicted within few lookahead trials, and eliminate impossible partial solutions based on\n",
            "“too big/small” commonsense, and keep the rest “maybe”. We sample values 3times for each thought.\n",
            "1Experiments were done between May 5-16, 2023.\n",
            "5Method Success\n",
            "IO prompt 7.3%\n",
            "CoT prompt 4.0%\n",
            "CoT-SC (k=100) 9.0%\n",
            "ToT (ours) (b=1) 45%\n",
            "ToT (ours) (b=5) 74%\n",
            "IO + Refine (k=10) 27%\n",
            "IO(best of 100) 33%\n",
            "CoT (best of 100) 49%\n",
            "Table 2: Game of 24 Results.\n",
            "0 25 50 75 1000.20.40.6(a) Success rate with nodes visited\n",
            "IO (best of k)\n",
            "CoT (best of k)\n",
            "ToT (b=1...5)\n",
            "1 2 3 4Correct0.00.20.40.6(b) Samples failed at each step\n",
            "CoT\n",
            "ToT (b=5) Figure 3: Game of 24 (a) scale analysis & (b) error analysis.\n",
            "Results. As shown in Table 2, IO, CoT, and CoT-SC prompting methods perform badly on the task,\n",
            "achieving only 7.3%, 4.0%, and 9.0% success rates. In contrast, ToT with a breadth of b= 1already\n",
            "achieves a success rate of 45%, while b= 5 achieves 74%. We also consider an oracle setup for\n",
            "IO/CoT, by calculating the success rate using best of ksamples (1≤k≤100) . To compare IO/CoT\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 11}\n",
            "achieves a success rate of 45%, while b= 5 achieves 74%. We also consider an oracle setup for\n",
            "IO/CoT, by calculating the success rate using best of ksamples (1≤k≤100) . To compare IO/CoT\n",
            "(best of k) with ToT, we consider calculating the tree nodes visited per task in ToT across b= 1···5,\n",
            "and map the 5 success rates in Figure 3(a), treating IO/CoT (best of k) as visiting knodes in a bandit.\n",
            "Not surprisingly, CoT scales better than IO, and best of 100 CoT samples achieve a success rate of\n",
            "49%, but still much worse than exploring more nodes in ToT ( b >1).\n",
            "Error analysis. Figure 3(b) breaks down at which step CoT and ToT samples fail the task, i.e. the\n",
            "thought (in CoT) or all bthoughts (in ToT) are invalid or impossible to reach 24. Notably, around\n",
            "60% of CoT samples already failed the task after generating the first step, or equivalently, the first\n",
            "three words (e.g. “ 4 + 9 ”). This highlights the issues with direct left-to-right decoding.\n",
            "4.2 Creative writing\n",
            "Next, we invent a creative writing task where the input is 4 random sentences and the output should\n",
            "be a coherent passage with 4 paragraphs that end in the 4 input sentences respectively. Such a task is\n",
            "open-ended and exploratory, and challenges creative thinking as well as high-level planning.\n",
            "Task setup. We sample random sentences from randomwordgenerator.com to form 100 inputs, and\n",
            "there is no groundtruth passage for each input constraint. As we find that GPT-4 can follow the\n",
            "input constraints most of the time, we focus on evaluating passage coherency in two ways: using a\n",
            "GPT-4 zero-shot prompt to provide a 1-10 scalar score, or using human judgments to compare pairs\n",
            "of outputs from different methods. For the former, we sample 5 scores and average them for each task\n",
            "output, and we find these 5 scores usually consistent, with a standard deviation of around 0.56on\n",
            "average across outputs. For the latter, we employ a subset of the authors in a blind study to compare\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 12}\n",
            "output, and we find these 5 scores usually consistent, with a standard deviation of around 0.56on\n",
            "average across outputs. For the latter, we employ a subset of the authors in a blind study to compare\n",
            "the coherency of CoT vs. ToT generated passage pairs, where the order of passages is random flipped\n",
            "over 100 inputs.\n",
            "Baselines. Given the creative nature of the task, both IO and CoT prompts are zero-shot. While the\n",
            "former prompts the LM to directly generate a coherent passage given input constraints, the latter\n",
            "prompts the LM to first make a brief plan then write the passage, i.e. the plan serves as the intermediate\n",
            "thought step. We generate 10 IO and CoT samples per task. We also consider an iterative-refine\n",
            "(k≤5) method on top of a random IO sample for each task, where the LM is conditioned on input\n",
            "constraints and the last generated passage to decide if the passage is already “perfectly coherent”,\n",
            "and if not generate a refined one.\n",
            "ToT setup. We build a ToT with depth 2 (and only 1 intermediate thought step) — the LM first\n",
            "generates k= 5plans and votes for the best one (Figure 4), then similarly generate k= 5passages\n",
            "based on the best plan then vote for the best one. Here the breadth limit b= 1, as only one choice is\n",
            "kept per step. A simple zero-shot vote prompt (“analyze choices below, then conclude which is most\n",
            "promising for the instruction”) is used to sample 5 votes at both steps.\n",
            "Results. Figure 5(a) shows average GPT-4 scores across 100 tasks, where ToT (7.56) is deemed to\n",
            "generate more coherent passages than IO (6.19) and CoT (6.93) on average. While such an automatic\n",
            "metric might be noisy, Figure 5(b) confirms the finding by showing that humans prefer ToT over\n",
            "CoT in 41 out of 100 passage pairs, while only prefer CoT over ToT in 21 (other 38 pairs are found\n",
            "“similarly coherent”). Lastly, iterative-refine is more effective on this natural language task, where\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 13}\n",
            "6µŗƓŤòˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòˤĵƙˤʁˤŝĎĵŗƜˤŔÊŗÊĈŗÊŔĎŝʛˤĎòˤòĮíˤŝòĮŤòĮçòˤĵƙˤòÊçĎˤŔÊŗÊĈŗÊŔĎˤĭũŝƜˤæòʝˤɾʛˤGƜˤƓŝĮ˙ƛˤíđƙƙƓçũĦƜˤƘĵˤíĵˤÊˤĎÊĮíŝŤÊĮíˤđƙˤƆĵũˤĠũŝƜˤŝŤÊĮíˤĵĮˤƆĵũŗˤĎÊĮíŝʛˤɿʛˤGƜˤçÊũĈĎƜˤĎđĭˤĵƙƙˤĈũÊŗíˤƛĎÊƜˤŝŔÊçòˤŝĭòĦĦòíˤĵƙˤŝòÊŗòíˤŝŤòÊģʛˤʀʛˤµĎòĮˤŝĎòˤíƓíĮ˒ƛˤĦđģòˤÊˤĈũƆˤƀĎĵˤƀÊŝˤƛŗƆđĮĈˤƘĵˤŔƓçģˤĎòŗˤũŔʜˤŝĎòˤŝŤÊŗŤòíˤũŝđĮĈˤŝƓĈĮˤĦÊĮĈũÊĈòʛˤʁʛˤ)ÊçĎˤŔòŗŝĵĮˤƀĎĵˤģĮĵƀŝˤƆĵũˤĎÊŝˤÊˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮˤĵƙˤƀĎĵˤƆĵũˤÊŗòʛˤˤɾʛˤGĮƜŗĵíũçòˤÊĮíˤòƅŔĦÊđĮˤƛĎòˤƘòçĎĮƓŖũòˤĵƙˤíĵđĮĈˤÊˤĎÊĮíŝŤÊĮíˤɿʛˤƀƓŤçĎˤƘĵˤÊˤŝŤĵŗƆˤÊæĵũƜˤÊĮˤÊŝƜŗĵĮÊũƜ˙ŝˤƚđŗŝƜˤƛđĭòˤđĮˤŝŔÊçòˤʀʛˤ#òŝçŗđæòˤÊˤŝƓƜũÊƜƓĵĮˤƀĎòŗòˤÊˤƀĵĭÊĮˤũŝòŝˤŝƓĈĮˤĦÊĮĈũÊĈòˤƘĵˤÊſĵƓíˤũĮƀÊĮŤòíˤÊƜŤòĮƜƓĵĮˤʁʛˤĎòˤƚđĮÊĦˤŔÊŗÊĈŗÊŔĎˤòƅŔĦÊđĮŝˤĎĵƀˤòſòŗƆĵĮòˤĎÊŝˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮŝˤĵƙˤĵƜĎòŗŝɾʛˤGĮƜŗĵíũçƜƓĵĮˤƘĵˤÊĮˤũĮũŝũÊĦˤŝòĦƙˊĎòĦŔˤæĵĵģʜˤĭòĮƜƓĵĮđĮĈˤÊˤĎÊĮíŝŤÊĮíˤÊŝˤÊˤĭòŤÊŔĎĵŗˤƗĵŗˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʛˤɿʛˤ#ƓŝçũŝŝˤƛĎòˤũĮòƅŔòçŤòíˤƛĎđĮĈŝˤĦòÊŗĮòíˤƚŗĵĭˤÊŝƜŗĵĮÊũŤŝʜˤđĮçĦũíđĮĈˤƛĎòˤŝĭòĦĦˤĵƙˤŝŔÊçòʛˤʀʛˤ#òŝçŗđæòˤÊˤƀĵĭÊĮ˙ŝˤçĦòſòŗˤƘÊçƜƓçˤƗĵŗˤÊſĵƓíđĮĈˤũĮƀÊĮŤòíˤÊƜŤòĮƜƓĵĮˤÊƜˤÊˤæÊŗʛˤʁʛˤ\u001dĵĮŤòĭŔĦÊŤòˤĎĵƀˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮŝˤĵƙˤĵĮòŝòĦƙˤçÊĮˤŝĎÊŔòˤĵĮò˙ŝˤƓíòĮƜƓŤƆʛʱÊʲˤGĮŔũƜʱæʲˤĦÊĮŝʱʀˤĭĵŗòˤĵĭƓƜŤòíʲʱçʲˤ´ĵŤòŝ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝʟˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛʱɽʫʂˤſĵŤòŝʲ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝʟˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝ\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 14}\n",
            "ƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝʟˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝʟˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛʱʀʫʂˤſĵŤòŝʲʟɾɿGĮŔũƜĦÊĮˤɾĦÊĮˤɿʟʟÊŝŝÊĈòɾÊŝŝÊĈòɿʟʟ)L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\fµŗƓŤòˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòˤĵƙˤʁˤŝĎĵŗƜˤŔÊŗÊĈŗÊŔĎŝʛˤĎòˤòĮíˤŝòĮŤòĮçòˤĵƙˤòÊçĎˤŔÊŗÊĈŗÊŔĎˤĭũŝƜˤæòʝˤɾʛˤGƜˤƓŝĮ˙ƛˤíđƙƙƓçũĦƜˤƘĵˤíĵˤÊˤĎÊĮíŝŤÊĮíˤđƙˤƆĵũˤĠũŝƜˤŝŤÊĮíˤĵĮˤƆĵũŗˤĎÊĮíŝʛˤɿʛˤGƜˤçÊũĈĎƜˤĎđĭˤĵƙƙˤĈũÊŗíˤƛĎÊƜˤŝŔÊçòˤŝĭòĦĦòíˤĵƙˤŝòÊŗòíˤŝŤòÊģʛˤʀʛˤµĎòĮˤŝĎòˤíƓíĮ˒ƛˤĦđģòˤÊˤĈũƆˤƀĎĵˤƀÊŝˤƛŗƆđĮĈˤƘĵˤŔƓçģˤĎòŗˤũŔʜˤŝĎòˤŝŤÊŗŤòíˤũŝđĮĈˤŝƓĈĮˤĦÊĮĈũÊĈòʛˤʁʛˤ)ÊçĎˤŔòŗŝĵĮˤƀĎĵˤģĮĵƀŝˤƆĵũˤĎÊŝˤÊˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮˤĵƙˤƀĎĵˤƆĵũˤÊŗòʛˤˤɾʛˤGĮƜŗĵíũçòˤÊĮíˤòƅŔĦÊđĮˤƛĎòˤƘòçĎĮƓŖũòˤĵƙˤíĵđĮĈˤÊˤĎÊĮíŝŤÊĮíˤɿʛˤƀƓŤçĎˤƘĵˤÊˤŝŤĵŗƆˤÊæĵũƜˤÊĮˤÊŝƜŗĵĮÊũƜ˙ŝˤƚđŗŝƜˤƛđĭòˤđĮˤŝŔÊçòˤʀʛˤ#òŝçŗđæòˤÊˤŝƓƜũÊƜƓĵĮˤƀĎòŗòˤÊˤƀĵĭÊĮˤũŝòŝˤŝƓĈ\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 15}\n",
            "òĮƜˤŔòŗçòŔƜƓĵĮˤĵƙˤƀĎĵˤƆĵũˤÊŗòʛˤˤɾʛˤGĮƜŗĵíũçòˤÊĮíˤòƅŔĦÊđĮˤƛĎòˤƘòçĎĮƓŖũòˤĵƙˤíĵđĮĈˤÊˤĎÊĮíŝŤÊĮíˤɿʛˤƀƓŤçĎˤƘĵˤÊˤŝŤĵŗƆˤÊæĵũƜˤÊĮˤÊŝƜŗĵĮÊũƜ˙ŝˤƚđŗŝƜˤƛđĭòˤđĮˤŝŔÊçòˤʀʛˤ#òŝçŗđæòˤÊˤŝƓƜũÊƜƓĵĮˤƀĎòŗòˤÊˤƀĵĭÊĮˤũŝòŝˤŝƓĈĮˤĦÊĮĈũÊĈòˤƘĵˤÊſĵƓíˤũĮƀÊĮŤòíˤÊƜŤòĮƜƓĵĮˤʁʛˤĎòˤƚđĮÊĦˤŔÊŗÊĈŗÊŔĎˤòƅŔĦÊđĮŝˤĎĵƀˤòſòŗƆĵĮòˤĎÊŝˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮŝˤĵƙˤĵƜĎòŗŝɾʛˤGĮƜŗĵíũçƜƓĵĮˤƘĵˤÊĮˤũĮũŝũÊĦˤŝòĦƙˊĎòĦŔˤæĵĵģʜˤĭòĮƜƓĵĮđĮĈˤÊˤĎÊĮíŝŤÊĮíˤÊŝˤÊˤĭòŤÊŔĎĵŗˤƗĵŗˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʛˤɿʛˤ#ƓŝçũŝŝˤƛĎòˤũĮòƅŔòçŤòíˤƛĎđĮĈŝˤĦòÊŗĮòíˤƚŗĵĭˤÊŝƜŗĵĮÊũŤŝʜˤđĮçĦũíđĮĈˤƛĎòˤŝĭòĦĦˤĵƙˤŝŔÊçòʛˤʀʛˤ#òŝçŗđæòˤÊˤƀĵĭÊĮ˙ŝˤçĦòſòŗˤƘÊçƜƓçˤƗĵŗˤÊſĵƓíđĮĈˤũĮƀÊĮŤòíˤÊƜŤòĮƜƓĵĮˤÊƜˤÊˤæÊŗʛˤʁʛˤ\u001dĵĮŤòĭŔĦÊŤòˤĎĵƀˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮŝˤĵƙˤĵĮòŝòĦƙˤçÊĮˤŝĎÊŔòˤĵĮò˙ŝˤƓíòĮƜƓŤƆʛʱÊʲˤGĮŔũƜʱæʲˤĦÊĮŝʱçʲˤ´ĵŤòŝ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛGĮŔũƜĦÊĮˤɾˤĦÊĮˤɿˤˤʟʟÊŝŝÊĈòɾÊŝŝÊĈòɿʟʟɽʫʂˤſĵŤòŝĦÊĮˤɾˤˤˤʟʛʟʛɾʟʛɿʟʟʀʫʂˤſĵŤòŝĦÊĮˤʀˁʂˤˤˤ8VH\u0003UHG\u0012JUHHQ\u0003WR\u0003VKRZ\u0003ILQDO\u0003FKRLFH\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 16}\n",
            "ĮʫʂˤſĵŤòŝĦÊĮˤɿˤˤˤ\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤçĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛFigure 4: A step of deliberate search in a randomly picked Creative Writing task. Given the input, the\n",
            "LM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is\n",
            "used to consequently write the output passage with the same sample-vote procedure.\n",
            "IO CoT ToT IO\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 17}\n",
            "LM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is\n",
            "used to consequently write the output passage with the same sample-vote procedure.\n",
            "IO CoT ToT IO\n",
            "+refineToT\n",
            "+refine468\n",
            "(a) GPT-4 coherency scores\n",
            "CoT > ToT Similar ToT > CoT010203040\n",
            "213841(b) Human coherency comparison\n",
            "Figure 5: Creative Writing results.Method Success Rate (%)\n",
            "Letter Word Game\n",
            "IO 38.7 14 0\n",
            "CoT 40.6 15.6 1\n",
            "ToT (ours) 78 60 20\n",
            "+best state 82.4 67.5 35\n",
            "-prune 65.4 41.5 5\n",
            "-backtrack 54.6 20 5\n",
            "Table 3: Mini Crosswords results.\n",
            "it improves IO coherency score from 6.19 to 7.67, and ToT coherency score from 7.56 to 7.91. We\n",
            "believe it could be thought of as a third approach to thought generation in the ToT framework, where\n",
            "new thoughts can arise from refining old thoughts instead of i.i.d. or sequentially generated.\n",
            "4.3 Mini crosswords\n",
            "In Game of 24 and Creative Writing, ToT is relatively shallow — at most 3 thought steps are needed\n",
            "to reach the final output. Here we explore 5×5mini crosswords as a harder search problem involving\n",
            "natural language. Again, the goal is not just to solve the task, as more general crosswords can be\n",
            "readily solved with specialized NLP pipelines [ 34] that leverages large-scale retrieval instead of LM.\n",
            "Rather, we aim to explore the limit of LM as a general problem solver that explores its own thoughts\n",
            "and guides its own exploration with deliberate reasoning as heuristics.\n",
            "Task setup. We scrape data from GooBix, which contains 156 games of 5×5mini crosswords. As\n",
            "we observe adjacent games contain similar clues, we use 20 games with indices 1,6,···,91,96for\n",
            "testing, and games 136,141,146,151,156for prompting. For each task, the input describes the 5\n",
            "horizontal clues and 5 vertical clues, and the output should be a board of 5×5 = 25 letters to solve\n",
            "the crosswords. For evaluation, we consider three levels of success: the portion of correct letters (25\n",
            "per game), words (10 per game), and games.\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 18}\n",
            "the crosswords. For evaluation, we consider three levels of success: the portion of correct letters (25\n",
            "per game), words (10 per game), and games.\n",
            "Baselines. We provide 5 example input-output pairs in the IO prompt, and in the CoT prompt\n",
            "additionally include intermediate words in the order h1..5 then v1..5. We run each prompt for 10\n",
            "samples and average the results.\n",
            "ToT setup. We leverage a depth-first search (Algorithm 2) that keeps exploring the most promising\n",
            "subsequent word clue until the state is no longer promising, then backtrack to the parent state to\n",
            "explore alternative thoughts. To make search tractable, subsequent thoughts are constrained not to\n",
            "change any filled words or letters, so that the ToT has at most 10 intermediate steps. For thought\n",
            "generation, at each state we translate all existing thoughts (e.g. “h2.motor; h1.tasks” for the state\n",
            "in Figure 6(a)) into letter constraints for remaining clues (e.g. “v1.To heap: tm ;...”) and prompt\n",
            "a proposal prompt 5times to come up with candidates for where and what to fill in the next word.\n",
            "Importantly, we also prompt the LM to give a confidence level for different thoughts, and aggregate\n",
            "7>\u000b\n",
            "Y\u0016\u0011\u0003HORSH\n",
            "\u000f\u0003\u0016\u0011\u0015\f\u000f\u0003\u000b\n",
            "K\u0015\u0011\u0003YDOXH\n",
            "\u000f\u0003\u0015\u0011\u0013\f\u000f\u0003\u000b\n",
            "K\u0014\u0011\u0003SDUFK\n",
            "\u000f\u0003\u0014\u0011\u001c\f\u000f\u0003\u000b\n",
            "Y\u0018\u0011\u0003FRYHW\n",
            "\u000f\u0003\u0013\u0011\u0019\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0014\f\u000f\u0003\u000b\n",
            "K\u0015\u0011\u0003PHULW\n",
            "\u000f\u0003\u0013\u0011\u0017\f\u000f\u0003\u000b\n",
            "Y\u0014\u0011\u0003DOORZ\n",
            "\u000f\u0003\u0013\u0011\u0015\f\u000f\u0003\u000b\n",
            "Y\u0015\u0011\u0003JULQG\n",
            "\u000f\u0003\u0013\u0011\u0014\f\u000f\u0003\u000b\n",
            "K\u0017\u0011\u0003OHSHU\n",
            "\u000f\u0003\u0013\u0011\u0014\f@\n",
            "Y\u0016\u0011\u0003HORSH\n",
            "0XOWLSOH\u0003UXQV3DUVH\u000f\u0003ILOWHU\u0003RXW\u0003QRQ\u0010ILYH\u0010OHWWHU\u000f\u0003VFRUH\u000f\u0003DJJUHJDWH\n",
            "&KRRVH\u0003\u000bVRIW\u0003VHOI\u0010FRQVLVWHQF\\\"\f\u0014\u00110D[\u0015\u00110D[\u0003ZLWKRXW\u0003YLRODWH\u0016\u0011')6\n",
            "GĮŔũƜˤ\u001dĦũòŝĎɿʛĭĵŤĵŗĎɾʛƘÊŝģŝĎʁʛŝÊĦĵĮƘÊŝģŝƘÊŝģŝ\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 19}\n",
            "\u000f\u0003\u0013\u0011\u0014\f@\n",
            "Y\u0016\u0011\u0003HORSH\n",
            "0XOWLSOH\u0003UXQV3DUVH\u000f\u0003ILOWHU\u0003RXW\u0003QRQ\u0010ILYH\u0010OHWWHU\u000f\u0003VFRUH\u000f\u0003DJJUHJDWH\n",
            "&KRRVH\u0003\u000bVRIW\u0003VHOI\u0010FRQVLVWHQF\\\"\f\u0014\u00110D[\u0015\u00110D[\u0003ZLWKRXW\u0003YLRODWH\u0016\u0011')6\n",
            "GĮŔũƜˤ\u001dĦũòŝĎɿʛĭĵŤĵŗĎɾʛƘÊŝģŝĎʁʛŝÊĦĵĮƘÊŝģŝƘÊŝģŝ\n",
            "ĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲſʂʛˤŝŗíŗƆˤʱĦĵƀʲſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲʟʟ7KRXJKW\u00033URSRVDOVÊĈĈŗòĈÊŤòſʀʛˤŗòŤòĮƜƓĵũŝʞˤƚĦĵƀòŗƆʝˤˈˈˈˈˈˤŝũŗò6WDWH\u0003(YDOXDWRU\u0003\u000bRYHU\u0003HDFK\u0003FOXH\fſɾʛˤÉĵˤĎòÊŔʝˤƛĭˈŝˈˤʳʛʛʛʴˤđĭŔĵŝŝđæĦòſʂʛˤ#òŝƓççÊŤĵŗʞˤĭĵŗòˤíŗƆʝˤŝŗˈĮˈˤʳʛʛʛʴˤĭÊƆæòʟʟʱæÊçģƜŗÊçģʲĎʀʛĈŗÊĮíʟʟʱŝũæƜŗòòˤŔŗũĮòíʲĎʁʛˤŝÊĦĵĮĎʀʛˤĈŗÊĮíſʀʛˤŝƜŗđĮĈʟʟ')6\u00032UGHUĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲſʂʛˤŝŗíŗƆˤʱĦĵƀʲſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲʟʟ7KRXJKW\u00033URSRVDOVĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲſʂʛˤŝŗíŗƆˤʱĦĵƀʲſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲʟʟ7KRXJKW\u00033URSRVDOVĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲſʂʛˤŝŗíŗƆˤʱĦĵƀʲſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲʟʟ7KRXJKW\u00033URSRVDOVĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲſʂʛˤŝŗíŗƆˤʱĦĵƀʲſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲʟʟ7KRXJKW\u00033URSRVDOVʱÊʲʱæʲƛˤÊˤŝˤģˤŝĭˤĵˤƛˤĵˤŗˈˤˈˤˈˤˈˤˈŝˤÊˤĦˤĵˤĮˈˤˈˤˈˤˈˤˈFigure 6: In Mini Crosswords, (a) how thoughts are proposed and aggregated in a priority queue\n",
            "for depth-first search (DFS), and (b) how a state is evaluated based on the possibility of filling in\n",
            "each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM.\n",
            "Then DFS backtracks to the parent state and explore the next promising thought for clue.\n",
            "these across proposals to obtain a sorted list of next thoughts to explore (Figure 6(a)). For state\n",
            "evaluations, we similarly translate each state into letter constraints for remaining clues, then evaluate\n",
            "for each clue if it is possible to fill given the constraints. If any remaining clue is deemed “impossible”\n",
            "to fill in (e.g. “v1. To heap: tm s”), then the exploration of the state’s subtree is pruned and DFS\n",
            "backtracks to its parent to explore the next promising thought. We limit DFS search steps to 100, and\n",
            "simply render the deepest explored state (the first explored one if multiple) into the final output.\n",
            "Results. As shown in Table 3, IO and CoT prompting methods perform poorly with a word-level\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 20}\n",
            "simply render the deepest explored state (the first explored one if multiple) into the final output.\n",
            "Results. As shown in Table 3, IO and CoT prompting methods perform poorly with a word-level\n",
            "success rate less than 16%, while ToT significantly improves all metrics, achieving a word-level\n",
            "success rate of 60% and solving 4 out of 20 games. Such an improvement is not surprising, given IO\n",
            "and CoT lack mechanisms to try different clues, make changes to decisions, or backtrack.\n",
            "Oracle and ablation studies. When outputting from the oracle best DFS state (instead of the\n",
            "heuristically determined best state) per task, ToT performance is even higher and actually solves\n",
            "7/20 games (Table 3, “+best state”), indicating our simple output heuristics can be readily improved.\n",
            "Interestingly, sometimes when the crosswords game is actually solved, the state evaluator might still\n",
            "deem some words as “impossible” and prune — possibly because 5×5crosswords by design have\n",
            "some rare or obselete words that GPT-4 cannot recognize2. Given the state evaluation as a pruning\n",
            "heuristic is imperfect, we also explore ablating the pruning, and find the performance generally worse\n",
            "(Table 3, “-prune”). However, it could actually find the correct solution for 4/20 games (though only\n",
            "outputting 1 via heuristic), 3 of which are games ToT+pruning cannot solve within 100 steps. Thus,\n",
            "better heuristics for DFS pruning are critical for problem solving in this case. Lastly, we confirm the\n",
            "importance of backtracking by running an ablation that keeps filling the most promising clue for at\n",
            "most 20 steps, allowing overwrites. This is similar to a “greedy” BFS search with breadth limit of\n",
            "b= 1, and performs poorly with a word level success of only 20% (Table 3, “-backtrack”).\n",
            "5 Related Work\n",
            "Planning and decision making. Smart planning and decision making are critical to achieving\n",
            "predefined goals. As they are trained on vast amount of world knowledge and human examples, LMs\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 21}\n",
            "Planning and decision making. Smart planning and decision making are critical to achieving\n",
            "predefined goals. As they are trained on vast amount of world knowledge and human examples, LMs\n",
            "are known to have already absorbed rich commonsense that makes it possible to propose reasonable\n",
            "plans conditioned on problem setting and environmental states [ 12,42,37,13,35,41,40]. Our\n",
            "proposed ToT approach extends existing planning formulations by considering multiple potentially\n",
            "feasible plans simultaneously at each problem-solving step, and proceeding with the most promising\n",
            "ones. The integration between thought sampling and value feedback organically integrates planning\n",
            "and decision-making mechanisms, enabling effective search inside a solution tree. On the other hand,\n",
            "traditional decision-making procedures usually require training dedicated reward and policy models\n",
            "as in reinforcement learning (for example CHAI [ 33]), whereas we use the LM itself to provide\n",
            "the value estimates for decision making. RAP [ 9] is a concurrent work that treats language model\n",
            "2For example, “agend” is an obsolete form of “agendum”, but GPT-4 deems it a typo for “agenda”. External\n",
            "retrieval or web interaction could augment LM for problem solving under knowledge uncertainty.\n",
            "8reasoning as planning with its internal world model, and proposes a MCTS-based method similar to\n",
            "ToT. However, its tasks are simpler than ours, and its framework lacks the modularity to incorporate\n",
            "different tree search algorithms.\n",
            "Self-reflection. Using LLMs to assess the viability of their own predictions is becoming an in-\n",
            "creasingly important procedure in problem solving. [ 28,20,24] introduced the “self-reflection”\n",
            "mechanism, in which LMs provide feedback to their generation candidates. [ 4] improves LMs code\n",
            "generation accuracy by injecting feedback messages generated by the LM itself based on its code\n",
            "execution results. Similarly, [ 17] also introduces “critic” or review steps over the actions and states,\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 22}\n",
            "execution results. Similarly, [ 17] also introduces “critic” or review steps over the actions and states,\n",
            "deciding the next action to take in solving computer operation tasks. Another recent work very\n",
            "relevant to ours is “self-eval guided decoding” [ 39]. Similar to our method, self-eval decoding\n",
            "also follows a tree-search procedure with leaves sampled from stochastic beam search decoding,\n",
            "which are then evaluated by LLM itself with carefully prepared self-eval prompts. Their approach\n",
            "however, uses the PAL formulation [ 8] which represents thoughts as codes, which makes it difficult\n",
            "to tackle challenging tasks like creative writing which we consider in this paper. Our Tree-of-Thought\n",
            "formulation is thus more versatile and handles challenging tasks on which GPT-4 only achieves very\n",
            "low accuracy with standard prompts.\n",
            "Program-guided LLM generation. Our proposal is also related to recent advancements that organize\n",
            "LM’s behavior with systematic procedures [ 14,44,6,43] or symbolic program guidance. For example,\n",
            "Schlag et al. [27] embeds LMs in an algorithmic search procedure to help solve problems like question\n",
            "answering step-by-step, in which the search trees are expanded by relevant paragraphs that might\n",
            "provide answers. This approach however differs from ours in that trees are expanded by sampling\n",
            "external paragraphs instead of the LM’s own thoughts, and there is no reflection or voting steps.\n",
            "Another approach, LLM+P [ 18], goes one step further and delegates the actual planning process to a\n",
            "classical planner.\n",
            "Classical search methods. Last but not least, our approach can be treated as a modern rendition\n",
            "of classical search methods for problem solving. For example it can be considered as a heuristic\n",
            "search algorithm like A* [ 10], in which the heuristic at each search node is provided by the LM’s self-\n",
            "assessment. From this perspective, our method is also related to NeuroLogic A*esque decoding [ 19],\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 23}\n",
            "assessment. From this perspective, our method is also related to NeuroLogic A*esque decoding [ 19],\n",
            "which is inspired by A* search but introduces look-ahead heuristics that are efficient for LMs to\n",
            "improve the beam-search or top-k sampling decoding. This method however is constrained to\n",
            "sentence generation tasks, whereas our framework are designed for complex, multi-step problem\n",
            "solving guarded by value feedback.\n",
            "6 Discussion\n",
            "Limitations and future directions. Deliberate search such as ToT might not be necessary for many\n",
            "existing tasks that GPT-4 already excels at (see Appendix B.1), and as an initial step this work only\n",
            "explores three relatively simple tasks that challenges GPT-4 (see Appendix B.2 for some GPT-3.5\n",
            "experiment results) and calls of better search and planning abilities incorporated with LMs. However,\n",
            "as we begin to deploy LMs for more real-world decision making applications (e.g. coding, data\n",
            "analysis, robotics, etc.), more complex tasks could emerge and present new opportunities to study\n",
            "these research questions. Also, search methods like ToT requires more resources (e.g. GPT-4 API\n",
            "cost) than sampling methods in order to improve task performances, but the modular flexibility of\n",
            "ToT allows users to customize such performance-cost tradeoffs, and ongoing open-source efforts [ 32]\n",
            "should readily reduce such costs in the near future. More details about cost and efficiency are in\n",
            "Appendix B.3. Lastly, this work focuses on using an off-the-shelf LM, and fine-tuning LMs using\n",
            "a ToT-style high-level counterfactual decision making (e.g. deliberating over potential choices for\n",
            "the next paragraph, instead of predicting the next token) might present opportunities to enhance the\n",
            "problem-solving capabilities of LMs.\n",
            "Conclusion. The associative “System 1” of LMs can be beneficially augmented by a “System 2”\n",
            "based on searching a tree of possible paths to the solution to a problem. The Tree of Thoughts\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 24}\n",
            "Conclusion. The associative “System 1” of LMs can be beneficially augmented by a “System 2”\n",
            "based on searching a tree of possible paths to the solution to a problem. The Tree of Thoughts\n",
            "framework provides a way to translate classical insights about problem-solving into actionable\n",
            "methods for contemporary LMs. At the same time, LMs address a weakness of these classical\n",
            "methods, providing a way to solve complex problems that are not easily formalized, such as creative\n",
            "writing. We see this intersection of LMs with classical approaches to AI as an exciting direction.\n",
            "9Broader Impact\n",
            "ToT is a framework that empowers LMs to more autonomously and intelligently make decisions\n",
            "and solve problems. While current tasks are limited to reasoning and search problems, future\n",
            "applications involving interaction with external environments or humans could bring potential danger,\n",
            "e.g. facilitating harmful uses of LMs. On the other hand, ToT also improves the interpretability\n",
            "of model decisions and the opportunity for human alignment, as the resulting representations are\n",
            "readable, high-level language reasoning instead of implicit, low-level token values.\n",
            "Acknowledgements\n",
            "SY and KN acknowledge support from an Oracle Collaborative Research award and the National\n",
            "Science Foundation under Grant No. 2239363. Any opinions, findings, conclusions, or recommenda-\n",
            "tions expressed in this material are those of the author(s) and do not necessarily reflect the views of\n",
            "the National Science Foundation. SY is also supported by the Harold W. Dodds Fellowship from\n",
            "Princeton.\n",
            "{'filename': '2305.10601_v2.pdf', 'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models', 'vs_index': 25}\n",
            "Preprint\n",
            "RAVEN : In-Context Learning with Retrieval-Augmented\n",
            "Encoder-Decoder Language Models\n",
            "Jie Huang1,2,∗Wei Ping2Peng Xu2Mohammad Shoeybi2\n",
            "Kevin Chen-Chuan Chang1Bryan Catanzaro2\n",
            "1University of Illinois at Urbana-Champaign2NVIDIA\n",
            "jeffhj@illinois.edu, wping@nvidia.com\n",
            "Abstract\n",
            "In this paper, we investigate the in-context learning ability of retrieval-\n",
            "augmented encoder-decoder language models. We first conduct a com-\n",
            "prehensive analysis of existing models and identify their limitations in\n",
            "in-context learning, primarily due to a mismatch between pretraining and\n",
            "inference, as well as a restricted context length. To address these issues,\n",
            "we propose RAVEN , a model that combines retrieval-augmented masked\n",
            "language modeling and prefix language modeling. We further introduce\n",
            "Fusion-in-Context Learning to enhance the few-shot performance by enabling\n",
            "the model to leverage more in-context examples without requiring addi-\n",
            "tional training. Through extensive experiments, we demonstrate that our\n",
            "simple yet effective design significantly improves performance, achieving\n",
            "results comparable to the most advanced language models in certain scenar-\n",
            "ios, despite having substantially fewer parameters. Our work underscores\n",
            "the potential of retrieval-augmented encoder-decoder language models for\n",
            "in-context learning and encourages further research in this direction.\n",
            "1 Introduction\n",
            "Recent advancements in natural language processing have been predominantly driven by\n",
            "the development of large language models (LLMs) (Brown et al., 2020; OpenAI, 2022; 2023;\n",
            "Chowdhery et al., 2023; Smith et al., 2022). These models have demonstrated remarkable\n",
            "performance across a wide range of tasks (Qin et al., 2023; Bubeck et al., 2023; Huang &\n",
            "Chang, 2023). One of the key features that enables these models to excel is their ability to\n",
            "perform in-context learning (Dong et al., 2022). By conditioning on given context, LLMs can\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 26}\n",
            "Chang, 2023). One of the key features that enables these models to excel is their ability to\n",
            "perform in-context learning (Dong et al., 2022). By conditioning on given context, LLMs can\n",
            "adapt to new tasks and domains without the need for task-specific fine-tuning. This enables\n",
            "LLMs to perform well on zero-shot or few-shot learning tasks, where only a limited number\n",
            "of examples are available.\n",
            "While in-context learning has been extensively studied for decoder-only language models\n",
            "like GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2023), research on encoder-\n",
            "decoder language models, which have shown to learn stronger representations (Devlin et al.,\n",
            "2019; Raffel et al., 2020), remains limited. Notably, Patel et al. (2023) tap into the potential of\n",
            "mT5 (Xue et al., 2021), a multilingual encoder-decoder LM, by iteratively prompting the\n",
            "model to produce long generations with in-context examples. Chung et al. (2022); Longpre\n",
            "et al. (2023) finetune T5 (Raffel et al., 2020) with a large mixture of tasks using instruction\n",
            "tuning (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022) to improve model performance\n",
            "and generalization to unseen tasks in both zero-shot and few-shot settings.\n",
            "On the other hand, LLMs still face challenges such as hallucination and limitations in\n",
            "representing the long-tail and most recent knowledge (Mallen et al., 2022; Huang et al.,\n",
            "2022; Luu et al., 2022; Jang et al., 2022; Zheng et al., 2023). Retrieval-augmented language\n",
            "models (Izacard et al., 2023; Borgeaud et al., 2022; Wang et al., 2023; Shi et al., 2023) have\n",
            "emerged as a powerful approach to address these issues by retrieving relevant knowledge\n",
            "∗Work done at NVIDIA. Code is available at https://github.com/jeffhj/RAVEN .\n",
            "1arXiv:2308.07922v2  [cs.CL]  1 Apr 2024In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "from an external corpus. Among these, the encoder-decoder models, such as ATLAS (Izacard\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 27}\n",
            "from an external corpus. Among these, the encoder-decoder models, such as ATLAS (Izacard\n",
            "et al., 2023), stand out. They benefit from the strong representation ability of a bidirectional\n",
            "encoder, coupled with of the efficacy of a Fusion-in-Decoder architecture (Izacard & Grave,\n",
            "2021), enabling the effective integration of multiple retrieved passages. Despite these\n",
            "advancements, in-context learning with these models remains underexplored.\n",
            "In this regard, we first conduct a comprehensive analysis of the state-of-the-art retrieval-\n",
            "augmented encoder-decoder language models by designing and experimenting with differ-\n",
            "ent prompting strategies. We find that these models exhibit a certain in-context learning\n",
            "ability; however, due to a mismatch between pretraining and inference and a limited context\n",
            "length—issues that are common to existing encoder-decoder LMs trained with masked\n",
            "language modeling—its few-shot performance is not stable and providing more than, e.g.,\n",
            "8-shot, examples does not lead to further improvement.\n",
            "Based on the analysis, we develop RAVEN1by first mitigating the mismatch between\n",
            "pretraining and inference through a combination of retrieval-augmented masked language\n",
            "modeling and prefix language modeling. Moreover, to enable the model to learn from more\n",
            "in-context examples, we propose Fusion-in-Context Learning , a novel approach that allows\n",
            "the model to utilize more in-context examples without modifying the model configuration\n",
            "or requiring additional training. Furthermore, we suggest using the retriever of the model to\n",
            "obtain relevant in-context examples to further enhance few-shot performance. Our empirical\n",
            "results demonstrate that RAVEN significantly outperforms previous retrieval-augmented\n",
            "encoder-decoder LMs in both zero-shot and few-shot settings, even achieving comparable\n",
            "results to decoder-only LLMs in some settings despite having 180 times fewer parameters.\n",
            "The main contributions of this paper are twofold:\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 28}\n",
            "results to decoder-only LLMs in some settings despite having 180 times fewer parameters.\n",
            "The main contributions of this paper are twofold:\n",
            "•From an analytical standpoint, we provide a thorough analysis of the in-context learning\n",
            "ability of retrieval-augmented encoder-decoder language models. We demonstrate the\n",
            "possibilities and offer insights for future development.\n",
            "•From a technological perspective, we introduce RAVEN , coupled with our Fusion-\n",
            "in-Context Learning and In-Context Example Retrieval strategies, building upon the\n",
            "analytical groundwork. These techniques, though simple, are highly effective. They not\n",
            "only enhance the base model’s capabilities but also highlight the potential of in-context\n",
            "learning with retrieval-augmented encoder-decoder LMs.\n",
            "2 Background and Related Work\n",
            "Retrieval-augmented language models are a class of language models designed to enhance\n",
            "their performance by incorporating external knowledge. These models typically employ\n",
            "an information retrieval mechanism to access relevant information from a large corpus,\n",
            "which is then integrated into the model’s prediction process. Retrieval-augmented LMs\n",
            "can be based on both encoder-decoder (Izacard et al., 2023; Lewis et al., 2020) and decoder-\n",
            "only (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022) architectures. For\n",
            "decoder-only LMs, the computational cost typically increases quadratically with the input\n",
            "length, as well as with the number of retrieval passages. In contrast, for encoder-decoder\n",
            "LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the\n",
            "number of retrieved passages, as they only perform self-attention over one passage at a\n",
            "time (Izacard & Grave, 2021). This concept is also investigated by Ye et al. (2023) for more\n",
            "efficient in-context learning.\n",
            "While there has been some research on in-context learning with retrieval-augmented\n",
            "decoder-only LMs, which can be straightforwardly implemented by concatenating retrieved\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 29}\n",
            "While there has been some research on in-context learning with retrieval-augmented\n",
            "decoder-only LMs, which can be straightforwardly implemented by concatenating retrieved\n",
            "passages with the query as the input of the LM (Mallen et al., 2022; Shi et al., 2023; Khattab\n",
            "et al., 2022), in-context learning with retrieval-augmented encoder-decoder LMs remains\n",
            "unexplored to the best of our knowledge. This is despite the fact that encoder-decoder LMs\n",
            "can be more efficient at incorporating multiple (e.g., 40) retrieved passages.\n",
            "1RAVEN , a bird known for its intelligence and adaptability, has the letters “RA” in its name, which\n",
            "represents “ Retrieval- Augmented” in our context.\n",
            "2In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 30}\n",
            "represents “ Retrieval- Augmented” in our context.\n",
            "2In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "InputtoEncoder:Question:WhatisthecapitaloftheProvence-Alpes-Coted'AzurregionofFrance?Answer:MarseillesQuestion:TheGreekwordXero(pronouncedzero)inxerographyandrelatedterminologymeanswhat?Answer:DryQuestion:Inwhichcountrywasthefirstpermanentbungeejumpingsitesituated?Answer:<extra_id_0>Passage:…firstpermanentcommercialbungeesite,theKawarauBridgeBungyattheKawarauGorgeSuspensionBridgenearQueenstownintheSouthIslandofNewZealand…InputtoDecoder:NoneOutput:<extra_id_0>NewZealandInputtoEncoder:Question:WhatisthecapitaloftheProvence-Alpes-Coted'AzurregionofFrance?Answer:<extra_id_0>Question:TheGreekwordXero(pronouncedzero)inxerographyandrelatedterminologymeanswhat?Answer:<extra_id_1>Question:Inwhichcountrywasthefirstpermanentbungeejumpingsitesituated?Answer:<extra_id_2>Passage:…firstpermanentcommercialbungeesite,theKawarauBridgeBungyattheKawarauGorgeSuspensionBridgenearQueenstownintheSouthIslandofNewZealand…InputtoDecoder:<extra_id_0>Marseilles<extra_id_1>DryOutput:<extra_id_2>NewZealandPrompting Strategy 1Prompting Strategy 2Masked Language Modeling (Pretraining)InputtoEncoder:Machinelearningalgorithmsbuildamodelbasedonsampledata,<extra_id_0>astrainingdata,inorderto<extra_id_1>beingexplicitlyprogrammedtodoso.Machinelearningalgorithmsareusedinawidevarietyofapplications,suchasinmedicine,emailfiltering,speechrecognition,agriculture,andcomputervision,<extra_id_2>unfeasibletodevelopconventionalalgorithmstoperformthe<extra_id_3>Passage:…machinelearningmodelsrequireahighquantityofreliabledatainorderforthemodels…InputtoDecoder:NoneOutput:<extra_id_0>known<extra_id_1>makepredictionsordecisionswithout<extra_id_2>whereitisdifficultor<extra_id_3>neededtasks.\n",
            "Figure 1: Retrieval-augmented masked language modeling and prompting strategies for\n",
            "in-context learning.\n",
            "3 Methodology\n",
            "In this section, we first explore in-context learning with retrieval-augmented encoder-\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 31}\n",
            "in-context learning.\n",
            "3 Methodology\n",
            "In this section, we first explore in-context learning with retrieval-augmented encoder-\n",
            "decoder language models in the literature. Building upon the analysis, we develop models\n",
            "with enhanced zero-shot performance and improved in-context learning abilities.\n",
            "3.1 In-Context Learning with Retrieval-Augmented Encoder-Decoder LMs\n",
            "To investigate the in-context learning ability of retrieval-augmented encoder-decoder\n",
            "language models, we first aim to gain insights from the state-of-the-art designs in the\n",
            "literature. Among them, the design of ATLAS (Izacard et al., 2023) stands out; it combines\n",
            "a general-purpose dense retriever with a sequence-to-sequence reader (i.e., T5 (Raffel et al.,\n",
            "2020)) using the Fusion-in-Decoder architecture (Izacard & Grave, 2021). The retriever,\n",
            "encoder and decoder are jointly trained during the pretraining process. In this process,\n",
            "the dense retriever, based on the Contriever model (Izacard et al., 2022), is responsible\n",
            "for selecting relevant passages from an external knowledge source, e.g., Wikipedia, based\n",
            "on the given corrupted context. The retrieved passages are then processed along with the\n",
            "context by the encoder, which generates the corresponding output, i.e., the masked spans,\n",
            "at the decoder (Figure 1, left). ATLAS demonstrates exceptional few-shot performance on\n",
            "knowledge-intensive language tasks (Petroni et al., 2021), despite having a lower parameter\n",
            "count compared to other recent LLMs.\n",
            "However, in Izacard et al. (2023), the few-shot performance is achieved by finetuning\n",
            "the model with few-shot examples, which requires additional training and may limit its\n",
            "applications, such as dealing with dynamic and diverse real-time user queries like GPT-\n",
            "3/4 (Brown et al., 2020; OpenAI, 2023), where in-context learning plays a vital role. Therefore,\n",
            "we take the initiative to explore the in-context learning ability of this type of models, using\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 32}\n",
            "3/4 (Brown et al., 2020; OpenAI, 2023), where in-context learning plays a vital role. Therefore,\n",
            "we take the initiative to explore the in-context learning ability of this type of models, using\n",
            "open-domain question answering (Chen et al., 2017) as a representative task for some\n",
            "preliminary experiments.\n",
            "Prompting Strategies. To facilitate in-context learning, an effective prompting strategy is\n",
            "paramount. In contrast to decoder-only LMs, where the input can only be fed to the decoder,\n",
            "encoder-decoder LMs can take input in either the encoder or the decoder. In alignment with\n",
            "the pretraining objective, we identify two prompting strategies for in-context learning:\n",
            "Strategy 1. The first strategy involves feeding all example question-answer pairs and the\n",
            "target question to the encoder, without any input to the decoder. The prompt is designed as:2\n",
            "2Here we present a format designed for better demonstration. The actual prompt, which follows\n",
            "the template used in pretraining, can be found in Appendix B.4.\n",
            "3In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "Natural Questions TriviaQA\n",
            "0-shot 1-shot 5-shot 8-shot 0-shot 1-shot 5-shot 8-shot\n",
            "ATLAS 11B S126.721.3 29.8 31.356.935.5 62.3 63.9\n",
            "ATLAS 11B S2 21.4 16.3 9.8 49.8 48.4 44.4\n",
            "Table 1: Results of A TLAS 11B with prompting strategy 1 (S1) and strategy 2 (S2).\n",
            "0 1 2 3 4 5 8 16\n",
            "# In-Context Examples222426283032Natural Questions, EM\n",
            "ATLAS 11B (XXL)\n",
            "ATLAS 3B (XL)\n",
            "ATLAS 770M (Large)\n",
            "0 1 2 3 4 5 8 16\n",
            "# In-Context Examples52545658606264TriviaQA, EM\n",
            "Figure 2: Results of A TLAS with different numbers of in-context examples.\n",
            "Enc: Question: q1Answer: a1. . .Question: qkAnswer: akQuestion: q0Answer: <extra id0>\n",
            "d\n",
            "where (q1,a1),. . .,(qk,ak)represent example QA pairs, q0denotes the target question,\n",
            "<extra id0>is a sentinel token (Raffel et al., 2020), and dis the relevant passage retrieved\n",
            "with q0. An example in a 2-shot setting is illusated in Figure 1 (middle).\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 33}\n",
            "<extra id0>is a sentinel token (Raffel et al., 2020), and dis the relevant passage retrieved\n",
            "with q0. An example in a 2-shot setting is illusated in Figure 1 (middle).\n",
            "Strategy 2. As the decoder of the encoder-decoder model can also accept input, we can\n",
            "feed the answers of in-context examples to the decoder and only feed the questions to the\n",
            "encoder, using multiple sentinel tokens:\n",
            "Enc: Question: q1Answer: <extra id0>. . .Question: qkAnswer: <extra id(k−1)>Ques-\n",
            "tion: q0Answer: <extra idk>d\n",
            "Dec:<extra id0>a1. . .<extra id(k−1)>ak\n",
            "Figure 1 (right) demonstrates an example. The model is expected to learn from in-context\n",
            "examples by examining both the input to the encoder and input to the decoder.\n",
            "We select two widely-used datasets in the domain of open-domain question answering for\n",
            "the preliminary study: Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA\n",
            "(TQA) (Joshi et al., 2017)3. Table 1 summarizes the results. We find that the model struggles\n",
            "to learn from in-context examples using strategy 2, as the few-shot performance is worse\n",
            "than the zero-shot performance. We hypothesize that this is because the model has difficulty\n",
            "learning the pattern of S2 with masked language modeling during its pretraining, since it is\n",
            "unlikely to obtain several consecutive question-answer pairs (or something similar) in the\n",
            "form of strategy 2 by randomly masking several spans in a sequence.\n",
            "On the other hand, we observe that with strategy 1, the model does exhibit some in-context\n",
            "learning ability, where the 5-shot and 8-shot performance is significantly better than the\n",
            "zero-shot performance on both NQ and TriviaQA. Therefore, we choose to focus on strategy\n",
            "1 for further study and disregard strategy 2 for the remainder of the paper.\n",
            "Effect of Number of In-Context Examples. The number of in-context examples is a crucial\n",
            "hyperparameter for in-context learning. Generally, we expect better performance from a\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 34}\n",
            "Effect of Number of In-Context Examples. The number of in-context examples is a crucial\n",
            "hyperparameter for in-context learning. Generally, we expect better performance from a\n",
            "model with more in-context examples, but there is an upper limit due to 1) the maximum con-\n",
            "text length setup, e.g., 512 tokens, during the pretraining process, and 2) the point at which\n",
            "the model has received sufficient examples and cannot gain additional information from\n",
            "more examples. The optimal number of in-context examples also varies between models.\n",
            "For instance, on TriviaQA, PaLM (Chowdhery et al., 2023) exhibits better 1-shot performance\n",
            "than settings with more examples, while this is not the case for GPT-3 (Brown et al., 2020).\n",
            "3Experimental setup is detailed in the Appendix B.1.\n",
            "4In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "Figure 2 illustrates the impact of varying the number of in-context examples across different\n",
            "model sizes. Interestingly, the 11B model demonstrates poor performance in low-shot\n",
            "settings, e.g., 1-shot, but improves significantly after 4-shot and 5-shot. Upon examining the\n",
            "generated responses, we find that the model tends to produce answers with more tokens\n",
            "in low-shot settings, while the ground truth typically consists of shorter answers with\n",
            "fewer than 5 tokens. By relaxing the criteria for a correct prediction to include instances\n",
            "where the ground-truth answer is a substring of the model output, we find that the 1-shot\n",
            "performance surpasses that of the 0-shot setting (38.3 vs 32.1 on NQ).\n",
            "All models perform well in the 5-shot and 8-shot settings, but their performance does not\n",
            "continue to improve with more in-context examples (e.g., 16-shot). We believe this plateau\n",
            "may be attributed to two factors: 1) the sequence length constraints during pretraining,\n",
            "where the maximum input length to the encoder is set to 384 tokens, and the average input\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 35}\n",
            "may be attributed to two factors: 1) the sequence length constraints during pretraining,\n",
            "where the maximum input length to the encoder is set to 384 tokens, and the average input\n",
            "sequence length (excluding passages) is around 130 tokens; 2) the model’s ability to learn\n",
            "adequately with 5 or 8 examples, making additional examples less beneficial.\n",
            "In Appendices C.1 and C.2, we also study the effect of the target question’s position and the\n",
            "effect of the number of retrieved passages. We observe that positioning the target question\n",
            "after all the in-context examples yields the optimal performance. Additionally, for both\n",
            "0-shot and 5-shot settings, the performance of the models increases significantly with the\n",
            "number of retrieved passages, with the 5-shot performance consistently outperforming the\n",
            "0-shot setting. This highlights the superiority of the encoder-decoder (Fusion-in-Decoder)\n",
            "architecture, which offers an advantage not available to decoder-only language models.\n",
            "3.2 R AVEN : Combining Retrieval-Augmented Masked and Prefix Language Modeling\n",
            "InputtoEncoder:Machinelearningalgorithmsbuildamodelbasedonsampledata,knownastrainingdata,inordertomakepredictionsordecisionswithoutbeingexplicitlyprogrammedtodoso.Machinelearningalgorithmsareusedinawidevarietyofapplications,suchasinmedicine,emailfiltering,speechrecognition,agriculture,andcomputervision,whereitisdifficultor<extra_id_0>Passage:…machinelearningmodelsrequireahighquantityofreliabledatainorderforthemodels…InputtoDecoder:NoneOutput:<extra_id_0>unfeasibletodevelopconventionalalgorithmstoperformtheneededtasks.   ��������������������������������������������\n",
            "Figure 3: Retrieval-augmented prefix\n",
            "language modeling.In§3.1, we observe that retrieval-augmented\n",
            "encoder-decoder LMs exhibit a certain ability for in-\n",
            "context learning, which has been overlooked in previ-\n",
            "ous studies. However, there are also some limitations\n",
            "such as unstable performance in low-shot settings,\n",
            "and the fact that providing more in-context examples\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 36}\n",
            "ous studies. However, there are also some limitations\n",
            "such as unstable performance in low-shot settings,\n",
            "and the fact that providing more in-context examples\n",
            "does not consistently improve performance.\n",
            "To learn a better retriever and enhance the bidirec-\n",
            "tional understanding ability of the reader, as demon-\n",
            "strated in Izacard et al. (2023), a practical choice is to\n",
            "pretrain the model with the masked language mod-\n",
            "eling objective, where the input is a corrupted text\n",
            "with several masked spans placed randomly within\n",
            "the sequence (refer to Figure 1 (left) for an example).\n",
            "However, in testing, based on our analysis in §3.1\n",
            "and Appendix C.1, it is most effective to place the tar-\n",
            "get question after all the in-context examples, with a\n",
            "masked token (i.e., <extra id0>) following the ques-\n",
            "tion (Figure 1, middle)). Thus, there exists a mismatch between pretraining and inference.\n",
            "To solve this issue, we propose combining retrieval-augmented masked and prefix language\n",
            "modeling. Specifically, in the first stage, the retriever and reader are trained jointly with\n",
            "retrieval-augmented masked language modeling. The training objective for the retriever is\n",
            "to minimize the KL divergence KL(pREADER ∥pRETRIEVER )between the passage posterior\n",
            "distribution according to the reader and the passage distribution from the retriever over\n",
            "the top-K retrieved passages, i.e., pREADER (d) =exp(logpLM(a|d,q))\n",
            "∑K\n",
            "i=1exp(logpLM(a|di,q)),pRETRIEVER (d) =\n",
            "exp(s(d,q)/T)\n",
            "∑K\n",
            "i=1exp(s(di,q)/T), where s(·)calculates the dot product between the query qand passage d\n",
            "vectors, and Tis a hyperparameter. The training objective for the reader is to maximize the\n",
            "likelihood of the masked spans with nretrieved passages: ∑ilogp(ai|q,{dk}1,...,n,a1:i−1).\n",
            "In the second stage, for each sequence, we mask 10% of the tokens on average at the end of\n",
            "the sequence with the <extra id0>token. Then, we use the retriever obtained from the first\n",
            "5In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 37}\n",
            "the sequence with the <extra id0>token. Then, we use the retriever obtained from the first\n",
            "5In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "𝑞!𝑎!;𝑞\"𝑎\"; 𝑞#+ passage 1𝑞!𝑎!;𝑞\"𝑎\"; 𝑞#+ passage 2𝑞!𝑎!;𝑞\"𝑎\"; 𝑞#+ passage 𝑛…encodeencodeencodeconcat………decode𝑎#𝑞!𝑎!;𝑞\"𝑎\"; 𝑞#+ passage 1𝑞$𝑎$;𝑞%𝑎%; 𝑞#+ passage 2𝑞&𝑎&;𝑞'𝑎'; 𝑞#+ passage 𝑛encodeencodeencodeconcat…decode𝑎#\n",
            "………Standard In-Context Learning\n",
            "Fusion-in-Context Learning\n",
            "Figure 4: Standard In-Context Learning vs Fusion-in-Context Learning.\n",
            "stage to retrieve relevant passages using the prefix and train the reader to recover the suffix\n",
            "of this sequence with the prefix and the passages as input. An example of input and output\n",
            "for retrieval-augmented prefix language modeling is shown in Figure 3. We can observe\n",
            "that the pretraining objective aligns more closely with prompting strategy 1 in Figure 1. We\n",
            "refer to the model trained with this combined objective as R AVEN .\n",
            "RAVEN benefits from both the retrieval-augmented masked language modeling, which con-\n",
            "tributes to a better reader and retriever, and retrieval-augmented prefix language modeling,\n",
            "which mitigates the gap between pretraining and inference. This design is non-trivial . In\n",
            "Appendix C.3, we verify the effectiveness of it by exploring different training strategies.\n",
            "3.3 Fusion-in-Context Learning\n",
            "In§3.1, we observe that the performance does not further improve with more in-context\n",
            "examples after 8-shot. One major reason for this is the limited sequence length during the\n",
            "pretraining process, which makes it difficult for the model to handle long sequences during\n",
            "inference. Pretraining models with longer contexts would be a potential solution, but it\n",
            "would significantly increase computation cost. Additionally, the maximum input length is\n",
            "also constrained by the maximum sequence length of the retriever, i.e., Contriever, which is\n",
            "based on BERT (Devlin et al., 2019) and has a maximum length of 512 tokens.\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 38}\n",
            "also constrained by the maximum sequence length of the retriever, i.e., Contriever, which is\n",
            "based on BERT (Devlin et al., 2019) and has a maximum length of 512 tokens.\n",
            "As an alternative, we propose an approach to enable models to learn from more in-context\n",
            "examples without requiring additional training. As described in §3.1, the reader is based\n",
            "on the Fusion-in-Decoder architecture (Izacard & Grave, 2021), where multiple passages\n",
            "are retrieved, and each passage, concatenated with the in-context examples and target\n",
            "question, is fed to the encoder separately (Figure 4, top). To allow the model to process more\n",
            "in-context examples, we can feed different in-context examples to the encoder with each\n",
            "passage (Figure 4, bottom). In this way, the model can incorporate more in-context examples\n",
            "during its inference process. We refer to this strategy as Fusion-in-Context Learning (FiCL) .\n",
            "In implementation, for a k-shot setting, such as a 64-shot setting, to effectively utilize the 64\n",
            "examples, we randomly shuffle these examples and select m(e.g., 5) examples in order as the\n",
            "input for the encoder each time. If all the examples have been used, we shuffle the 64 exam-\n",
            "ples again. We denote the configuration of FiCL as [ k,m], which stands for [ k-shot, m-fusion].\n",
            "3.4 In-Context Example Retrieval\n",
            "Liu et al. (2022); Rubin et al. (2022); Su et al. (2023) demonstrate that a well-chosen selection\n",
            "of in-context examples can enhance in-context learning. Building on this insight, we propose\n",
            "utilizing the retriever of RAVEN to retrieve in-context examples. Specifically, we use RAVEN ’s\n",
            "retriever to build an index during the preparation step, and then, during testing, when the\n",
            "model receives an input, it could efficiently retrieve in-context examples with its retriever.\n",
            "6In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "0 1 2 3 4 5 8>8\n",
            "# In-Context Examples2628303234Natural Questions, EM\n",
            "RAVEN 11B\n",
            " + FICL\n",
            "ATLAS 11B\n",
            "0 1 2 3 4 5 8>8\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 39}\n",
            "6In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "0 1 2 3 4 5 8>8\n",
            "# In-Context Examples2628303234Natural Questions, EM\n",
            "RAVEN 11B\n",
            " + FICL\n",
            "ATLAS 11B\n",
            "0 1 2 3 4 5 8>8\n",
            "# In-Context Examples56586062646668TriviaQA, EM\n",
            "Figure 5: RAVEN vsATLAS . We report the best observed performance achieved with more\n",
            "than eight shots for “ >8”.\n",
            "By integrating RAVEN ’s retriever in this manner, we aim to: 1) automate in-context learning,\n",
            "which is particularly practical for model owners who have a database of examples. Without\n",
            "this, users would need to manually provide in-context examples; and 2) optimize the\n",
            "selection of in-context examples, thereby improving in-context learning performance.\n",
            "4 Experiments\n",
            "4.1 Experimental Setup\n",
            "Datasets. Following the setup in §3.1, we first evaluate on two widely-used open-domain\n",
            "question answering datasets: Natural Questions (Kwiatkowski et al., 2019) and Trivi-\n",
            "aQA (Joshi et al., 2017). Additionally, we conduct a case study on long-form question\n",
            "answering using the ELI5 dataset (Fan et al., 2019). Furthermore, we assess the models’\n",
            "language understanding ability using the Massively Multitask Language Understanding\n",
            "(MMLU) benchmark (Hendrycks et al., 2021). Detailed information regarding the MMLU\n",
            "evaluation is in Appendix B.5. Other evaluation settings are the same as §B.1.\n",
            "Baselines. Since both RAVEN and ATLAS (Izacard et al., 2023) are trained starting from T5,\n",
            "we choose ATLAS as a primary baseline for comparison. We also compare our model with\n",
            "decoder-only LLMs such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), and\n",
            "LLaMA (Touvron et al., 2023) (in a closed-book setting). Additionally, for open-domain\n",
            "QA, we evaluate our approach against REPLUG (Shi et al., 2023) and RETRO (Borgeaud\n",
            "et al., 2022), as well as its improved version RETRO ++(Wang et al., 2023). These models are\n",
            "decoder-only language models augmented with retrieval. REPLUG is based on Codex (Chen\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 40}\n",
            "et al., 2022), as well as its improved version RETRO ++(Wang et al., 2023). These models are\n",
            "decoder-only language models augmented with retrieval. REPLUG is based on Codex (Chen\n",
            "et al., 2021) and Contriever (Izacard et al., 2022), where the passages are retrieved by\n",
            "Contriever (using ensemble and additional adaptation) and fed directly to Codex. RETRO is\n",
            "a GPT model (Radford et al., 2019) augmented with a transformer encoder to encode the\n",
            "retrieved passages. RETRO ++is a variant of RETRO that feeds the most relevant retrieved\n",
            "passage into the GPT decoder while providing other passages to its encoder. For MMLU,\n",
            "we also include FLAN-T5 (Chung et al., 2022), an enhanced version of T5 that has been\n",
            "trained on a large mixture of tasks with instruction finetuning.4\n",
            "4.2 Open-Domain Question Answering\n",
            "We choose open-domain QA as our primary evaluation task, as it effectively represents\n",
            "knowledge-intensive challenges and is widely employed in real-world applications.\n",
            "RAVEN vsATLAS .Figure 5 and Table 2 present the exact match (EM) scores for ATLAS and\n",
            "RAVEN on the NQ and TriviaQA datasets. Both the 3B and 11B RAVEN models significantly\n",
            "outperform ATLAS . For instance, on TriviaQA, RAVEN 11B achieves an improvement of\n",
            "8.8%, 30.7%, and 2.8% in the 0-shot, 1-shot, and few-shot settings respectively, compared to\n",
            "ATLAS 11B. Furthermore, the performance of RAVEN increases steadily with the number of\n",
            "in-context examples, while the performance of ATLAS experiences a substantial decline in\n",
            "low-shot settings, demonstrating the effectiveness of R AVEN across various shot settings.\n",
            "4Implementation details are described in Appendix B.2.\n",
            "7In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "Natural Questions TriviaQA\n",
            "0-shot 1-shot few-shot 0-shot 1-shot few-shot\n",
            "GPT-3 13B 7.8 13.7 21.0 (64) 41.8 51.3 57.5 (64)\n",
            "GPT-3 175B 14.6 23.0 29.9 (64) 64.3 68.0 71.2 (64)\n",
            "PaLM 8B 8.4 10.6 14.6 (5) 39.5 48.5 47.2 (5)\n",
            "PaLM 62B 18.1 23.1 27.6 (5) 67.3 72.7 70.1 (5)\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 41}\n",
            "GPT-3 13B 7.8 13.7 21.0 (64) 41.8 51.3 57.5 (64)\n",
            "GPT-3 175B 14.6 23.0 29.9 (64) 64.3 68.0 71.2 (64)\n",
            "PaLM 8B 8.4 10.6 14.6 (5) 39.5 48.5 47.2 (5)\n",
            "PaLM 62B 18.1 23.1 27.6 (5) 67.3 72.7 70.1 (5)\n",
            "PaLM 540B 21.2 29.3 39.6 (64) 76.9 81.4 81.4 (1)*\n",
            "Codex 175B - - 40.6 (16) - - 73.6 (16)\n",
            "LLaMA 7B 16.8 18.7 26.1 (64) 50.0 53.4 57.6 (64)\n",
            "LLaMA 65B 23.8 31.0 39.9 (64) 68.2 71.6 73.0 (64)\n",
            "Retrieval-Augmented Language Models\n",
            "Codex + Contriever 175B - - 44.2 (16) - - 76.0 (16)\n",
            "Codex + R EPLUG 175B - - 44.7 (16) - - 76.8 (16)\n",
            "Codex + R EPLUG LSR 175B - - 45.5 (16) - - 77.3 (16)\n",
            "RETRO 9.5B 8.9 - - 36.0 - -\n",
            "RETRO ++ 9.5B 25.8 - - 48.3 - -\n",
            "ATLAS 3B 23.7 25.1 28.4 (5) 54.3 55.5 61.1 (5)\n",
            "ATLAS + FiCL 3B 29.6 [64-5] 62.0 [64-5]\n",
            "ATLAS 11B 26.7 21.3 31.3 (8) 56.9 35.5 63.9 (8)\n",
            "ATLAS + FiCL 11B 32.0 [64-8] 64.9 [64-8]\n",
            "RAVEN 3B 29.3 31.7 31.4 (5) 62.4 63.2 62.6 (5)\n",
            "RAVEN + FiCL 3B 32.8 [40-1] 63.6 [40-1]\n",
            "RAVEN 11B 29.6 31.4 32.7 (5) 65.7 66.2 66.7 (5)\n",
            "RAVEN + FiCL 11B 33.5 [64-5] 67.3 [64-5]\n",
            "* For TriviaQA, PaLM’s 1-shot performance surpasses other settings. We follow the original paper to report the 1-shot result.\n",
            "For other models, we select the best k-shot ( k∈ {2, 3, 4, 5, 8, 16 }) performance or report the number in the original paper.\n",
            "Table 2: Results on NQ and TriviaQA. Since the performance varies significantly depending\n",
            "on the capability of the base model, the results from models other than ATLAS should only\n",
            "be used for reference to gauge the position. And we assume RAVEN can achieve significant\n",
            "performance improvement when based on a stronger base model.\n",
            "1 5 10 20 30 40\n",
            "# Retrieved Passages15202530Natural Questions, EM\n",
            "5-shot\n",
            "0-shot\n",
            "1 5 10 20 30 40\n",
            "# Retrieved Passages4550556065TriviaQA, EM\n",
            "Figure 6: Results of R AVEN 11B with different numbers of retrieved passages.\n",
            "Fusion-in-Context Learning. We also report the results of models with Fusion-in-Context\n",
            "Learning (FiCL) in Table 2. For both ATLAS and RAVEN , FiCL contributes to approximately\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 42}\n",
            "Fusion-in-Context Learning. We also report the results of models with Fusion-in-Context\n",
            "Learning (FiCL) in Table 2. For both ATLAS and RAVEN , FiCL contributes to approximately\n",
            "a 1% improvement, which is not attainable by standard in-context learning, where perfor-\n",
            "mance does not further improve (or even decreases) with more than 8 in-context examples.\n",
            "This demonstrates the superiority of FiCL for enabling models to learn from more examples.\n",
            "Comparison to Other Models. In Table 2, we further compare RAVEN to other baselines.\n",
            "On NQ, RAVEN ’s zero-shot and one-shot performance surpasses all the baselines, including\n",
            "PaLM, even though RAVEN 3B has 180 times fewer parameters than PaLM 540B. The\n",
            "zero-shot performance of RAVEN on TriviaQA is also on par with PaLM 62B. Furthermore,\n",
            "RAVEN ’s zero-shot performance significantly exceeds that of both RETRO and RETRO ++,\n",
            "which are retrieval-augmented language models of a similar scale.\n",
            "In the few-shot setting, with FiCL, RAVEN achieves performance comparable to GPT-3 175B\n",
            "and PaLM 62B. However, there remains a gap between RAVEN and the larger PaLM 540B\n",
            "and Codex 175B models. Nevertheless, given the considerably smaller scale of RAVEN\n",
            "8In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "in comparison to PaLM and Codex, its performance can be considered impressive. The\n",
            "performance of RAVEN may be further improved if it is built upon a larger model, in which\n",
            "case its few-shot performance is likely to surpass that of PaLM and Codex.\n",
            "NQ TQA\n",
            "1-shot 5-shot 1-shot 5-shot\n",
            "3B +9.1 +11.6 +0.0 +1.6\n",
            "11B +9.8 +11.1 -0.5 +1.0\n",
            "Table 3: Performance improvement of\n",
            "RAVEN with In-Context Example Retrieval.Effect of Number of Retrieved Passages. Figure 6 illustrates the effect of the number of\n",
            "retrieved passages. As the number of retrieved passages increases, we observe a significant\n",
            "performance improvement of R AVEN 11B in both the 0-shot and 5-shot settings.\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 43}\n",
            "retrieved passages. As the number of retrieved passages increases, we observe a significant\n",
            "performance improvement of R AVEN 11B in both the 0-shot and 5-shot settings.\n",
            "In-Context Example Retrieval. §3.4 suggests using RAVEN ’s retriever for in-context example\n",
            "retrieval. Results in Table 3 show that this approach improves RAVEN ’s few-shot results,\n",
            "especially on NQ where a ∼10% improvement is observed. This indicates the positive\n",
            "impact of incorporating more relevant in-context examples.\n",
            "Additional Results. We conduct an ablation study of different training strategies in\n",
            "Appendix C.3 and provide a case study on long-form question answering in Appendix C.4.\n",
            "4.3 MMLU\n",
            "0-shot 1-shot 5-shot\n",
            "GPT-3 13B - - 26.0\n",
            "GPT-3 175B - - 43.9\n",
            "PaLM 8B - - 25.3\n",
            "PaLM 62B - - 53.7\n",
            "PaLM 540B - - 69.3\n",
            "T5 3B - - 25.7\n",
            "T5 11B - - 25.9\n",
            "FLAN-T5 3B - - 52.4\n",
            "FLAN-T5 11B - - 55.1\n",
            "ATLAS 3B 43.7 36.9 38.5\n",
            "+ FiCL 3B 42.6 [40-1]\n",
            "ATLAS 11B 47.4 45.3 44.2\n",
            "+ FiCL 11B 48.0 [40-1]\n",
            "RAVEN 3B 45.7 40.0 40.4\n",
            "+ FiCL 3B 44.5 [64-5]\n",
            "RAVEN 11B 48.9 49.2 48.7\n",
            "+ FiCL 11B 50.5 [40-1]\n",
            "Table 4: Results on MMLU.Table 4 summarizes the results (accuracy)\n",
            "on Massive Multitask Language Under-\n",
            "standing (MMLU). We find that the zero-\n",
            "shot performance of RAVEN is impressive,\n",
            "surpassing the few-shot performance of\n",
            "GPT-3 175B and being slightly worse than\n",
            "PaLM 62B, despite having a significantly\n",
            "smaller number of parameters. Further-\n",
            "more, with the same number of parameters,\n",
            "the performance of RAVEN is far superior to\n",
            "T5. Additionally, even without instruction\n",
            "finetuning, RAVEN achieves performance\n",
            "comparable to FLAN-T5, a model finetuned\n",
            "on a large collection of tasks. We expect fur-\n",
            "ther improvement of RAVEN by applying\n",
            "instruction tuning as well and leave it for\n",
            "future study.\n",
            "Interestingly, with standard in-context\n",
            "learning, the few-shot performance of\n",
            "RAVEN is worse than zero-shot, possibly\n",
            "due to the longer questions and answer\n",
            "options in MMLU causing context length\n",
            "issues in the 5-shot setting. Also, in\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 44}\n",
            "learning, the few-shot performance of\n",
            "RAVEN is worse than zero-shot, possibly\n",
            "due to the longer questions and answer\n",
            "options in MMLU causing context length\n",
            "issues in the 5-shot setting. Also, in\n",
            "the one-shot setting, since MMLU is a\n",
            "multiple-choice QA task, providing only\n",
            "one example might introduce bias in the\n",
            "model’s prediction, favoring a specific\n",
            "option. However, with Fusion-in-Context\n",
            "Learning, the performance improves significantly, leading to better few-shot performance\n",
            "for the 11B model compared to its zero-shot performance, further demonstrating the\n",
            "effectiveness of FiCL.\n",
            "5 Conclusion\n",
            "In this study, we have delved into the in-context learning ability of retrieval-augmented\n",
            "encoder-decoder language models. We commenced with a comprehensive analysis of the\n",
            "models in the literature and subsequently developed our model based on the analysis.\n",
            "Our extensive experimental results demonstrated that our model significantly outperforms\n",
            "previous models and achieves results on par with some of the most advanced language\n",
            "models, even with substantially fewer parameters. These findings highlight the potential of\n",
            "retrieval-augmented encoder-decoder language models in the realm of in-context learning.\n",
            "9In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models\n",
            "{'filename': '2308.07922_v2.pdf', 'title': 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder  Language Models', 'vs_index': 45}\n",
            "How to Train Data-Efficient LLMs\n",
            "Noveen Sachdeva1 2Benjamin Coleman1Wang-Cheng Kang1Jianmo Ni1Lichan Hong1Ed H. Chi1\n",
            "James Caverlee1 3Julian McAuley2Derek Zhiyuan Cheng1\n",
            "Abstract\n",
            "The training of large language models (LLMs) is\n",
            "expensive. In this paper, we study data-efficient\n",
            "approaches for pre-training LLMs, i.e., techniques\n",
            "that aim to optimize the Pareto frontier of model\n",
            "quality and training resource/data consumption.\n",
            "We seek to understand the tradeoffs associated\n",
            "with data selection routines based on (i) expensive-\n",
            "to-compute data-quality estimates, and (ii) max-\n",
            "imization of coverage and diversity-based mea-\n",
            "sures in the feature space. Our first technique,\n",
            "ASK-LLM , leverages the zero-shot reasoning ca-\n",
            "pabilities of instruction-tuned LLMs to directly\n",
            "assess the quality of a training example. To tar-\n",
            "get coverage, we propose DENSITY sampling,\n",
            "which models the data distribution to select a\n",
            "diverse sample. In our comparison of 19sam-\n",
            "plers, involving hundreds of evaluation tasks and\n",
            "pre-training runs, we find that ASK-LLM and\n",
            "DENSITY are the best methods in their respec-\n",
            "tive categories. Coverage sampling can recover\n",
            "the performance of the full data, while models\n",
            "trained on ASK-LLM data consistently outper-\n",
            "form full-data training—even when we reject 90%\n",
            "of the original dataset, while converging up to\n",
            "70% faster.\n",
            "1. Introduction\n",
            "Large language model (LLM) pre-training is perhaps the\n",
            "most data- and compute-intensive task attempted by the\n",
            "machine learning community to date, with impressive capa-\n",
            "bilities primarily being accomplished by training massive\n",
            "transformer architectures on trillions of tokens of text (Ope-\n",
            "nAI, 2023; Gemini et al., 2023; Touvron et al., 2023b).\n",
            "But even these incredibly capable LLMs are subject to em-\n",
            "pirical scaling laws, which predict sharply diminishing re-\n",
            "turns from a linear increase in model- or data-size (Hoff-\n",
            "mann et al., 2022; Kaplan et al., 2020). Power-law scaling\n",
            "therefore acts as a soft limit on model quality, beyond which\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 46}\n",
            "turns from a linear increase in model- or data-size (Hoff-\n",
            "mann et al., 2022; Kaplan et al., 2020). Power-law scaling\n",
            "therefore acts as a soft limit on model quality, beyond which\n",
            "1Google DeepMind2University of California, San Diego\n",
            "3Texas A&M University. Correspondence to: Noveen Sachdeva\n",
            "<noveen@google.com>.it is prohibitively expensive to drive performance by scal-\n",
            "ing up the data or model. At the same time, Sorscher et al.\n",
            "(2022)—in the context of vision pre-training—show that\n",
            "we can significantly improve the power law constants in\n",
            "the aforementioned scaling laws if we prioritize important\n",
            "training examples using some robust notion of data quality\n",
            "or impact.\n",
            "A similar call for data-curation is also apparent in the context\n",
            "of training LLMs, where our largest models are quickly ap-\n",
            "proaching their capacity and data thresholds. LIMA (Zhou\n",
            "et al., 2023) showed that LLaMA-65B (Touvron et al.,\n",
            "2023a) can be better aligned with human preferences when\n",
            "trained on a set of 1,000 carefully selected fine-tuning\n",
            "prompts, compared to training on as much as 52,000 unfil-\n",
            "tered examples. Tirumala et al. (2023) recently conducted a\n",
            "large-scale data-efficient pre-training evaluation, showing\n",
            "that a 6.7B OPT model (Zhang et al., 2022) can converge up\n",
            "to 20% faster on data curated by a technique based on strati-\n",
            "fied cluster sampling. The Phi-2 experiments also suggest\n",
            "that when data curation is performed at a human-expert level\n",
            "(e.g., by textbook editors), models can outperform baselines\n",
            "that are up to 25x larger (Javaheripi et al., 2023).\n",
            "Data curation routines can be fundamentally characterized\n",
            "as selecting training samples for quality, coverage, or some\n",
            "mixture of both (Figure 2). In this work, we seek to under-\n",
            "stand how quality and coverage affect the data efficiency of\n",
            "LLM pre-training. Our core research question is:\n",
            "“Are cheap-to-compute heuristics like maximum-\n",
            "coverage enough to pre-train a SoTA LLM, or\n",
            "are there real benefits from costly samplers that\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 47}\n",
            "LLM pre-training. Our core research question is:\n",
            "“Are cheap-to-compute heuristics like maximum-\n",
            "coverage enough to pre-train a SoTA LLM, or\n",
            "are there real benefits from costly samplers that\n",
            "carefully evaluate the quality of each example?”\n",
            "This question is crucial to answer because data-curation\n",
            "algorithms can improve the Pareto frontier of the data-\n",
            "quantity ↔model-quality tradeoff, directly addressing the\n",
            "bottleneck of power-law scaling by enabling higher-quality\n",
            "models to be trained using less data. Data curation also\n",
            "unlocks new tradeoffs between training time, inference cost,\n",
            "data collection effort, and downstream performance. For\n",
            "example, if we consider the compute-constrained (single-\n",
            "epoch) regime, a data-efficient LLM training routine may\n",
            "reach the desired performance using only X% of the data\n",
            "1arXiv:2402.09668v1  [cs.LG]  15 Feb 2024How to Train Data-Efficient LLMs\n",
            "100 200 300 400 500\n",
            "Training T okens (Billion)3.803.9044.104.20Perplexity (HQ)\n",
            "44% speedup\n",
            "100 200 300 400 500\n",
            "Training T okens (Billion)86.508787.508888.50GLUE\n",
            "70% speedup\n",
            "100 200 300 400 500\n",
            "Training T okens (Billion)798081828384Super-GLUE\n",
            "50% speedup\n",
            "Full data Random Ask-LLM (XL)\n",
            "Figure 1. Data-efficient pre-training run of T5-Large ( 800M) using ASK-LLM with Flan-T5-XL as the data quality scorer. Training on\n",
            "60% of the original dataset, ASK-LLM is able to train T5-Large both better and 70% faster, compared to training on 100% of the dataset.\n",
            "(corresponding to an X% training speedup).\n",
            "Despite considerable interest from the community for build-\n",
            "ing data-efficient training methods (Sorscher et al., 2022;\n",
            "Paul et al., 2021; Coleman et al., 2020; Jiang et al., 2019;\n",
            "Katharopoulos & Fleuret, 2018), large-scale analyses of data\n",
            "pruning strategies are rare because of the extreme computa-\n",
            "tional cost—especially in the context of LLM pre-training.\n",
            "To be more specific, an extensive comparative study neces-\n",
            "sarily entails pre-training (i) various sizes of LLMs, (ii) for\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 48}\n",
            "tional cost—especially in the context of LLM pre-training.\n",
            "To be more specific, an extensive comparative study neces-\n",
            "sarily entails pre-training (i) various sizes of LLMs, (ii) for\n",
            "a variety of data sampling rates, (iii) obtained through vari-\n",
            "ous pruning strategies. Further, downstream evaluations for\n",
            "LLMs also frequently involve fine-tuning, which is resource\n",
            "intensive in itself.\n",
            "1.1. Contributions\n",
            "We hypothesize that the roles of coverage and quality de-\n",
            "pend on the stage of training, size of the model, and the\n",
            "sampling rate. To understand the coverage/quality design\n",
            "choice better, we develop new data-efficiency routines that\n",
            "independently (and solely) target quality and coverage. Our\n",
            "ASK-LLM sampler prioritizes high-quality and informative\n",
            "training samples by asking a proxy LLM. Our DENSITY\n",
            "sampler seeks to maximize the coverage of latent topics in\n",
            "the input dataset through a diversified sampling procedure.\n",
            "To summarize, our contributions are as follows:\n",
            "ASK-LLM sampling. We find that ASK-LLM can train\n",
            "better models ( vs.training on the entire dataset ) even af-\n",
            "ter removing up to 90% of training samples, while also\n",
            "consistently beating well-established data curation routines.\n",
            "We note that even a tiny proxy model in ASK-LLM (60M\n",
            "parameters) can outperform most baselines.\n",
            "Exhaustive benchmark. We implement 19different sam-\n",
            "pling strategies for pre-training T5-Large ( 800M) and T5-\n",
            "Small ( 60M) on 524B tokens and evaluate them on 111\n",
            "downstream evaluation tasks. This leads to a total of 170\n",
            "pre-training and 2,500fine-tuning runs.\n",
            "New insights. By analyzing the differences between ASK-LLM andDENSITY sampling, we study the role of coverage,\n",
            "quality, and sampling cost in LLM pre-training. We support\n",
            "our conclusions with additional studies of the convergence\n",
            "rate, correlations between sampler outputs, and impact of\n",
            "sampling cost on downstream performance.\n",
            "Takeaway. Our results show that while coverage sampling\n",
            "canrecover the performance of the full data, ASK-LLM\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 49}\n",
            "sampling cost on downstream performance.\n",
            "Takeaway. Our results show that while coverage sampling\n",
            "canrecover the performance of the full data, ASK-LLM\n",
            "(quality filtering) can often exceed it. These experiments\n",
            "suggest that LLM-based quality raters are a worthwhile and\n",
            "effective way to drive performance in pre-training.\n",
            "2. Related Work\n",
            "Data selection is a classical problem with well-established\n",
            "literature on coresets, sketching, importance sampling, filter-\n",
            "ing, denoising, and a host of other algorithms with similar\n",
            "goals. While we cannot possibly catalog the entire sampling\n",
            "literature, we hope to provide an overview of the principles\n",
            "behind common data selection algorithms. We also describe\n",
            "how these algorithms have been applied to machine learning,\n",
            "with a focus on language model training.\n",
            "2.1. Coverage Sampling\n",
            "The first class of methods maximize the coverage of the\n",
            "sample by selecting points that are evenly distributed across\n",
            "the entire input domain, e.g., an ϵ-net for a Lipschitz func-\n",
            "tion (Phillips, 2017). When training language models, cov-\n",
            "erage sampling is motivated by the intuition that we ought\n",
            "to show the model the full breadth of genres, topics, and\n",
            "languages (Longpre et al., 2023b). Coverage sampling is\n",
            "typically accomplished by embedding examples into a met-\n",
            "ric space and selecting points which are mutually far from\n",
            "each other (Lee et al., 2023).\n",
            "Cluster sampling algorithms group inputs based on embed-\n",
            "ding similarity and select representatives from each group.\n",
            "These algorithms are popular, scalable, interpretable, and\n",
            "enjoy strong theoretical support – k-means sampling prov-\n",
            "ably approximates the SVM objective (Tukan et al., 2021)\n",
            "2How to Train Data-Efficient LLMs\n",
            "CoverageQualityAsk-LLM\n",
            "Perplexity\n",
            "Prototypes\n",
            "SemDeDup\n",
            "DensityDPPs\n",
            "Figure 2. While there is no inherent tradeoff between coverage and\n",
            "quality, samplers target these metrics on a spectrum (up and to the\n",
            "left indicates a more aggressive prioritization). See Appendix B\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 50}\n",
            "Figure 2. While there is no inherent tradeoff between coverage and\n",
            "quality, samplers target these metrics on a spectrum (up and to the\n",
            "left indicates a more aggressive prioritization). See Appendix B\n",
            "for a description of the plotted samplers.\n",
            "and many others (Feldman et al., 2020). However, there are\n",
            "also recent techniques based on submodular optimization of\n",
            "a coverage score (Chen et al., 2012; Indyk et al., 2014; Bor-\n",
            "sos et al., 2020), models of the data distribution (Coleman\n",
            "et al., 2022), discrepancy minimization (Karnin & Liberty,\n",
            "2019), and deduplication through token matching / locality-\n",
            "sensitive hashing (Lee et al., 2022).\n",
            "Many variations of cluster sampling have been applied to\n",
            "vision and language model training. Sorscher et al. (2022)\n",
            "propose the “SSL prototypes” method for vision models,\n",
            "which removes points that fall too close to the nearest k-\n",
            "means centroid. SemDeDup (Abbas et al., 2023) also re-\n",
            "moves points based on this distance, but targets pairs of\n",
            "nearby examples, or “semantic duplicates,” and prefers\n",
            "points close to the centroid. The D4 sampler chains Min-\n",
            "Hash deduplication, SemDeDup, and SSL prototypes to-\n",
            "gether to prune both high-variance, sparse regions and pro-\n",
            "totypical, dense regions of LLM pre-training datasets (Tiru-\n",
            "mala et al., 2023). Coleman et al. (2020) considers a k-\n",
            "centers submodular selection routine on the last-layer em-\n",
            "beddings of ResNet vision models.\n",
            "2.2. Quality-score Sampling\n",
            "Another class of methods are based on quality scores , where\n",
            "a scoring algorithm rates every example and the sampler\n",
            "preferentially selects points with high scores. Even though\n",
            "this framework was originally developed for importance\n",
            "sampling (Hastings, 1970), the machine learning community\n",
            "has expanded the theoretical “score-and-sample” framework\n",
            "to include a variety of practical heuristics.\n",
            "For example, the selection-via-proxy (SVP) algorithm de-\n",
            "termines the importance of an input using the validation\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 51}\n",
            "to include a variety of practical heuristics.\n",
            "For example, the selection-via-proxy (SVP) algorithm de-\n",
            "termines the importance of an input using the validation\n",
            "loss and uncertainty scores of a pre-trained model on the\n",
            "input (Coleman et al., 2020; Sachdeva et al., 2021). Paul\n",
            "et al. (2021) sample according to an “EL2N score” formed\n",
            "by ensembling the losses of 10 lightly-trained models. En-\n",
            "semble prediction variance has also been used as the scoringmetric (Chitta et al., 2021), as have ensemble disagreement\n",
            "rates (Meding et al., 2021). Other scores measure whether\n",
            "an example is likely to be forgotten (Toneva et al., 2019),\n",
            "memorized (Feldman & Zhang, 2020), or un-learnable (Min-\n",
            "dermann et al., 2022).\n",
            "In the context of pre-training LLMs, there exist a few dif-\n",
            "ferent schools-of-thought for scoring the quality of train-\n",
            "ing samples. The first (and arguably most used) camp is\n",
            "perplexity-filtering, where we prioritize samples with low\n",
            "perplexity and filter out highly surprising examples (Wen-\n",
            "zek et al., 2019; Marion et al., 2023; Muennighoff et al.,\n",
            "2023). Notably, recent advancements in cheaper to run\n",
            "model-based training-run simulators for LLMs can be used\n",
            "toestimate the perplexity of a training sample instead of run-\n",
            "ning an LLM inference (Guu et al., 2023). Another group\n",
            "of methods selects training data that minimizes the distance\n",
            "between the distribution of selected data and a handcrafted\n",
            "high-quality data source (typically wikipedia and books).\n",
            "Typical ways are to do this in a feature space (Xie et al.,\n",
            "2023b) or by training a contrastive-style classifer (Radford\n",
            "et al., 2019; Anil et al., 2023; Javaheripi et al., 2023). Sim-\n",
            "ilar ideas have also been explored for optimizing the data\n",
            "mixture weights for pre-training (Xie et al., 2023a).\n",
            "In concurrent work, Maini et al. (2024) also consider an\n",
            "LLM-based approach similar to our ASK-LLM sampler,\n",
            "but with a focus on data paraphrasing rather than selection\n",
            "via quality evaluation. Engstrom et al. (2024) consider a\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 52}\n",
            "LLM-based approach similar to our ASK-LLM sampler,\n",
            "but with a focus on data paraphrasing rather than selection\n",
            "via quality evaluation. Engstrom et al. (2024) consider a\n",
            "quality evaluation based on datamodels, though their anal-\n",
            "ysis suggests that this approach selects for strongly model-\n",
            "dependent notions of quality.\n",
            "3. Methods\n",
            "We propose two samplers, ASK-LLM andDENSITY . These\n",
            "samplers have significantly different costs— ASK-LLM re-\n",
            "quires an LLM inference call for each training sample,\n",
            "whereas DENSITY is based on a diversified sampling rou-\n",
            "tine that is cheaper than even clustering the dataset. They\n",
            "also exhibit substantially different selection behavior: ASK-\n",
            "LLM conducts a highly nuanced andcontextual quality\n",
            "evaluation for each sample, while DENSITY asks whether\n",
            "we have already sampled many similar examples. By study-\n",
            "ing samplers on extreme ends of this spectrum, we hope to\n",
            "better understand the salient factors for LLM data curation.\n",
            "3.1. A SK-LLM Sampling\n",
            "Intuition. Our intuition is that humans can easily identify\n",
            "commonly occurring failure modes in state-of-the-art data\n",
            "quality scorers. Hence, it should be possible to correct\n",
            "these mistakes using the reasoning capabilities of modern\n",
            "instruction-tuned LLMs.\n",
            "To do so, in ASK-LLM , we prompt an instruction-tuned\n",
            "3How to Train Data-Efficient LLMs\n",
            "Ask-LLM prompt \n",
            "### \n",
            "This is a pretraining …. datapoint. \n",
            "### \n",
            "Does the previous paragraph demarcated within ### and ### \n",
            "contain informative signal for pre-training a large-language model? \n",
            "An informative datapoint should be well-formatted, contain some \n",
            "usable knowledge of the world, and strictly NOT have any harmful, \n",
            "racist, sexist, etc. content. \n",
            "OPTIONS: \n",
            "- yes \n",
            "- no \n",
            "Sampling score  = P(“yes” | prompt) \n",
            "Figure 3. The prompt for obtaining the sampling score for each\n",
            "training sample in A SK-LLM.\n",
            "proxy LLM with the prospective training example and ask\n",
            "whether the example should be used for training (see Fig-\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 53}\n",
            "training sample in A SK-LLM.\n",
            "proxy LLM with the prospective training example and ask\n",
            "whether the example should be used for training (see Fig-\n",
            "ure 3 for the prompt). We take the softmax probability of the\n",
            "token “ yes” as the estimated data-quality score. Consider\n",
            "the following common failure modes of perplexity filter-\n",
            "ing, which the ASK-LLM scoring model fixes (see more\n",
            "qualitative examples in Appendix E).\n",
            "Contextuality. Perplexity filters often select samples that\n",
            "lack context, e.g., containing questions without answers\n",
            "(Examples 11, 12, 15). ASK-LLM correctly identifies that\n",
            "these examples do not provide new information.\n",
            "Nonsense. Perplexity filters can select examples that end-\n",
            "lessly repeat the same phrases / words (Examples 14 and\n",
            "15), likely because these word combinations are common\n",
            "(resulting in high likelihood).\n",
            "Niche examples. Perplexity filters can reject niche topics\n",
            "that are otherwise informative, well-written, and contain\n",
            "useful tail knowledge of the world. Example 17 contains\n",
            "detailed information about a Manchester art installation\n",
            "but is assigned a high perplexity, likely because it contains\n",
            "uncommon (but valid) word combinations. Examples 20-22\n",
            "display similar behavior for other niche topics.\n",
            "3.2. D ENSITY Sampling\n",
            "Intuition. Our intuition is that the data distribution provides\n",
            "a strong coverage signal. High-probability regions contain\n",
            "“prototypical” examples—ones with many near-duplicates\n",
            "and strong representation in the dataset. Low-probability\n",
            "regions will contain outliers, noise, and unique/rare inputs.\n",
            "If we wish to maximize topic coverage, we should boost the\n",
            "signal from under-represented portions of the input domain\n",
            "and downsample redundant, high-density information.\n",
            "The key difficulty for our DENSITY sampler is to accurately\n",
            "estimate an example’s local density. Like Tirumala et al.\n",
            "(2023) (D4), we assume access to embeddings from a pre-trained LLM. However, we depart from the traditional ap-\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 54}\n",
            "estimate an example’s local density. Like Tirumala et al.\n",
            "(2023) (D4), we assume access to embeddings from a pre-trained LLM. However, we depart from the traditional ap-\n",
            "proach of clustering and opt to sample based on kernel sums.\n",
            "Given a dataset Dof embeddings and a kernel k(x, y), we\n",
            "estimate the density using the following score.\n",
            "score( y) =X\n",
            "x∈Dkλ(x, y).\n",
            "λis a smoothing parameter called the kernel bandwidth that\n",
            "controls the scale of the points’ effects. To reduce the com-\n",
            "plexity from O(N2)toO(NlogN), we use recent break-\n",
            "throughs from the algorithm community to approximate\n",
            "the sum (Siminelakis et al., 2019; Coleman & Shrivastava,\n",
            "2020). Our method resembles that of Coleman et al. (2022),\n",
            "except that (i) we adopt a two-pass sampling algorithm with\n",
            "stronger theoretical guarantees (Theorem A.2) and (ii) we\n",
            "perform the density estimation in the latent space of the\n",
            "model, rather than using Jaccard distances on n-grams.\n",
            "3.3. Sampling Techniques\n",
            "DENSITY andASK-LLM are both scoring methods that\n",
            "reduce an example to a floating point value that measures\n",
            "coverage or quality. Once we have scores for a complete\n",
            "dataset of training examples (sentences, paragraphs, etc.),\n",
            "we can make score-based decisions about which examples\n",
            "to include in the training set.\n",
            "Top / Bottom K.The simplest method is to sort examples\n",
            "by score and accept the top or bottom K. While straight-\n",
            "forward, this approach is supported by the “permutation”\n",
            "theory of Sorscher et al. (2022), and sensitivity score sam-\n",
            "pling (a softened version) is the core subroutine for many\n",
            "coresets (Mai et al., 2021). When applied to DENSITY and\n",
            "perplexity scores, top- Ksampling selects for the head of\n",
            "the data distribution (similar to SSL prototypes). Bottom- K\n",
            "sampling selects the tail and removes common items.\n",
            "Inverse Propensity Sampling. Inverse propensity sam-\n",
            "pling (IPS) selects items proportional to their reweighted\n",
            "and normalized inverse score (Rosenbaum & Rubin, 1983).\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 55}\n",
            "Inverse Propensity Sampling. Inverse propensity sam-\n",
            "pling (IPS) selects items proportional to their reweighted\n",
            "and normalized inverse score (Rosenbaum & Rubin, 1983).\n",
            "When applied to D ENSITY or perplexity scores, IPS imple-\n",
            "ments a form of diversified sampling that uniformizes the\n",
            "distribution of selected inputs (Theorem A.2).\n",
            "In our experiments, the DENSITY sampler uses IPS to max-\n",
            "imize the coverage of the dataset.1For our ASK-LLM\n",
            "filter, we adopt top- ksampling because we expect the “yes”\n",
            "probability to be a reliable and strong measure of quality.\n",
            "3.4. Relationships Between Methods\n",
            "DENSITY , Perplexity, and Loss. When a language model\n",
            "is trained to minimize perplexity, the LLM itself is a data\n",
            "distribution model. Therefore, the perplexity and loss fil-\n",
            "1We also implemented top- Kand bottom- Ksampling, but\n",
            "these samplers do not maintain coverage and perform poorly.\n",
            "4How to Train Data-Efficient LLMs\n",
            "tering approaches of Marion et al. (2023), Muennighoff\n",
            "et al. (2023), and other authors can be viewed as model-\n",
            "based density sampling. However, our sampler measures\n",
            "the density of the training dataset in a latent geometric space,\n",
            "while perplexity measures the likelihood under the scoring\n",
            "model. The samplers also differ in terms of decision com-\n",
            "plexity. Thanks to the capacity of the LLM, a perplexity\n",
            "filter can make highly-nuanced decisions between two texts\n",
            "on the same topic. On the other hand, our DENSITY sam-\n",
            "pler is constructed from a simple nonparametric density\n",
            "model (Rosenblatt, 1956) that does not have the capacity to\n",
            "distinguish examples at such a granular level.\n",
            "ASK-LLM and Perplexity. Perplexity filters exhibit a\n",
            "strong in-distribution bias, making decisions based on the\n",
            "data used to train the scoring model (not the dataset we\n",
            "wish to sample). By using the LLM for quality evaluation\n",
            "rather than likelihood estimation, our sampler can escape\n",
            "this bias because the additional context and alternative task\n",
            "change the sampling distribution. This occurs even when\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 56}\n",
            "rather than likelihood estimation, our sampler can escape\n",
            "this bias because the additional context and alternative task\n",
            "change the sampling distribution. This occurs even when\n",
            "the A SK-LLM and perplexity models are the same size.\n",
            "DENSITY and Clustering. The kernel sum procedure at\n",
            "the core of our DENSITY sampler operates on embedding-\n",
            "similarity relationships in a similar way to D4, SemDeDup,\n",
            "and SSL prototypes. Indeed, near-duplicate detection can be\n",
            "viewed as a discretized version of similarity-based density\n",
            "estimation (Kirsch & Mitzenmacher, 2006). Outlier rejec-\n",
            "tion, which motivates the “nearest-to-centroid” heuristic of\n",
            "SSL prototypes, also has intimate connections with density\n",
            "estimation (Schubert et al., 2014).\n",
            "Intuition. Perplexity should be viewed as a “difficulty”\n",
            "or “quality” score rather than as a coverage-maximizing\n",
            "score. Our ASK-LLM sampler should be viewed as a con-\n",
            "textualized quality score that incorporates reasoning.2Our\n",
            "DENSITY sampler is a pure “coverage” score in the latent\n",
            "representation space, while SemDeDup, and SSL Prototypes\n",
            "all incorporate quality / outlier filtering to some extent ( e.g.,\n",
            "by preferring points near / far from a centroid).\n",
            "4. Experiments\n",
            "4.1. Models\n",
            "We pre-train T5-style models (Raffel et al., 2020), which\n",
            "belong to the encoder-decoder family of Transformer mod-\n",
            "els and offer competitive performance on many tasks (Shen\n",
            "et al., 2023). See Phuong & Hutter (2022) for a formal\n",
            "introduction to various Transformer model configurations.\n",
            "We train T5-Small ( 60M) and T5-Large ( 800M), reusing all\n",
            "of the training settings from the original T5 implementation\n",
            "except the batch size ( 2048→1024) . We train on batches\n",
            "2Note that ASK-LLM may also incidentally improve coverage\n",
            "because it does not suffer from in-distribution bias.of1024 sequences of length 512for1M steps.\n",
            "For the quality-based data samplers ( ASK-LLM and Per-\n",
            "plexity filtering) we use proxy quality scoring models of\n",
            "five different sizes: T5-{Small, Base, Large, XL, XXL}.\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 57}\n",
            "For the quality-based data samplers ( ASK-LLM and Per-\n",
            "plexity filtering) we use proxy quality scoring models of\n",
            "five different sizes: T5-{Small, Base, Large, XL, XXL}.\n",
            "ForASK-LLM , we use FLAN-T5. For ASK-LLM , we\n",
            "use FLAN-T5, which are the same sizes but have been\n",
            "instruction-tuned on Flan (Longpre et al., 2023a).\n",
            "4.2. Datasets\n",
            "We use the C4 dataset3, which was also used for pre-training\n",
            "the original T5. The C4 dataset is a version of the Common\n",
            "Crawl—a publicly available archive of web-text—that has\n",
            "been pre-processed using several heuristics (Raffel et al.,\n",
            "2020, Section 2.2). In its entirety, the C4 dataset contains\n",
            "184B tokens. We use our algorithms (see Appendix B for a\n",
            "list) to sample {10,20,40,60,80}% of C4.\n",
            "Because a low sampling ratio yields exceedingly small\n",
            "datasets, we choose to train in the iso-compute setting, i.e.,\n",
            "training all models for exactly 524B tokens. This results in\n",
            "more epochs (repetitions) at smaller sampling rates. We be-\n",
            "lieve this gives each data curation method an equal chance\n",
            "to maximize model performance, and not penalize meth-\n",
            "ods that sample a small number of high-quality repeatable\n",
            "tokens vs.large number of non-repeatable tokens. See\n",
            "Appendix B, Figure 8 for a demonstration of this process.\n",
            "4.3. Evaluation\n",
            "We use 111 downstream evaluation tasks to assess di-\n",
            "verse performance indicators for pre-trained LLMs (see\n",
            "Appendix C for a complete list). In addition to these indi-\n",
            "vidual tasks, to compare a normalized average performance\n",
            "improvement over all downstream evaluations, we devise a\n",
            "metric called “over-scaling.”\n",
            "Over-scaling (%) measures the relative improvement of\n",
            "a model when compared against the next-largest model\n",
            "size, averaged over alldownstream evaluations listed in\n",
            "Appendix C. For example, a T5-Large variant with 100%\n",
            "over-scaling performs at the same level as T5-XL, while\n",
            "the standard T5-Large model would have an over-scaling of\n",
            "0%. We call this metric over-scaling because it measures\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 58}\n",
            "over-scaling performs at the same level as T5-XL, while\n",
            "the standard T5-Large model would have an over-scaling of\n",
            "0%. We call this metric over-scaling because it measures\n",
            "the extent to which the performance exceeds the level we\n",
            "would expect from naïvely scaling up the model or data.\n",
            "We compute the metric by normalizing the performance\n",
            "improvement from sampling, e.g., for T5-Large:\n",
            "E\n",
            "metric\u0014\n",
            "100·∆metric(T5-L(Dsampled ),T5-L(Dfull)\n",
            "∆metric(T5-XL (Dfull),T5-L(Dfull)\u0015\n",
            "where ∆metric(A,B) =Perf metric(A)−Perf metric(B).\n",
            "3www.tensorflow.org/datasets/catalog/c4\n",
            "5How to Train Data-Efficient LLMs\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)3.683.763.843.9244.08Perplexity (C4)\n",
            "T5-Large\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)3.803.9044.104.204.30Perplexity (HQ)\n",
            "T5-Large\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)-60-45-30-1501530Over-scaling (%)\n",
            "T5-Large\n",
            "Full data\n",
            "RandomDensity\n",
            "Ask-LLM (XL)Perplexity (XL) Prototypes SemDeDup\n",
            "(a)\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)3.683.763.843.9244.08Perplexity (C4)\n",
            "T5-Large\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)3.803.9044.104.204.30Perplexity (HQ)\n",
            "T5-Large\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)-60-45-30-1501530Over-scaling (%)\n",
            "T5-Large\n",
            "Full data\n",
            "RandomDensity\n",
            "Ask-LLM (XL)Perplexity (XL) Prototypes SemDeDup (b)\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)3.683.763.843.9244.08Perplexity (C4)\n",
            "T5-Large\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)3.803.9044.104.204.30Perplexity (HQ)\n",
            "T5-Large\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)-60-45-30-1501530Over-scaling (%)\n",
            "T5-Large\n",
            "Full data\n",
            "RandomDensity\n",
            "Ask-LLM (XL)Perplexity (XL) Prototypes SemDeDup (c)\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)6.306.456.606.756.907.05Perplexity (C4)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)6.907.207.507.808.108.40Perplexity (HQ)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)-24-18-12-606Over-scaling (%)\n",
            "T5-Small\n",
            "Full data\n",
            "RandomDensity\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 59}\n",
            "Dataset Size (Billion T okens)6.907.207.507.808.108.40Perplexity (HQ)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)-24-18-12-606Over-scaling (%)\n",
            "T5-Small\n",
            "Full data\n",
            "RandomDensity\n",
            "Ask-LLM (Small)Ask-LLM (XL)\n",
            "Perplexity (Small)Perplexity (XL)\n",
            "PrototypesSemDeDup\n",
            "(d)\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)6.306.456.606.756.907.05Perplexity (C4)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)6.907.207.507.808.108.40Perplexity (HQ)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)-24-18-12-606Over-scaling (%)\n",
            "T5-Small\n",
            "Full data\n",
            "RandomDensity\n",
            "Ask-LLM (Small)Ask-LLM (XL)\n",
            "Perplexity (Small)Perplexity (XL)\n",
            "PrototypesSemDeDup (e)\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)6.306.456.606.756.907.05Perplexity (C4)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)6.907.207.507.808.108.40Perplexity (HQ)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)-24-18-12-606Over-scaling (%)\n",
            "T5-Small\n",
            "Full data\n",
            "RandomDensity\n",
            "Ask-LLM (Small)Ask-LLM (XL)\n",
            "Perplexity (Small)Perplexity (XL)\n",
            "PrototypesSemDeDup (f)\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)6.306.456.606.756.907.05Perplexity (C4)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)6.907.207.507.808.108.40Perplexity (HQ)\n",
            "T5-Small\n",
            "20 40 75 115 150 184\n",
            "Dataset Size (Billion T okens)-24-18-12-606Over-scaling (%)\n",
            "T5-Small\n",
            "Full data\n",
            "RandomDensity\n",
            "Ask-LLM (Small)Ask-LLM (XL)\n",
            "Perplexity (Small)Perplexity (XL)\n",
            "PrototypesSemDeDup\n",
            "Figure 4. Tradeoff between data quantity and model quality for T5-Small and T5-Large pre-training. Each point corresponds to a\n",
            "converged pre-training run over a sub-sample. C4 perplexity is over the in-distribution validation subset of C4, while HQ perplexity is for\n",
            "a higher-quality validation set (lower is better). Over-scaling measures the extent to which the sampling routine closes the performance\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 60}\n",
            "a higher-quality validation set (lower is better). Over-scaling measures the extent to which the sampling routine closes the performance\n",
            "gap with the next-larger (non-sampled) model (higher is better). Not all methods are shown in Figure 4 or Table 1; see Appendix D.\n",
            "Table 1. Comparison of sampling algorithms at a fixed sample size. For each sampling strategy, we sample the dataset to X% of the\n",
            "original size and pre-train T5-Small and T5-Large for 524B tokens. This table is a cross-section of Figure 4 but with more metrics.\n",
            "LLMTraining config.\n",
            "Over-scaling (%)Downstream tasks FLAN Instruction Tuning\n",
            "Sampler # Tokens GLUE SuperGLUE CNN/DM SQuAD MMLU BBH Reasoning QA\n",
            "T5-Small — 184B 0.0 79.9 58.6 18.6 78.6 25.5 28.5 15.2 37.0\n",
            "T5-Small Random 36B ( ≡20%) -0.2 79.9 58.3 18.6 78.1 26.9 27.8 15.2 38.1\n",
            "T5-Small Density 36B ( ≡20%) -2.1 80.5 59.7 18.5 78.4 28.1 30.3 14.5 33.4\n",
            "T5-Small SemDeDup 46B ( ≡25%) -4.5 80.7 59.2 18.4 77.8 28.0 26.6 14.8 37.0\n",
            "T5-Small Prototypes 46B ( ≡25%) -8.0 79.7 58.8 18.5 78.0 26.8 27.7 15.7 34.2\n",
            "T5-Small Perplexity (Small) 36B ( ≡20%) -7.8 79.9 58.4 18.4 77.5 28.1 28.2 15.0 35.0\n",
            "T5-Small Ask-LLM (XL) 36B ( ≡20%) 4.2 80.3 59.8 18.6 79.1 29.9 28.5 15.8 36.4\n",
            "T5-Large — 184B 0.0 88.2 82.5 20.8 86.7 40.7 33.6 21.6 73.0\n",
            "T5-Large Random 36B ( ≡20%) -6.5 88.6 82.8 20.7 86.1 43.3 34.8 18.6 70.1\n",
            "T5-Large Density 36B ( ≡20%) 2.8 88.8 82.4 20.8 86.4 41.4 35.4 19.4 72.8\n",
            "T5-Large SemDeDup 46B ( ≡25%) -20.5 88.3 81.4 20.7 86.0 41.2 36.7 21.8 70.2\n",
            "T5-Large Prototypes 46B ( ≡25%) 0.2 88.4 82.7 20.8 87.0 40.0 35.5 17.6 71.1\n",
            "T5-Large Perplexity (XL) 36B ( ≡20%) -32.7 87.9 81.8 20.6 85.7 38.1 33.9 20.0 69.0\n",
            "T5-Large Ask-LLM (XL) 36B ( ≡20%) 33.0 88.8 83.0 21.0 87.3 43.6 33.0 20.0 77.1\n",
            "4.4. Does reasoning improve data efficiency?\n",
            "Figure 4c shows that ASK-LLM closes up to 33% of the\n",
            "performance gap to the next-largest model size ( i.e., the\n",
            "over-scaling metric). ASK-LLM consistently outperforms\n",
            "training on the full dataset as well as perplexity filtering\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 61}\n",
            "performance gap to the next-largest model size ( i.e., the\n",
            "over-scaling metric). ASK-LLM consistently outperforms\n",
            "training on the full dataset as well as perplexity filtering\n",
            "(and coverage-maximizing baselines), despite having accessto a scoring model of the same model capacity (XL). Similar\n",
            "findings hold true for training efficiency (Figure 5). ASK-\n",
            "LLM converges faster than perplexity filters, both in terms\n",
            "of the average (expected final performance over all proxy\n",
            "model sizes) and pointwise for the best configuration (Small\n",
            "and XL for training T5-Small and T5-Large).\n",
            "6How to Train Data-Efficient LLMs\n",
            "200 250 300 350 400 450 500 5503.84.04.24.4Perplexity (HQ)\n",
            "T5-Large\n",
            "100 200 300 400 50086.086.587.087.588.088.5GLUE\n",
            "T5-Large\n",
            "100 200 300 400 50076788082Super-GLUE\n",
            "T5-Large\n",
            "200 250 300 350 400 450 500 550\n",
            "Training T okens (Billion)6.87.07.27.47.67.88.0Perplexity (HQ)\n",
            "T5-Small\n",
            "100 200 300 400 500\n",
            "Training T okens (Billion)79.079.580.080.5GLUE\n",
            "T5-Small\n",
            "100 200 300 400 500\n",
            "Training T okens (Billion)56575859Super-GLUE\n",
            "T5-Small\n",
            "Ask-LLM (Avg.)\n",
            "Perplexity (Avg.)Ask-LLM (XL)\n",
            "Perplexity (XL)Ask-LLM (Small)\n",
            "Perplexity (Small)Full Data\n",
            "Figure 5. Training efficiency comparison between two quality-score based samplers: ASK-LLM and Perplexity filtering. ASK-LLM\n",
            "(Avg.) and Perplexity filtering (Avg.) represent the training run averaged across (i) proxy model sizes, i.e., T5-{Small, Base, Large, XL,\n",
            "XXL}; and (ii) sampling ratios, i.e., {10, 20, 40, 60, 80}%. The training runs for ASK-LLM and perplexity filtering with T5-{Small,\n",
            "XL} specifically are averaged only over the sampling ratios. Each point in this plot is the (averaged) performance of an intermediate\n",
            "checkpoint during the course of training on sampled data.\n",
            "Figure 7 further demonstrates that prompting adds criti-\n",
            "cal information to the sampler not present in perplexity:\n",
            "ASK-LLM scores show no correlation with the perplex-\n",
            "ity scores. Based on this clear behavioral difference, we\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 62}\n",
            "cal information to the sampler not present in perplexity:\n",
            "ASK-LLM scores show no correlation with the perplex-\n",
            "ity scores. Based on this clear behavioral difference, we\n",
            "conclude that reasoning and context are crucial ingredients.\n",
            "We expect prompting techniques such as chain-of-thought\n",
            "reasoning (Wei et al., 2022) to further drive performance.\n",
            "4.5. When are expensive quality scores justified?\n",
            "Figures 4c and 4f suggest that coverage scores—especially\n",
            "those provided by DENSITY —perform well in the mid-\n",
            "data regime (roughly 25% to 50% sampling rate). On\n",
            "the other hand, expensive quality scoring—via the ASK-\n",
            "LLM procedure—is Pareto optimal for the entire quantity-\n",
            "quality trade-off. The higher costs of LLM-based filters\n",
            "are most justified in two scenarios: (i) improving full-data\n",
            "performance, where quality filtering by removing the lowest-\n",
            "quality data is the main way to push the upper limit of model\n",
            "performance; or (ii) in the low-data regime, where keeping\n",
            "only the highest-quality data drives the most model perfor-\n",
            "mance compared to other sampling strategies.\n",
            "We also observe that random sampling is a strong baseline,\n",
            "aligning with recent observations in the literature. Guo\n",
            "et al. (2022a) found that only three methods outperformed\n",
            "random sampling in a computer vision benchmark of 15\n",
            "algorithms. Ayed & Hayou (2023a) prove the existence of\n",
            "adversarial problem instances where score-based samplingcannot outperform random sampling. These results only\n",
            "serve to highlight the significance of A SK-LLM’s gains.\n",
            "4.6. Effect of quality-scoring model capacity\n",
            "Figure 6 demonstrates a clear scaling trend for ASK-LLM ’s\n",
            "quality-scoring model: larger scoring models are increas-\n",
            "ingly beneficial as the scale of the to-be-trained LLM in-\n",
            "creases. Perplexity filters do not seem to exhibit such trends.\n",
            "The strongly consistent scaling for ASK-LLM also suggests\n",
            "an interesting performance-recipe: to improve downstream\n",
            "data-efficiency, use better quality-scoring models. Creating\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 63}\n",
            "The strongly consistent scaling for ASK-LLM also suggests\n",
            "an interesting performance-recipe: to improve downstream\n",
            "data-efficiency, use better quality-scoring models. Creating\n",
            "better quality scorers for ASK-LLM (via fine-tuning, chain-\n",
            "of-thought prompting, more capable scoring models, etc.)\n",
            "is thus an exciting direction for future work.\n",
            "Despite the scaling trends, we would also like to empha-\n",
            "size that even small ASK-LLM models provide compelling\n",
            "sampling performance already for both training T5-Small\n",
            "and T5-Large models. For example, ASK-LLM (Small)\n",
            "outperforms perplexity filtering with anyscoring-model in\n",
            "Figure 4f (including T5-XXL) by a sizable margin.\n",
            "4.7. Do samplers prioritize different examples?\n",
            "To understand whether different algorithms prioritize differ-\n",
            "ent examples, we sorted examples by score and computed\n",
            "the Kendall Tau rank correlation between samplers (Fig-\n",
            "ure 7). We find that samplers differ in significant and inter-\n",
            "7How to Train Data-Efficient LLMs\n",
            "Small Base Large XL XXL3\n",
            "2\n",
            "1\n",
            "0123Rank ( )\n",
            "Ask-LLM Sampling\n",
            "Small Base Large XL XXL\n",
            "Proxy Model Size3\n",
            "2\n",
            "1\n",
            "0123Rank ( )\n",
            "Perplexity Filtering\n",
            "Figure 6. We investigate the change in ranking of quality-scoring\n",
            "models when pre-training different LLMs. A positive ∆Rank\n",
            "indicates that the scorer’s task-averaged rank within {Small, Base,\n",
            "Large, XL, XXL} increased when training T5-Large vs.T5-Small.\n",
            "esting ways. For example, the “T5-Large” row shows that\n",
            "(i) T5-Large outputs perplexity scores similar to T5-Small\n",
            "early in training, but becomes progressively more nuanced\n",
            "on the path from 20k to 700k training steps, and (ii) perplex-\n",
            "ity and ASK-LLM select for wildly different criteria, with\n",
            "almost no ranking correlation.\n",
            "DENSITY prioritizes coverage over de-noising, maintain-\n",
            "ing the in-distribution test perplexity better than any other\n",
            "strategy (Figures 4a and 4d). This suggests that coverage\n",
            "sampling preserves the objective function, in contrast with\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 64}\n",
            "ing the in-distribution test perplexity better than any other\n",
            "strategy (Figures 4a and 4d). This suggests that coverage\n",
            "sampling preserves the objective function, in contrast with\n",
            "other methods that preferentially select for quality in addi-\n",
            "tion to diversity.\n",
            "5. Discussion\n",
            "Amortized scoring. TheASK-LLM and perplexity scorers\n",
            "require considerable computation—one LLM inference call\n",
            "for every training sample—which is concerning from both a\n",
            "carbon-emissions and cost perspective (Strubell et al., 2019).\n",
            "However, we argue that the scoring costs are amortized over\n",
            "Flan T5-Small\n",
            "Flan T5-Base\n",
            "Flan T5-Large\n",
            "Flan T5-XL\n",
            "Flan T5-XXL\n",
            "T5-Small\n",
            "T5-Base\n",
            "T5-Large\n",
            "T5-XL\n",
            "T5-XXL\n",
            "T5-Large (20k)\n",
            "T5-Large (100k)\n",
            "T5-Large (300k)\n",
            "T5-Large (500k)\n",
            "T5-Large (700k)\n",
            "Density\n",
            "Prototypes\n",
            "SemDeDupFlan T5-Small\n",
            "Flan T5-Base\n",
            "Flan T5-Large\n",
            "Flan T5-XL\n",
            "Flan T5-XXL\n",
            "T5-Small\n",
            "T5-Base\n",
            "T5-Large\n",
            "T5-XL\n",
            "T5-XXL\n",
            "T5-Large (20k)\n",
            "T5-Large (100k)\n",
            "T5-Large (300k)\n",
            "T5-Large (500k)\n",
            "T5-Large (700k)\n",
            "Density\n",
            "Prototypes\n",
            "SemDeDup1.00 0.19 -0.03 0.11 -0.04 0.01 -0.01 -0.02 -0.03 -0.04 0.02 0.00 -0.00 -0.01 -0.01 0.00 -0.25 -0.21\n",
            "0.19 1.00 0.09 0.24 0.15 -0.03 -0.04 -0.05 -0.05 -0.05 -0.04 -0.04 -0.04 -0.05 -0.05 0.00 -0.26 -0.21\n",
            "-0.03 0.09 1.00 0.01 -0.11 0.04 0.04 0.04 0.05 0.05 0.03 0.04 0.05 0.05 0.05 -0.00 0.01 -0.03\n",
            "0.11 0.24 0.01 1.00 0.42 0.06 0.04 0.04 0.03 0.02 0.06 0.05 0.04 0.04 0.04 0.00 -0.20 -0.18\n",
            "-0.04 0.15 -0.11 0.42 1.00 0.06 0.06 0.05 0.04 0.04 0.06 0.05 0.05 0.05 0.05 0.00 -0.14 -0.13\n",
            "0.01 -0.03 0.04 0.06 0.06 1.00 0.79 0.73 0.67 0.63 0.82 0.79 0.76 0.75 0.74 -0.00 -0.07 -0.13\n",
            "-0.01 -0.04 0.04 0.04 0.06 0.79 1.00 0.82 0.77 0.72 0.78 0.84 0.84 0.84 0.83 -0.00 -0.05 -0.12\n",
            "-0.02 -0.05 0.04 0.04 0.05 0.73 0.82 1.00 0.83 0.79 0.71 0.79 0.83 0.84 0.84 -0.00 -0.04 -0.11\n",
            "-0.03 -0.05 0.05 0.03 0.04 0.67 0.77 0.83 1.00 0.84 0.66 0.74 0.78 0.80 0.80 -0.00 -0.03 -0.11\n",
            "-0.04 -0.05 0.05 0.02 0.04 0.63 0.72 0.79 0.84 1.00 0.62 0.70 0.74 0.76 0.76 -0.00 -0.02 -0.10\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 65}\n",
            "-0.03 -0.05 0.05 0.03 0.04 0.67 0.77 0.83 1.00 0.84 0.66 0.74 0.78 0.80 0.80 -0.00 -0.03 -0.11\n",
            "-0.04 -0.05 0.05 0.02 0.04 0.63 0.72 0.79 0.84 1.00 0.62 0.70 0.74 0.76 0.76 -0.00 -0.02 -0.10\n",
            "0.02 -0.04 0.03 0.06 0.06 0.82 0.78 0.71 0.66 0.62 1.00 0.80 0.76 0.74 0.74 -0.00 -0.07 -0.14\n",
            "0.00 -0.04 0.04 0.05 0.05 0.79 0.84 0.79 0.74 0.70 0.80 1.00 0.86 0.84 0.83 -0.00 -0.06 -0.13\n",
            "-0.00 -0.04 0.05 0.04 0.05 0.76 0.84 0.83 0.78 0.74 0.76 0.86 1.00 0.90 0.88 -0.00 -0.06 -0.13\n",
            "-0.01 -0.05 0.05 0.04 0.05 0.75 0.84 0.84 0.80 0.76 0.74 0.84 0.90 1.00 0.91 -0.00 -0.05 -0.12\n",
            "-0.01 -0.05 0.05 0.04 0.05 0.74 0.83 0.84 0.80 0.76 0.74 0.83 0.88 0.91 1.00 -0.00 -0.05 -0.12\n",
            "0.00 0.00 -0.00 0.00 0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00\n",
            "-0.25 -0.26 0.01 -0.20 -0.14 -0.07 -0.05 -0.04 -0.03 -0.02 -0.07 -0.06 -0.06 -0.05 -0.05 -0.00 1.00 0.68\n",
            "-0.21 -0.21 -0.03 -0.18 -0.13 -0.13 -0.12 -0.11 -0.11 -0.10 -0.14 -0.13 -0.13 -0.12 -0.12 -0.00 0.68 1.000.2\n",
            " 0.0 0.2 0.4 0.6 0.8 1.0\n",
            "Figure 7. Kendall’s Tau correlation amongst the scores from mod-\n",
            "els in the ASK-LLM (first 5) and perplexity filtering (next 10)\n",
            "frameworks over 500k randomly selected training samples.\n",
            "many pre-training runs , which together cost significantly\n",
            "more than the ASK-LLM inference calls (Luccioni et al.,\n",
            "2023). In practical systems, cheaper samplers / scoring\n",
            "models can also pre-filter examples for our more expensive\n",
            "scorers. While LLM pre-training is often thought of as a\n",
            "one-time cost, this has historically not been the case. We\n",
            "therefore view quality scores as a long-term investment.\n",
            "See Appendix A.1 for a deeper discussion about the cost of\n",
            "ASK-LLM scoring.\n",
            "LLM-Based Data Refinement. Recursively training on\n",
            "model-generated data causes degredation in both diffusion\n",
            "models and LLMs, inciting concerns about whether the inter-\n",
            "net will remain a viable source of training data (Shumailov\n",
            "et al., 2023; Alemohammad et al., 2023; Briesch et al., 2023).\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 66}\n",
            "models and LLMs, inciting concerns about whether the inter-\n",
            "net will remain a viable source of training data (Shumailov\n",
            "et al., 2023; Alemohammad et al., 2023; Briesch et al., 2023).\n",
            "It is therefore somewhat surprising that LLMs are so ef-\n",
            "fective at deciding which training data to consume. Our\n",
            "ASK-LLM results raise important questions about whether\n",
            "LLM-based filters can function as an intervention in the\n",
            "self-consumption loop, allowing LLMs to self-improve.\n",
            "6. Conclusion\n",
            "We studied the performance of sampling algorithms that\n",
            "select high-quality data through highly-capable proxies and\n",
            "maximize coverage through embedding similarity. Our ex-\n",
            "periments reveal that LLM-based quality filtering yields a\n",
            "Parteo optimal efficiency tradeoff between data quantity and\n",
            "model quality, with important implications for training cost,\n",
            "self-improvement, and LLM training data curation.\n",
            "8How to Train Data-Efficient LLMs\n",
            "Impact Statement\n",
            "While increased LLM accessibility has well-documented\n",
            "risks, we expect data-efficient pre-training to be a net social\n",
            "good that reduces (amortized) carbon emissions and pre-\n",
            "training cost while improving quality.\n",
            "Acknowledgements\n",
            "We sincerely thank Xinyun Chen and Kelvin Guu for their\n",
            "insightful feedback on early drafts of this paper.\n",
            "{'filename': '2402.09668_v1.pdf', 'title': 'How to Train Data-Efficient LLMs', 'vs_index': 67}\n",
            "ORPO : Monolithic Preference Optimization without Reference Model\n",
            "Jiwoo Hong Noah Lee James Thorne\n",
            "KAIST AI\n",
            "{jiwoo_hong, noah.lee, thorne}@kaist.ac.kr\n",
            "Abstract\n",
            "While recent preference alignment algorithms\n",
            "for language models have demonstrated promis-\n",
            "ing results, supervised fine-tuning (SFT) re-\n",
            "mains imperative for achieving successful con-\n",
            "vergence. In this paper, we study the cru-\n",
            "cial role of SFT within the context of pref-\n",
            "erence alignment, emphasizing that a minor\n",
            "penalty for the disfavored generation style is\n",
            "sufficient for preference-aligned SFT. Build-\n",
            "ing on this foundation, we introduce a straight-\n",
            "forward and innovative reference model-free\n",
            "monolithic odds ratio preference optimization\n",
            "algorithm, ORPO , eliminating the necessity for\n",
            "an additional preference alignment phase. We\n",
            "demonstrate, both empirically and theoretically,\n",
            "that the odds ratio is a sensible choice for con-\n",
            "trasting favored and disfavored styles during\n",
            "SFT across the diverse sizes from 125M to 7B.\n",
            "Specifically, fine-tuning Phi-2 (2.7B), Llama-2\n",
            "(7B), and Mistral (7B) with ORPO on the Ul-\n",
            "traFeedback alone surpasses the performance\n",
            "of state-of-the-art language models with more\n",
            "than 7B and 13B parameters: achieving up to\n",
            "12.20% on AlpacaEval2.0(Figure 1), 66.19%\n",
            "on IFEval (instruction-level loose, Table 6), and\n",
            "7.32 in MT-Bench (Figure 12). We release\n",
            "code1and model checkpoints for Mistral- ORPO -\n",
            "α(7B)2and Mistral- ORPO -β(7B).3\n",
            "1 Introduction\n",
            "Pre-trained language models (PLMs) with vast\n",
            "training corpora such as web texts (Gokaslan and\n",
            "Cohen, 2019; Penedo et al., 2023) or textbooks (Li\n",
            "et al., 2023c) have shown remarkable abilities in\n",
            "diverse natural language processing (NLP) tasks\n",
            "(Brown et al., 2020; Zhang et al., 2022; Touvron\n",
            "et al., 2023; Jiang et al., 2023; Almazrouei et al.,\n",
            "2023). However, the models must undergo further\n",
            "1https://github.com/xfactlab/orpo\n",
            "2https://huggingface.co/kaist-ai/\n",
            "mistral-orpo-alpha\n",
            "3https://huggingface.co/kaist-ai/\n",
            "mistral-orpo-beta\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 68}\n",
            "2023). However, the models must undergo further\n",
            "1https://github.com/xfactlab/orpo\n",
            "2https://huggingface.co/kaist-ai/\n",
            "mistral-orpo-alpha\n",
            "3https://huggingface.co/kaist-ai/\n",
            "mistral-orpo-beta\n",
            "4.967.79.448.3510.9911.3312.2Llama−2 MistralAlpacaEval 2.0\n",
            "Llama (7B) Llama (13B) Llama−ORPO (7B) Zephyr−a Zephyr−b Mistral−ORPO−a Mistral−ORPO−b0.02.55.07.510.012.5Win Rate (%)\n",
            "Algorithm RLHF DPO ORPOFigure 1: AlpacaEval2.0result of Llama-2 (7B) and\n",
            "Mistral (7B) fine-tuned with ORPO (blue) in comparison\n",
            "to the state-of-the-art models. Notably, Mistral- ORPO -α\n",
            "&βsurpasses Zephyr βand Llama-2-Chat (13B) with a\n",
            "single epoch training exclusively on the UltraFeedback.\n",
            "tuning to be usable in general-domain applications,\n",
            "typically through processes such as instruction tun-\n",
            "ingandpreference alignment .\n",
            "Instruction-tuning (Wei et al., 2022; Taori et al.,\n",
            "2023; Wang et al., 2023; Zhou et al., 2023a) trains\n",
            "models to follow task descriptions given in natu-\n",
            "ral language, which enables models to generalize\n",
            "well to previously unseen tasks. However, despite\n",
            "the ability to follow instructions, models may gen-\n",
            "erate harmful or unethical outputs (Carlini et al.,\n",
            "2021; Gehman et al., 2020; Pryzant et al., 2023).\n",
            "To further align these models with human values,\n",
            "additional training is required with pairwise prefer-\n",
            "ence data using techniques such as reinforcement\n",
            "learning with human feedback (Ziegler et al., 2020;\n",
            "Stiennon et al., 2022, RLHF) and direct preference\n",
            "optimization (Rafailov et al., 2023, DPO).\n",
            "Preference alignment methods have demon-\n",
            "strated success in several downstream tasks beyond\n",
            "reducing harm. For example, improving factuality\n",
            "(Tian et al., 2023; Cheng et al., 2024; Chen and\n",
            "Li, 2024), code-based question answering (Gorba-\n",
            "tovski and Kovalchuk, 2024), and machine transla-\n",
            "tion (Ramos et al., 2023). The versatility of align-\n",
            "ment algorithms over a wide range of downstreamarXiv:2403.07691v2  [cs.CL]  14 Mar 2024SFT\n",
            "Reward Model\n",
            "SFT\n",
            "RLHFChosen Responses\n",
            "Rejected ResponsesStrong\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 69}\n",
            "ment algorithms over a wide range of downstreamarXiv:2403.07691v2  [cs.CL]  14 Mar 2024SFT\n",
            "Reward Model\n",
            "SFT\n",
            "RLHFChosen Responses\n",
            "Rejected ResponsesStrong \n",
            "Adaptation\n",
            "Weak PenaltyPre-trained\n",
            "Ref.\n",
            " Policy\n",
            "Ref.\n",
            " Policy\n",
            "DPO ORPO\n",
            "Odds Ratio Preference OptimizationFigure 2: Comparison of model alignment techniques. ORPO aligns the language model without a reference model in\n",
            "a single-step manner by assigning a weak penalty to the rejected responses and a strong adaptation signal to the\n",
            "chosen responses with a simple log odds ratio term appended to the negative log-likelihood loss.\n",
            "tasks highlights the necessity of understanding the\n",
            "alignment procedure and further improving the al-\n",
            "gorithms in terms of efficiency and performance.\n",
            "However, existing preference alignment methods\n",
            "normally consist of a multi-stage process, as shown\n",
            "in Figure 2, typically requiring a second refer-\n",
            "ence model and a separate warm-up phase with\n",
            "supervised fine-tuning (SFT) (Ziegler et al., 2020;\n",
            "Rafailov et al., 2023; Wu et al., 2023).\n",
            "In this paper, we study the role and impact of\n",
            "SFT in pairwise preference datasets for model\n",
            "alignment in Section 3 and propose a simple and\n",
            "novel monolithic alignment method, odds ratio\n",
            "preference optimization ( ORPO ), which efficiently\n",
            "penalizes the model from learning undesired gen-\n",
            "eration styles during SFT in Section 4. In con-\n",
            "trast to previous works, our approach requires\n",
            "neither an SFT warm-up stage nor a reference\n",
            "model, enabling resource-efficient development of\n",
            "preference-based aligned models.\n",
            "We demonstrate the effectiveness of our method\n",
            "with the evaluation of model alignment tasks and\n",
            "popular leaderboards in Section 6.1 and 6.2 by\n",
            "fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mis-\n",
            "tral (7B) with ORPO . Then, we conduct controlled\n",
            "experiments comparing ORPO against established\n",
            "methods for model alignment, RLHF, and DPO\n",
            "for different datasets and model sizes in Section\n",
            "6.3. Along with the post-hoc analysis of generation\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 70}\n",
            "experiments comparing ORPO against established\n",
            "methods for model alignment, RLHF, and DPO\n",
            "for different datasets and model sizes in Section\n",
            "6.3. Along with the post-hoc analysis of generation\n",
            "diversity in Section 6.4, we expound on the theo-\n",
            "retical, empirical, and computational justification\n",
            "of utilizing the odds ratio in monolithic preference\n",
            "alignment in Section 7.3. We release the training\n",
            "code and the checkpoints for Mistral- ORPO -α(7B)\n",
            "and Mistral- ORPO -β(7B). These models achieve\n",
            "7.24 and 7.32 in MT-Bench, 11.33% and 12.20%onAlpacaEval2.0, and 61.63% and 66.19% in IFE-\n",
            "val instruction-level loose accuracy, respectively.\n",
            "2 Related Works\n",
            "Alignment with Reinforcement Learning Rein-\n",
            "forcement learning with human feedback (RLHF)\n",
            "commonly applies the Bradley-Terry model\n",
            "(Bradley and Terry, 1952) to estimate the prob-\n",
            "ability of a pairwise competition between two in-\n",
            "dependently evaluated instances. An additional\n",
            "reward model is trained to score instances. Rein-\n",
            "forcement learning algorithms such as proximal\n",
            "policy optimization (PPO) (Schulman et al., 2017)\n",
            "are employed to train the model to maximize the\n",
            "score of the reward model for the chosen response,\n",
            "resulting in language models that are trained with\n",
            "human preferences (Ziegler et al., 2020; Stiennon\n",
            "et al., 2022). Notably, Ouyang et al. (2022) demon-\n",
            "strated the scalability and versatility of RLHF for\n",
            "instruction-following language models. Extensions\n",
            "such as language model feedback (RLAIF) could\n",
            "be a viable alternative to human feedback (Bai et al.,\n",
            "2022b; Lee et al., 2023; Pang et al., 2023). How-\n",
            "ever, RLHF faces challenges of extensive hyper-\n",
            "parameter searching due to the instability of PPO\n",
            "(Rafailov et al., 2023; Wu et al., 2023) and the\n",
            "sensitivity of the reward models (Gao et al., 2022;\n",
            "Wang et al., 2024). Therefore, there is a crucial\n",
            "need for stable preference alignment algorithms.\n",
            "Alignment without Reward Model Several\n",
            "techniques for preference alignment mitigate the\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 71}\n",
            "Wang et al., 2024). Therefore, there is a crucial\n",
            "need for stable preference alignment algorithms.\n",
            "Alignment without Reward Model Several\n",
            "techniques for preference alignment mitigate the\n",
            "need for reinforcement learning (Rafailov et al.,\n",
            "2023; Song et al., 2023; Azar et al., 2023; Etha-\n",
            "yarajh et al., 2024). Rafailov et al. (2023) introduce\n",
            "direct policy optimization (DPO), which combines\n",
            "the reward modeling stage into the preference learn-ing stage. Azar et al. (2023) prevented potential\n",
            "overfitting problems in DPO through identity pref-\n",
            "erence optimization (IPO). Ethayarajh et al. (2024)\n",
            "and Cai et al. (2023) proposed Kahneman-Tversky\n",
            "Optimisation (KTO) and Unified Language Model\n",
            "Alignment (ULMA) that does not require the pair-\n",
            "wise preference dataset, unlike RLHF and DPO.\n",
            "Song et al. (2023) further suggests incorporation\n",
            "of the softmax value of the reference response set\n",
            "in the negative log-likelihood loss to merge the\n",
            "supervised fine-tuning and preference alignment.\n",
            "Alignment with Supervised Fine-tuning Prefer-\n",
            "ence alignment methods in reinforcement learning\n",
            "(RL) often leverage supervised fine-tuning (SFT)\n",
            "to ensure the stable update of the active policy in\n",
            "relation to the old policy (Schulman et al., 2017).\n",
            "This is because the SFT model is the old policy in\n",
            "the context of RLHF (Ziegler et al., 2020). Fur-\n",
            "thermore, empirical findings indicate that, even in\n",
            "non-RL alignment methods, the SFT model is cru-\n",
            "cial for achieving convergence to desired results\n",
            "(Rafailov et al., 2023; Tunstall et al., 2023).\n",
            "In contrast, there have been approaches to build\n",
            "human-aligned language models by conducting\n",
            "SFT only with filtered datasets (Zhou et al., 2023a;\n",
            "Li et al., 2023a; Haggerty and Chandra, 2024; Zhou\n",
            "et al., 2023b). Zhou et al. (2023a) demonstrated\n",
            "that SFT with a small amount of data with fine-\n",
            "grained filtering and curation could be sufficient\n",
            "for building helpful language model assistants. Fur-\n",
            "thermore, Li et al. (2023a) and Haggerty and Chan-\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 72}\n",
            "grained filtering and curation could be sufficient\n",
            "for building helpful language model assistants. Fur-\n",
            "thermore, Li et al. (2023a) and Haggerty and Chan-\n",
            "dra (2024) proposed an iterative process of fine-\n",
            "tuning the supervised fine-tuned language mod-\n",
            "els with their own generations after fine-grained\n",
            "selection of aligned generations and Zhou et al.\n",
            "(2023b) suggested that a curated subset of prefer-\n",
            "ence dataset is sufficient for alignment. While these\n",
            "works highlight the impact and significance of SFT\n",
            "in the context of alignment, the actual role of SFT\n",
            "and the theoretical background for incorporating\n",
            "preference alignment in SFT remains understudied.\n",
            "3 The Role of Supervised Fine-tuning\n",
            "We study the behavior of supervised fine-tuning\n",
            "(SFT) as an initial stage of preference alignment\n",
            "methods (Ziegler et al., 2020; Rafailov et al., 2023)\n",
            "through analysis of the loss function in SFT and\n",
            "empirical demonstration of the preference compre-\n",
            "hension ability of the trained SFT model. SFT\n",
            "plays a significant role in tailoring the pre-trained\n",
            "language models to the desired domain (Zhou et al.,2023a; Dong et al., 2024) by increasing the log\n",
            "probabilities of pertinent tokens. Nevertheless, this\n",
            "inadvertently increases the likelihood of generating\n",
            "tokens in undesirable styles, as illustrated in Figure\n",
            "3. Therefore, it is necessary to develop methods\n",
            "capable of preserving the domain adaptation role of\n",
            "SFT while concurrently discerning and mitigating\n",
            "unwanted generation styles.\n",
            "Absence of Penalty in Cross-Entropy Loss The\n",
            "goal of cross-entropy loss model fine-tuning is to\n",
            "penalize the model if the predicted logits for the\n",
            "reference answers are low, as shown in Equation 2.\n",
            "L=−1\n",
            "mmX\n",
            "k=1logP(x(k),y(k)) (1)\n",
            "=−1\n",
            "mmX\n",
            "k=1|V|X\n",
            "i=1y(k)\n",
            "i·log(p(k)\n",
            "i) (2)\n",
            "where yiis a boolean value that indicates if ith to-\n",
            "ken in the vocabulary set Vis a label token, pi\n",
            "refers to the probability of ith token, and mis\n",
            "the length of sequence. Using cross-entropy alone\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 73}\n",
            "where yiis a boolean value that indicates if ith to-\n",
            "ken in the vocabulary set Vis a label token, pi\n",
            "refers to the probability of ith token, and mis\n",
            "the length of sequence. Using cross-entropy alone\n",
            "gives no direct penalty or compensation for the\n",
            "logits of non-answer tokens (Lin et al., 2017) as\n",
            "yiwill be set to 0. While cross-entropy is gen-\n",
            "erally effective for domain adaptation (Mao et al.,\n",
            "2023), there are no mechanisms to penalize rejected\n",
            "responses when compensating for the chosen re-\n",
            "sponses. Therefore, the log probabilities of the to-\n",
            "kens in the rejected responses increase along with\n",
            "the chosen responses, which is not desired from the\n",
            "viewpoint of preference alignment.\n",
            "Generalization over Both Response Styles We\n",
            "conduct a pilot study to empirically demonstrate\n",
            "the miscalibration of chosen and rejected responses\n",
            "with supervised fine-tuning alone. We fine-tune\n",
            "OPT-350M (Zhang et al., 2022) on the chosen re-\n",
            "sponses only from the HH-RLHF dataset (Bai et al.,\n",
            "2022b). Throughout the training, we monitor the\n",
            "log probability of rejected responses for each batch\n",
            "and report this in Figure 3. Both the log probability\n",
            "of chosen and rejected responses exhibited a simul-\n",
            "taneous increase. This can be interpreted from two\n",
            "different perspectives. First, the cross-entropy loss\n",
            "effectively guides the model toward the intended\n",
            "domain (e.g., dialogue). However, the absence of\n",
            "a penalty for unwanted generations results in re-\n",
            "jected responses sometimes having even higher log\n",
            "probabilities than the chosen ones.−2.5−2.4−2.3−2.2\n",
            "0 5000 10000 15000\n",
            "Training StepLog ProbabilityResponse Type Chosen RejectedFigure 3: Log probabilities for chosen and rejected\n",
            "responses during OPT-350M model fine-tuning on HH-\n",
            "RLHF dataset. Despite only chosen responses being\n",
            "used for supervision, rejected responses show a compa-\n",
            "rable likelihood of generation.\n",
            "Penalizing Undesired Generations Appending\n",
            "an unlikelihood penalty to the loss has demon-\n",
            "strated success in reducing unwanted degenerative\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 74}\n",
            "rable likelihood of generation.\n",
            "Penalizing Undesired Generations Appending\n",
            "an unlikelihood penalty to the loss has demon-\n",
            "strated success in reducing unwanted degenerative\n",
            "traits in models (Welleck et al., 2019; Li et al.,\n",
            "2020). For example, to prevent repetitions , an un-\n",
            "wanted token set of previous contexts, k∈ Crecent ,\n",
            "is disfavored by adding the following term to\n",
            "(1−p(k)\n",
            "i)to the loss (such as Equation 2) which\n",
            "penalizes the model for assigning high probabilities\n",
            "to recent tokens. Motivated by SFT ascribing high\n",
            "probabilities to rejected tokens (Figure 3) and the\n",
            "effectiveness of appending penalizing unwanted\n",
            "traits, we design a monolithic preference alignment\n",
            "method that dynamically penalizes the disfavored\n",
            "response for each query without the need for craft-\n",
            "ing sets of rejected tokens.\n",
            "4 Odds Ratio Preference Optimization\n",
            "We introduce a novel preference alignment algo-\n",
            "rithm, Odds Ratio Preference Optimization ( ORPO ),\n",
            "which incorporates an odds ratio-based penalty\n",
            "to the conventional negative log-likelihood (NLL)\n",
            "loss for differentiating the generation styles be-\n",
            "tween favored and disfavored responses.\n",
            "4.1 Preliminaries\n",
            "Given an input sequence x, the average log-\n",
            "likelihood of generating the output sequence y, of\n",
            "length mtokens, is computed as Equation 3. The\n",
            "odds of generating the output sequence ygiven an\n",
            "input sequence xis defined in Equation 4:\n",
            "logPθ(y|x) =1\n",
            "mmX\n",
            "t=1logPθ(yt|x, y<t)(3)odds θ(y|x) =Pθ(y|x)\n",
            "1−Pθ(y|x)(4)\n",
            "Intuitively, odds θ(y|x) = kimplies that it is k\n",
            "times more likely for the model θto generate the\n",
            "output sequence ythan not generating it. Thus,\n",
            "the odds ratio of the chosen response ywover the\n",
            "rejected response yl,ORθ(yw, yl), indicates how\n",
            "much more likely it is for the model θto generate\n",
            "ywthanylgiven input x, defined in Equation 5.\n",
            "ORθ(yw, yl) =odds θ(yw|x)\n",
            "odds θ(yl|x)(5)\n",
            "4.2 Objective Function of ORPO\n",
            "The objective function of ORPO in Equation 6 con-\n",
            "sists of two components: 1) supervised fine-tuning\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 75}\n",
            "ORθ(yw, yl) =odds θ(yw|x)\n",
            "odds θ(yl|x)(5)\n",
            "4.2 Objective Function of ORPO\n",
            "The objective function of ORPO in Equation 6 con-\n",
            "sists of two components: 1) supervised fine-tuning\n",
            "(SFT) loss ( LSFT); 2) relative ratio loss ( LOR).\n",
            "LORPO =E(x,yw,yl)[LSFT+λ· LOR](6)\n",
            "LSFT follows the conventional causal language\n",
            "modeling negative log-likelihood (NLL) loss func-\n",
            "tion to maximize the likelihood of generating the\n",
            "reference tokens as previously discussed in Sec-\n",
            "tion 3. LORin Equation 7 maximizes the odds\n",
            "ratio between the likelihood of generating the dis-\n",
            "favored response ywand the disfavored response\n",
            "yl. We wrap the log odds ratio with the log sig-\n",
            "moid function so that LORcould be minimized by\n",
            "increasing the log odds ratio between ywandyl.\n",
            "LOR=−logσ\u0012\n",
            "logodds θ(yw|x)\n",
            "odds θ(yl|x)\u0013\n",
            "(7)\n",
            "Together, LSFT andLORweighted with λtailor\n",
            "the pre-trained language model to adapt to the spe-\n",
            "cific subset of the desired domain and disfavor gen-\n",
            "erations in the rejected response sets.\n",
            "4.3 Gradient of ORPO\n",
            "The gradient of LRatio further justifies using the\n",
            "odds ratio loss. It comprises two terms: one that pe-\n",
            "nalizes the wrong predictions and one that contrasts\n",
            "between chosen and rejected responses, denoted in\n",
            "Equation 84ford= (x, yl, yw)∼D.\n",
            "∇θLOR=δ(d)·h(d) (8)\n",
            "δ(d) =\u0014\n",
            "1 +odds θP(yw|x)\n",
            "odds θP(yl|x)\u0015−1\n",
            "(9)\n",
            "h(d) =∇θlogPθ(yw|x)\n",
            "1−Pθ(yw|x)−∇θlogPθ(yl|x)\n",
            "1−Pθ(yl|x)\n",
            "(10)\n",
            "4The full derivation for ∇θLORis in Appendix A.When the odds of the favored responses are rela-\n",
            "tively higher than the disfavored responses, δ(d)\n",
            "in Equation 9 will converge to 0. This indicates\n",
            "that the δ(d)will play the role of a penalty term,\n",
            "accelerating the parameter updates if the model is\n",
            "more likely to generate the rejected responses.\n",
            "Meanwhile, h(d)in Equation 10 implies a\n",
            "weighted contrast of the two gradients from the cho-\n",
            "sen and rejected responses. Specifically, 1−P(y|x)\n",
            "in the denominators amplifies the gradients when\n",
            "the corresponding side of the likelihood P(y|x)\n",
            "is low. For the chosen responses, this accelerates\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 76}\n",
            "sen and rejected responses. Specifically, 1−P(y|x)\n",
            "in the denominators amplifies the gradients when\n",
            "the corresponding side of the likelihood P(y|x)\n",
            "is low. For the chosen responses, this accelerates\n",
            "the model’s adaptation toward the distribution of\n",
            "chosen responses as the likelihood increases.\n",
            "5 Experimental Settings\n",
            "5.1 Training Configurations\n",
            "Models We train a series of OPT models (Zhang\n",
            "et al., 2022) scaling from 125M to 1.3B parameters\n",
            "comparing supervised fine-tuning (SFT), proximal\n",
            "policy optimization (PPO), direct policy optimiza-\n",
            "tion (DPO), and compare these to our ORPO . PPO\n",
            "and DPO models were fine-tuned with TRL library\n",
            "(von Werra et al., 2020) on top of SFT models\n",
            "trained for a single epoch on the chosen responses\n",
            "following Rafailov et al. (2023) and Tunstall et al.\n",
            "(2023). We notate this by prepending \"+\" to each al-\n",
            "gorithm (e.g., +DPO). Additionally, we train Phi-2\n",
            "(2.7B) (Javaheripi and Bubeck, 2023), a pre-trained\n",
            "language model with promising downstream per-\n",
            "formance (Beeching et al., 2023), as well as Llama-\n",
            "2 (7B) (Touvron et al., 2023) and Mistral (7B)\n",
            "(Jiang et al., 2023). Further training details for\n",
            "each method are in Appendix C.\n",
            "Datasets We test each training configuration and\n",
            "model on two datasets: 1) Anthropic’s HH-RLHF\n",
            "(Bai et al., 2022a), 2) Binarized UltraFeedback\n",
            "(Tunstall et al., 2023). We filtered out instances\n",
            "where yw=ylor where yw=∅or where yl=∅.\n",
            "Reward Models We train OPT-350M and OPT-\n",
            "1.3B on each dataset for a single epoch for reward\n",
            "modeling with the objective function in Equation\n",
            "11 (Ziegler et al., 2020). The OPT-350M reward\n",
            "model was used for PPO, and OPT-1.3B reward\n",
            "model was used to assess the generations of fine-\n",
            "tuned models. We refer to these reward models as\n",
            "RM-350M and RM-1.3B in Section 6.\n",
            "−E(x,yl,yw)[logσ(r(x, yw)−r(x, yl))] (11)5.2 Leaderboard Evaluation\n",
            "In Section 6.1, we evaluate the models using the\n",
            "AlpacaEval1.0andAlpacaEval2.0(Li et al., 2023b)\n",
            "benchmarks, comparing ORPO to other instruction-\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 77}\n",
            "In Section 6.1, we evaluate the models using the\n",
            "AlpacaEval1.0andAlpacaEval2.0(Li et al., 2023b)\n",
            "benchmarks, comparing ORPO to other instruction-\n",
            "tuned models reported in the official leaderboard,5\n",
            "including Llama-2 Chat (7B) and (13B) (Touvron\n",
            "et al., 2023), and Zephyr αandβ(Almazrouei\n",
            "et al., 2023). Similarly, in Section 6.2, we evaluate\n",
            "the models with MT-Bench (Zheng et al., 2023)\n",
            "and report the results and the scores of the same\n",
            "models reported in the official leaderboard.6Us-\n",
            "ing GPT-4 (Achiam et al., 2023) as an evaluator\n",
            "inAlpacaEval1.0, we assess if the trained model\n",
            "can be preferred over the responses generated from\n",
            "text-davinci-003 . For AlpacaEval2.0, we used\n",
            "GPT-4-turbo7as an evaluator following the default\n",
            "setting. We assess if the generated responses are fa-\n",
            "vored over those generated from GPT-4. Finally, us-\n",
            "ing GPT-4 as an evaluator in MT-Bench, we check\n",
            "if the models can follow the instructions with hard\n",
            "answers in a multi-turn conversation.\n",
            "6 Results and Analysis\n",
            "First, we assess the general instruction-following\n",
            "abilities of the models by comparing the prefer-\n",
            "ence alignment algorithms in Sections 6.1 and 6.2.\n",
            "Second, we measure the win rate of OPT models\n",
            "trained with ORPO against other alignment methods\n",
            "training OPT 1.3B as a reward model in Section\n",
            "6.3. Then, we measure the lexical diversity of the\n",
            "models trained with ORPO and DPO in Section 6.4.\n",
            "6.1 Single-turn Instruction Following\n",
            "Phi-2 (2.7B) ORPO improved pre-trained Phi-2\n",
            "to exceed the performance of the Llama-2 Chat\n",
            "instruction-following language model by only us-\n",
            "ing UltraFeedback as the instruction-tuning dataset,\n",
            "as shown in Table 1. λof 0.25 was applied for Phi-\n",
            "2, resulting in 71.80% and 6.35% in AlpacaEval.\n",
            "Llama-2 (7B) Notably, UltraFeedback and ORPO\n",
            "withλof 0.2 on Llama-2 (7B) resulted in higher\n",
            "AlpacaEval scores than the chat versions of both\n",
            "7B and 13B scale trained with RLHF, eventually\n",
            "showing 81.26% and 9.44% in both AlpacaEvals.\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 78}\n",
            "withλof 0.2 on Llama-2 (7B) resulted in higher\n",
            "AlpacaEval scores than the chat versions of both\n",
            "7B and 13B scale trained with RLHF, eventually\n",
            "showing 81.26% and 9.44% in both AlpacaEvals.\n",
            "In contrast, in our controlled experimental set-\n",
            "ting of conducting one epoch of SFT and three\n",
            "5https://tatsu-lab.github.io/alpaca_eval/\n",
            "6https://huggingface.co/spaces/lmsys/\n",
            "chatbot-arena-leaderboard\n",
            "7https://platform.openai.com/docs/models/\n",
            "gpt-4-and-gpt-4-turboModel Name Size AlpacaEval1.0 AlpacaEval2.0\n",
            "Phi-2 + SFT 2.7B 48.37% (1.77) 0.11% (0.06)\n",
            "Phi-2 + SFT + DPO 2.7B 50.63% (1.77) 0.78% (0.22)\n",
            "Phi-2 + ORPO (Ours) 2.7B 71.80% (1.59) 6.35% (0.74)\n",
            "Llama-2 Chat * 7B 71.34% (1.59) 4.96% (0.67)\n",
            "Llama-2 Chat * 13B 81.09% (1.38) 7.70% (0.83)\n",
            "Llama-2 + ORPO (Ours) 7B 81.26% (1.37) 9.44% (0.85)\n",
            "Zephyr ( α) * 7B 85.76% (1.23) 8.35% (0.87)\n",
            "Zephyr ( β) * 7B 90.60% (1.03) 10.99% (0.96)\n",
            "Mistral- ORPO -α(Ours) 7B 87.92% (1.14) 11.33% (0.97)\n",
            "Mistral- ORPO -β(Ours) 7B 91.41% (1.15) 12.20% (0.98)\n",
            "Table 1: Table of instruction-following abilities of each checkpoint measured through AlpacaEval. While clearly\n",
            "showing the improvements in instruction-following abilities after training with ORPO , it is notable that ORPO models\n",
            "exceed RLHF or DPO models of Llama-2 and Mistral (* indicates the results from the official leaderboard.)\n",
            "epochs of DPO following Tunstall et al. (2023) and\n",
            "Rafailov et al. (2023), Llama-2 + SFT and Llama-\n",
            "2 + SFT + DPO yielded models with outputs that\n",
            "could not be evaluated. This supports the efficacy\n",
            "ofORPO , in which the model can rapidly learn the\n",
            "desired domain and the preference with limited\n",
            "data. This aligns with the h(d)examination in the\n",
            "gradient of our method studied in Section 4.3.\n",
            "Mistral- ORPO -α(7B) Furthermore, fine-tuning\n",
            "Mistral (7B) with single-turn conversation dataset,\n",
            "UltraFeedback, and ORPO withλof 0.1 outperforms\n",
            "Zephyr series, which are the Mistral (7B) models\n",
            "fine-tuned with SFT on 20K UltraChat (Ding et al.,\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 79}\n",
            "Mistral (7B) with single-turn conversation dataset,\n",
            "UltraFeedback, and ORPO withλof 0.1 outperforms\n",
            "Zephyr series, which are the Mistral (7B) models\n",
            "fine-tuned with SFT on 20K UltraChat (Ding et al.,\n",
            "2023) and DPO on the full UltraFeedback. As\n",
            "shown in Table 1, Mistral- ORPO -α(7B) achieves\n",
            "87.92% and 11.33%, which exceeds Zephyr αby\n",
            "1.98% and Zephyr βby 0.34% in AlpacaEval2.0.\n",
            "The sample responses and corresponding refer-\n",
            "ences from GPT-4 can be found in Appendix I.\n",
            "Mistral- ORPO -β(7B) Using the same configura-\n",
            "tion of Mistral- ORPO -α(7B), we additionally com-\n",
            "pare fine-tuning Mistral on the cleaned version of\n",
            "the UltraFeedback8to demonstrate the effect of the\n",
            "data quality (Bartolome et al., 2023). While the\n",
            "actual sizes of datasets are similar, ORPO gains fur-\n",
            "ther advantages from the dataset quality by scoring\n",
            "over 91% and 12% on AlpacaEval, as shown in\n",
            "Table 1. Further instruction-following evaluation\n",
            "on two Mistral-based models with IFEval (Zhou\n",
            "et al., 2023c) is reported in the Appendix D.\n",
            "8https://huggingface.co/datasets/argilla/\n",
            "ultrafeedback-binarized-preferences-cleaned\n",
            "Writing\n",
            "Roleplay\n",
            "Reasoning\n",
            "Math\n",
            "CodingExtractionSTEMHumanities\n",
            "0246810Model\n",
            "Llama-2-Chat (70B)\n",
            "GPT-3.5-turbo\n",
            "Claude V1\n",
            "GPT-4\n",
            "Mistral- ORPO -⍺ (7B): 7.23\n",
            "Mistral- ORPO -β (7B): 7.32\n",
            "Loading [MathJax]/extensions/MathMenu.jsFigure 4: MT-Bench result of Mistral- ORPO -α(7B) and\n",
            "Mistral- ORPO -β(7B) by the category. Further compari-\n",
            "son can be found in the Appendix G.\n",
            "6.2 Multi-turn Instruction Following\n",
            "With our best model, Mistral- ORPO -α(7B) and\n",
            "Mistral- ORPO -β(7B), we also assess the multi-turn\n",
            "instruction-following skills with deterministic an-\n",
            "swers (e.g., math) through MT-Bench.\n",
            "As shown in Figure 4, ORPO -Mistral (7B) series\n",
            "achieve comparable results to either larger or the\n",
            "proprietary models, including Llama-2-Chat (70B)\n",
            "and Claude. Eventually, Mistral- ORPO -α(7B) and\n",
            "Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-\n",
            "Bench without being exposed to the multi-turn con-\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 80}\n",
            "proprietary models, including Llama-2-Chat (70B)\n",
            "and Claude. Eventually, Mistral- ORPO -α(7B) and\n",
            "Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-\n",
            "Bench without being exposed to the multi-turn con-\n",
            "versation dataset during training.\n",
            "6.3 Reward Model Win Rate\n",
            "We assess the win rate of ORPO over other pref-\n",
            "erence alignment methods, including supervised\n",
            "fine-tuning (SFT), PPO, and DPO, using RM-1.3B\n",
            "to understand the effectiveness and scalability of\n",
            "ORPO in Tables 2 and 3. Additionally, we visu-\n",
            "ally verify that ORPO can effectively enhance theFigure 5: Reward distribution comparison between OPT-125M (left), OPT-350M (middle), and OPT-1.3B (right)\n",
            "trained with SFT (blue), RLHF (green), DPO (orange), and ORPO (red) on the test set of UltraFeedback using the\n",
            "RM-1.3B. While the rewards of the trained models are roughly normal and preference optimization algorithms\n",
            "(RLHF, DPO, and ORPO ) tend to move the reward distribution in the positive direction, ORPO is on par or better than\n",
            "RLHF and DPO in increasing the expected reward. The same plot for the HH-RLHF dataset is in Appendix F.\n",
            "expected reward compared to SFT in Figure 5.\n",
            "HH-RLHF In Table 2, ORPO outperforms SFT\n",
            "and PPO across all model scales. The highest win\n",
            "rate against SFT and PPO across the size of the\n",
            "model was 78.0% and 79.4%, respectively. Mean-\n",
            "while, the win rate over DPO was correlated to\n",
            "the model’s size, with the largest model having the\n",
            "highest win rate: 70.9%.\n",
            "ORPO vs SFT +DPO +PPO\n",
            "OPT-125M 84.0 (0.62) 41.7 (0.77) 66.1 (0.26)\n",
            "OPT-350M 82.7 (0.56) 49.4 (0.54) 79.4 (0.29)\n",
            "OPT-1.3B 78.0 (0.16) 70.9 (0.52) 65.9 (0.33)\n",
            "Table 2: Average win rate (%) and its standard deviation\n",
            "ofORPO and standard deviation over other methods on\n",
            "HH-RLHF dataset for three rounds. Sampling decoding\n",
            "with a temperature of 1.0 was used on the test set.\n",
            "UltraFeedback The win rate in UltraFeedback\n",
            "followed similar trends to what was reported in HH-\n",
            "RLHF, as shown in Table 3. ORPO was preferred\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 81}\n",
            "with a temperature of 1.0 was used on the test set.\n",
            "UltraFeedback The win rate in UltraFeedback\n",
            "followed similar trends to what was reported in HH-\n",
            "RLHF, as shown in Table 3. ORPO was preferred\n",
            "over SFT and PPO for maximum 80.5% and 85.8%,\n",
            "respectively. While consistently preferring ORPO\n",
            "over SFT and PPO, the win rate over DPO grad-\n",
            "ually increases as the size of the model increases.\n",
            "The scale-wise trend exceeding DPO will be further\n",
            "shown through 2.7B models in Section 6.1.\n",
            "Overall Reward Distribution In addition to the\n",
            "win rate, we compare the reward distribution of\n",
            "the responses generated with respect to the test set\n",
            "of the UltraFeedback dataset in Figure 5 and HH-\n",
            "RLHF dataset in Appendix F. Regarding the SFT\n",
            "reward distribution as a default, PPO, DPO, and\n",
            "ORPO shift it in both datasets. However, the magni-\n",
            "tude of reward shifts for each algorithm differs.ORPO vs SFT +DPO +PPO\n",
            "OPT-125M 73.2 (0.12) 48.8 (0.29) 71.4 (0.28)\n",
            "OPT-350M 80.5 (0.54) 50.5 (0.17) 85.8 (0.62)\n",
            "OPT-1.3B 69.4 (0.57) 57.8 (0.73) 65.7 (1.07)\n",
            "Table 3: Average win rate (%) and its standard devia-\n",
            "tion of ORPO and standard deviation over other methods\n",
            "onUltraFeedback dataset for three rounds. Sampling\n",
            "decoding with a temperature of 1.0 was used.\n",
            "In Figure 5, RLHF (i.e., SFT + PPO) has some\n",
            "abnormal properties of the distribution with a low\n",
            "expected reward. We attribute this to empirical\n",
            "evidence of the instability and reward mismatch\n",
            "problem of RLHF (Rafailov et al., 2023; Gao et al.,\n",
            "2022; Shen et al., 2023) as the RLHF models were\n",
            "trained with RM-350M and assessed with RM-\n",
            "1.3B. Meanwhile, it is notable that the ORPO dis-\n",
            "tribution (red) is mainly located on the very right\n",
            "side of each subplot, indicating higher expected re-\n",
            "wards. Recalling the intent of preference alignment\n",
            "methods, the distributions in Figure 5 indicate that\n",
            "ORPO tends to fulfill the aim of preference align-\n",
            "ment for all model sizes.\n",
            "6.4 Lexical Diversity\n",
            "The lexical diversity of the preference-aligned lan-\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 82}\n",
            "ORPO tends to fulfill the aim of preference align-\n",
            "ment for all model sizes.\n",
            "6.4 Lexical Diversity\n",
            "The lexical diversity of the preference-aligned lan-\n",
            "guage models was studied in previous works (Kirk\n",
            "et al., 2024). We expand the concept of per-input\n",
            "and across-input diversity introduced in Kirk et al.\n",
            "(2024) by using Gemini-Pro (Gemini Team et al.,\n",
            "2023) as an embedding model, which is suitable\n",
            "for assessing the diversity of instruction-following\n",
            "language models by encoding a maximum of 2048\n",
            "tokens. The diversity metric with the given set ofsampled responses is defined as Equation 13.\n",
            "Oi\n",
            "θ:={yj∼θ(y|xi)|j= 1,2, ..., K} (12)\n",
            "D(Oi\n",
            "θ) =1\n",
            "2·PN−1\n",
            "i=1PN\n",
            "j=i+1cos(hi, hj)\n",
            "N·(N−1)(13)\n",
            "where cos(hi, hj)refers to the cosine similarity\n",
            "between the embedding hiandhj. 5 different re-\n",
            "sponses are sampled with a temperature of 1.0 to\n",
            "160 queries in AlpacaEval (i.e., K= 5, N= 160 )\n",
            "using Phi-2 and Llama-2 trained with ORPO and\n",
            "DPO. We report the results in Table 4.\n",
            "Per Input Diversity (PID) We average the input-\n",
            "wise average cosine similarity between the gener-\n",
            "ated samples with Equation 14 to assess the per-\n",
            "input diversity. In Table 4, ORPO models have the\n",
            "highest average cosine similarity in the first column\n",
            "for both models, which implies the lowest diversity\n",
            "per input. This indicates that ORPO generally as-\n",
            "signs high probabilities to the desired tokens, while\n",
            "DPO has a relatively smoother logit distribution.\n",
            "PIDD(θ) =1\n",
            "NNX\n",
            "i=1D(Oi\n",
            "θ) (14)\n",
            "Across Input Diversity (AID) Using 8 samples\n",
            "generated per input, we sample the first item for\n",
            "each input and examine their inter cosine similarity\n",
            "with Equation 15 for across-input diversity. Un-\n",
            "like per-input diversity, it is noteworthy that Phi-2\n",
            "(ORPO ) has lower average cosine similarity in the\n",
            "second row of Table 4. We can infer that ORPO\n",
            "triggers the model to generate more instruction-\n",
            "specific responses than DPO.\n",
            "AID D(θ) =D N[\n",
            "i=1Oi,θ,j=1!\n",
            "(15)\n",
            "Per Input ↓Across Input ↓\n",
            "Phi-2 + SFT + DPO 0.8012 0.6019\n",
            "Phi-2 + ORPO 0.8909 0.5173\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 83}\n",
            "triggers the model to generate more instruction-\n",
            "specific responses than DPO.\n",
            "AID D(θ) =D N[\n",
            "i=1Oi,θ,j=1!\n",
            "(15)\n",
            "Per Input ↓Across Input ↓\n",
            "Phi-2 + SFT + DPO 0.8012 0.6019\n",
            "Phi-2 + ORPO 0.8909 0.5173\n",
            "Llama-2 + SFT + DPO 0.8889 0.5658\n",
            "Llama-2 + ORPO 0.9008 0.5091\n",
            "Table 4: Lexical diversity of Phi-2 and Llama-2 fine-\n",
            "tuned with DPO and ORPO . Lower cosine similarity is\n",
            "equivalent to higher diversity. The highest value in each\n",
            "column within the same model family is bolded.7 Discussion\n",
            "In this section, we expound on the theoretical and\n",
            "computational details of ORPO . The theoretical anal-\n",
            "ysis of ORPO is studied in Section 7.1, which will\n",
            "be supported with the empirical analysis in Section\n",
            "7.2. Then, we compare the computational load of\n",
            "DPO and ORPO in Section 7.3.\n",
            "7.1 Comparison to Probability Ratio\n",
            "The rationale for selecting the odds ratio instead of\n",
            "the probability ratio lies in its stability. The prob-\n",
            "ability ratio for generating the favored response\n",
            "ywover the disfavored response ylgiven an input\n",
            "sequence xcan be defined as Equation 16.\n",
            "PRθ(yw, yl) =Pθ(yw|x)\n",
            "Pθ(yl|x)(16)\n",
            "While this formulation has been used in previous\n",
            "preference alignment methods that precede SFT\n",
            "(Rafailov et al., 2023; Azar et al., 2023), the odds\n",
            "ratio is a better choice in the setting where the\n",
            "preference alignment is incorporated in SFT as the\n",
            "odds ratio is more sensitive to the model’s prefer-\n",
            "ence understanding. In other words, the probability\n",
            "ratio leads to more extreme discrimination of the\n",
            "disfavored responses than the odds ratio.\n",
            "We visualize this through the sample distribu-\n",
            "tions of the log probability ratio logPR(X2|X1)\n",
            "and log odds ratio logOR(X2|X1). We sample\n",
            "50,000 samples each with Equation 17 and plot the\n",
            "log probability ratio and log odds ratio in Figure\n",
            "6. We multiply βfor the probability ratio as it is\n",
            "practiced in the probability ratio-based methods\n",
            "and report the cases where β= 0.2andβ= 1.0.\n",
            "X1, X2∼Unif(0,1) (17)\n",
            "Y∼β(logX1−logX2) (18)\n",
            "Y∼logX1\n",
            "1−X1−logX2\n",
            "1−X2(19)\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 84}\n",
            "practiced in the probability ratio-based methods\n",
            "and report the cases where β= 0.2andβ= 1.0.\n",
            "X1, X2∼Unif(0,1) (17)\n",
            "Y∼β(logX1−logX2) (18)\n",
            "Y∼logX1\n",
            "1−X1−logX2\n",
            "1−X2(19)\n",
            "Recalling that the log sigmoid function is applied\n",
            "to the log probability ratio and log odds ratio, each\n",
            "ratio’s scale determines the expected margin be-\n",
            "tween the likelihood of the favored and disfavored\n",
            "styles when the loss is minimized. In that sense, the\n",
            "contrast should be relatively extreme to minimize\n",
            "the log sigmoid loss when PR(X2|X1)is inputted\n",
            "instead of OR(X2|X1)to the log sigmoid function,\n",
            "regarding the sharp distribution of logPR(X2|X1)10\n",
            " 5\n",
            " 0 5 105K10K15K20KCountProb. Ratio (=0.2)\n",
            "Prob. Ratio (=1.0)\n",
            "Odds RatioFigure 6: Sampled distribution of logPR(X2|X1)and\n",
            "logOR(X2|X1).logOR(X2|X1)has a wider range\n",
            "given the same input probability pairs (X1, X2).\n",
            "in Figure 6. This results in overly suppressing the\n",
            "logits for the tokens in the disfavored responses in\n",
            "the setting where SFT and preference alignment are\n",
            "incorporated, as the model is not adapted to the do-\n",
            "main. We empirically support this analysis through\n",
            "the ablation study in Appendix B. Therefore, the\n",
            "odds ratio is a better choice when the preference\n",
            "alignment is done with SFT due to the mild discrim-\n",
            "ination of disfavored responses and the prioritizing\n",
            "of the favored responses to be generated.\n",
            "Throughout fine-tuning, minimizing the log\n",
            "sigmoid loss leads to either PR(X2|X1)or\n",
            "OR(X2|X1)to be larger. This is equivalent to the\n",
            "rejected responses’ token-wise likelihood, which\n",
            "will generally get smaller. In this context, it is es-\n",
            "sential to avoid an overly extreme contrast. This\n",
            "precaution is especially important given the sharp\n",
            "distribution of logPR(X2|X1)depicted in Figure\n",
            "6. The excessive margin could lead to the unwar-\n",
            "ranted suppression of logits for tokens in disfavored\n",
            "responses within the incorporated setting, poten-\n",
            "tially resulting in issues of degeneration.\n",
            "7.2 Minimizing LOR\n",
            "We demonstrate that models trained with ORPO\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 85}\n",
            "responses within the incorporated setting, poten-\n",
            "tially resulting in issues of degeneration.\n",
            "7.2 Minimizing LOR\n",
            "We demonstrate that models trained with ORPO\n",
            "learned to reflect the preference throughout the\n",
            "training process. We monitored the log probabili-\n",
            "ties of the chosen and rejected responses and the\n",
            "log odds ratio with λ= 1.0. With the same dataset\n",
            "and model as Figure 3, Figure 7 shows that the\n",
            "log probability of rejected responses is diminishing\n",
            "while that of chosen responses is on par with Fig-\n",
            "ure 3 as the log odds ratio increases. This indicates\n",
            "that ORPO is successfully preserving the domain\n",
            "adaptation role of SFT while the penalty term LOR\n",
            "induces the model to lower the likelihood of un-\n",
            "wanted generations. We discuss the effect of λin\n",
            "Equation 6 in Appendix E, studying the proclivityof the log probability margin between the favored\n",
            "and disfavored responses with respect to λ.\n",
            "0.00.51.01.5\n",
            "02000400060008000\n",
            "Training StepLog Odds Ratio\n",
            "−1.8−1.5−1.2−0.9\n",
            "02000400060008000\n",
            "Training StepLog ProbabilityResponse Type\n",
            "Chosen\n",
            "Rejected\n",
            "Figure 7: Average log-likelihood for chosen and re-\n",
            "jected responses and log odds ratio per batch. The odds\n",
            "consistently increase during training with ORPO .\n",
            "7.3 Computational Efficiency\n",
            "As depicted in Figure 2, ORPO does not require a\n",
            "reference model, unlike RLHF and DPO. In that\n",
            "sense, ORPO is computationally more efficient than\n",
            "RLHF and DPO in two perspectives: 1) memory\n",
            "allocation and 2) fewer FLOPs per batch.\n",
            "The reference model ( πSFT) in the context of\n",
            "RLHF and DPO denotes the model trained with su-\n",
            "pervised fine-tuning (SFT), which will be the base-\n",
            "line model for updating the parameters with RLHF\n",
            "or DPO (Ziegler et al., 2020; Rafailov et al., 2023).\n",
            "Thus, two πSFTs, a frozen reference model and the\n",
            "model undergoing tuning, are required during train-\n",
            "ing. Furthermore, in theory, two forward passes\n",
            "should be calculated for each model to acquire the\n",
            "logits for the chosen and rejected responses. In\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 86}\n",
            "model undergoing tuning, are required during train-\n",
            "ing. Furthermore, in theory, two forward passes\n",
            "should be calculated for each model to acquire the\n",
            "logits for the chosen and rejected responses. In\n",
            "other words, four forward passes happen in total\n",
            "for a single batch. On the other hand, a reference\n",
            "model is not required in ORPO asπSFT is directly\n",
            "updated. This leads to half the number of forward\n",
            "passes required for each batch during training.\n",
            "8 Conclusion\n",
            "In this paper, we introduced a reference-free mono-\n",
            "lithic preference alignment method, odds ratio pref-\n",
            "erence optimization ( ORPO ), by revisiting and un-\n",
            "derstanding the value of the supervised fine-tuning\n",
            "(SFT) phase in the context of preference align-\n",
            "ment. ORPO was consistently preferred by the\n",
            "fine-tuned reward model against SFT and RLHF\n",
            "across the scale, and the win rate against DPO in-\n",
            "creased as the size of the model increased. Further-\n",
            "more, we validate the scalability of ORPO with 2.7B\n",
            "and 7B pre-trained language models by exceeding\n",
            "the larger state-of-the-art instruction-following lan-\n",
            "guage models in AlpacaEval. Specifically, Mistral-\n",
            "ORPO -αand Mistral- ORPO -βachieved 11.33% and12.20% in AlpacaEval2.0, 7.23 and 7.32 in MT-\n",
            "Bench, thereby underscoring the efficiency and ef-\n",
            "fectiveness of ORPO . We release fine-tuning code\n",
            "and model checkpoints for Mistral- ORPO -αand\n",
            "Mistral- ORPO -βto aid reproducibility.\n",
            "Limitations\n",
            "While conducting a comprehensive analysis of the\n",
            "diverse preference alignment methods, including\n",
            "DPO and RLHF, we did not incorporate a more\n",
            "comprehensive range of preference alignment algo-\n",
            "rithms. We leave the broader range of comparison\n",
            "against other methods as future work, along with\n",
            "scaling our method to over 7B models. In addi-\n",
            "tion, we will expand the fine-tuning datasets into\n",
            "diverse domains and qualities, thereby verifying\n",
            "the generalizability of our method in various NLP\n",
            "downstream tasks. Finally, we would like to study\n",
            "the internal impact of our method on the pre-trained\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 87}\n",
            "the generalizability of our method in various NLP\n",
            "downstream tasks. Finally, we would like to study\n",
            "the internal impact of our method on the pre-trained\n",
            "language model, expanding the understanding of\n",
            "preference alignment procedure to not only the\n",
            "supervised fine-tuning stage but also consecutive\n",
            "preference alignment algorithms.\n",
            "{'filename': '2403.07691_v2.pdf', 'title': 'ORPO: Monolithic Preference Optimization without Reference Model', 'vs_index': 88}\n",
            "KTO: Model Alignment as Prospect Theoretic Optimization\n",
            "Kawin Ethayarajh1Winnie Xu2Niklas Muennighoff2Dan Jurafsky1Douwe Kiela1 2\n",
            "Abstract\n",
            "Kahneman & Tversky’s prospect theory tells us\n",
            "that humans perceive random variables in a biased\n",
            "but well-defined manner (1992); for example, hu-\n",
            "mans are famously loss-averse. We show that\n",
            "objectives for aligning LLMs with human feed-\n",
            "back implicitly incorporate many of these biases—\n",
            "the success of these objectives (e.g., DPO) over\n",
            "cross-entropy minimization can partly be ascribed\n",
            "to them being human-aware loss functions (HA-\n",
            "LOs). However, the utility functions these meth-\n",
            "ods attribute to humans still differ from those in\n",
            "the prospect theory literature. Using a Kahneman-\n",
            "Tversky model of human utility, we propose a\n",
            "HALO that directly maximizes the utility of gen-\n",
            "erations instead of maximizing the log-likelihood\n",
            "of preferences, as current methods do. We call\n",
            "this approach Kahneman-Tversky Optimization\n",
            "(KTO), and it matches or exceeds the performance\n",
            "of preference-based methods at scales from 1B to\n",
            "30B. Crucially, KTO does not need preferences—\n",
            "only a binary signal of whether an output is desir-\n",
            "able or undesirable for a given input. This makes\n",
            "it far easier to use in the real world, where prefer-\n",
            "ence data is scarce and expensive.\n",
            "1. Introduction\n",
            "Aligning generative models with human feedback has been\n",
            "successfully used to make generations more helpful, factual,\n",
            "and ethical, among other desiderata (Ouyang et al., 2022;\n",
            "Tian et al., 2023). For LLMs, alignment methods such\n",
            "as RLHF and DPO have consistently proven to be more\n",
            "beneficial than doing supervised finetuning (SFT) alone.\n",
            "However, human feedback is often discussed only in the\n",
            "context of preferences (e.g., output A≻Bfor input x),\n",
            "despite preferences being a kind of data that is relatively\n",
            "scarce and expensive to collect in the real world (Casper\n",
            "et al., 2023). This is largely because the alignment methods\n",
            "shown to work best—RLHF (Christiano et al., 2017) and\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 89}\n",
            "scarce and expensive to collect in the real world (Casper\n",
            "et al., 2023). This is largely because the alignment methods\n",
            "shown to work best—RLHF (Christiano et al., 2017) and\n",
            "1Stanford University2Contextual AI. Correspondence to:\n",
            "Kawin Ethayarajh <kawin@stanford.edu >.the mathematically equivalent DPO (Rafailov et al., 2023)—\n",
            "take preference data as input.\n",
            "To understand why these alignment methods work so well,\n",
            "and whether feedback needs to be in the form of prefer-\n",
            "ences, we frame them through the lens of prospect the-\n",
            "ory(Kahneman & Tversky, 1979; Tversky & Kahneman,\n",
            "1992). Prospect theory explains why humans make deci-\n",
            "sions about uncertain events that do not maximize expected\n",
            "value. It formalizes how humans perceive random variables\n",
            "in a biased but well-defined manner; for example, relative to\n",
            "some reference point, humans are more sensitive to losses\n",
            "than gains, a property called loss aversion . We show that\n",
            "popular alignment methods such as PPO (Schulman et al.,\n",
            "2017), DPO (Rafailov et al., 2023), and SLiC (Zhao et al.,\n",
            "2023) implicitly model such biases, helping explain their\n",
            "success independently of the data used. For this reason, we\n",
            "call them human-aware loss functions (HALOs).\n",
            "Although it is impossible to say that HALOs are categori-\n",
            "cally better than non-HALOs, we find that among existing\n",
            "methods, those that meet the definition of a HALO work\n",
            "better than those that do not. We find that DPO perfor-\n",
            "mance can even be matched at most scales by running an\n",
            "offline PPO variant on dummy +1/-1 rewards, suggesting\n",
            "that preference data might not be needed if the inductive\n",
            "bias in the loss function is good enough. However, despite\n",
            "the surprising success of this simple baseline, it significantly\n",
            "lags behind DPO at the 30B model scale and suffers from\n",
            "hyperparameter sensitivity, making it difficult to use.\n",
            "Taking a more principled approach, we derive a HALO using\n",
            "the model of human utility that Kahneman & Tversky empir-\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 90}\n",
            "hyperparameter sensitivity, making it difficult to use.\n",
            "Taking a more principled approach, we derive a HALO using\n",
            "the model of human utility that Kahneman & Tversky empir-\n",
            "ically derived to describe how humans make decisions about\n",
            "uncertain monetary outcomes (Tversky & Kahneman, 1992).\n",
            "This approach, which we call Kahneman-Tversky Optimiza-\n",
            "tion (KTO), directly maximizes the utility of generations\n",
            "instead of maximizing the log-likelihood of preferences, as\n",
            "most current methods do. KTO only requires a binary signal\n",
            "of whether an output is desirable or undesirable for a given\n",
            "input. This data is much more abundant, cheaper, and faster\n",
            "to collect in the real world than preferences, making it easier\n",
            "to scale alignment in production environments and rapidly\n",
            "iterate on models.\n",
            "In our experiments, we find that:\n",
            "1arXiv:2402.01306v1  [cs.LG]  2 Feb 2024Model Alignment as Prospect Theoretic Optimization\n",
            "Figure 1. The traditional pipeline for LLM alignment starts with supervised finetuning, followed by fitting the LLM to paired preference\n",
            "data using a method such as RLHF or DPO. However, the paired preferences that existing approaches need are hard-to-get. Kahneman-\n",
            "Tversky Optimization (KTO) only needs to know whether a given output is (un)desirable for the input, giving it access to a source of data\n",
            "that is much more abundant, cheaper, and faster to collect in the real world.\n",
            "•KTO matches or exceeds DPO performance at scales\n",
            "from 1B to 30B parameters.1That is, taking a prefer-\n",
            "ence dataset of nDPO pairs and breaking it up into\n",
            "2nexamples for KTO can yield better generations,\n",
            "despite the model ostensibly learning from a weaker\n",
            "signal. We provide some theoretical explanations for\n",
            "this phenomenon (§4.3).\n",
            "•KTO can handle extreme data imbalances, matching\n",
            "DPO performance while using up to 90% fewer desir-\n",
            "able examples (i.e., examples of good generations). Its\n",
            "success thus cannot be ascribed to the alignment data\n",
            "being sourced from a preference dataset.\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 91}\n",
            "able examples (i.e., examples of good generations). Its\n",
            "success thus cannot be ascribed to the alignment data\n",
            "being sourced from a preference dataset.\n",
            "•When the pretrained model is sufficiently good, one\n",
            "can skip supervised finetuning and go straight to KTO\n",
            "without a loss in generation quality. In contrast, we\n",
            "find that without doing SFT first, DPO-aligned models\n",
            "are significantly worse at all scales.\n",
            "The fact that KTO can match and sometimes even outper-\n",
            "form DPO is surprising, given that it learns from a weaker\n",
            "signal. We conclude by discussing some theoretical expla-\n",
            "nations for this phenomenon.\n",
            "2. Background\n",
            "Feedback-aligned LLMs are traditionally trained in three\n",
            "stages (Ouyang et al., 2022):\n",
            "Pretraining Given a large corpus, train the model to pre-\n",
            "dict the next token conditioned on the preceding text using\n",
            "the cross-entropy loss. Let πdenote the pretrained model.\n",
            "1Our code is available on Github and models on Huggingface.Supervised Finetuning Finetune the model to predict the\n",
            "next token on data that is more relevant to the downstream\n",
            "task. Often, such data will comprise instructions and an\n",
            "appropriate response (i.e., instruction finetuning). Let πref\n",
            "denote the finetuned model.\n",
            "RLHF Given a dataset Dof preferences (x, yw, yl)—\n",
            "where xis an input, yw, ylare the preferred and dispreferred\n",
            "outputs (i.e., yw≻ylforx), and r∗is the “true” reward\n",
            "function underlying the preferences—it is first assumed that\n",
            "the probability that ywis preferred to ylcan be captured with\n",
            "a specific function class, typically a Bradley-Terry model\n",
            "(Bradley & Terry, 1952). Where σis the logistic function:\n",
            "p∗(yw≻yl|x) =σ(r∗(x, yw)−r∗(x, yl)) (1)\n",
            "Since getting the true reward from a human would be in-\n",
            "tractably expensive, a reward model rϕlearns to serve as a\n",
            "proxy, done by minimizing the negative log-likelihood of\n",
            "the human preference data:\n",
            "LR(rϕ) =Ex,yw,yl∼D[−logσ(rϕ(x, yw)−rϕ(x, yl))]\n",
            "But solely maximizing the reward might come at the ex-\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 92}\n",
            "proxy, done by minimizing the negative log-likelihood of\n",
            "the human preference data:\n",
            "LR(rϕ) =Ex,yw,yl∼D[−logσ(rϕ(x, yw)−rϕ(x, yl))]\n",
            "But solely maximizing the reward might come at the ex-\n",
            "pense of desiderata such as generating grammatical text. To\n",
            "avoid this, a KL divergence penalty is introduced to restrict\n",
            "how far the language model can drift from πref. Where πθis\n",
            "the model we are optimizing, the optimal model π∗is that\n",
            "which maximizes\n",
            "Ex∈D,y∈πθ[rϕ(x, y)]−βD KL(πθ(y|x)∥πref(y|x))(2)\n",
            "where β > 0is a hyperparameter. Since this objective is\n",
            "not differentiable, we need to use an RL algorithm like PPO\n",
            "(Schulman et al., 2017).\n",
            "2Model Alignment as Prospect Theoretic Optimization\n",
            "However, RLHF is often slow (largely because of having\n",
            "to sample generations) and quite unstable in practice (espe-\n",
            "cially in a distributed setting). For this reason, recent work\n",
            "has focused on designing closed-form losses that maximize\n",
            "the margin between the preferred and dispreferred gener-\n",
            "ations, such as Sequence-Likelihood Calibration (SLiC)\n",
            "(Zhao et al., 2023) and Direct Preference Optimization\n",
            "(DPO) (Rafailov et al., 2023). The latter has become popular\n",
            "due to its mathematical equivalence with RLHF:\n",
            "LDPO(πθ, πref) =\n",
            "E\u0014\n",
            "−logσ\u0012\n",
            "βlogπθ(yw|x)\n",
            "πref(yw|x)−βlogπθ(yl|x)\n",
            "πref(yl|x)\u0013\u0015(3)\n",
            "3. A Prospect Theoretic View of Alignment\n",
            "Kahneman & Tversky’s prospect theory explains why, faced\n",
            "with an uncertain event, humans make decisions that do not\n",
            "maximize the expected value (1992). For example, because\n",
            "humans are loss-averse, given a gamble that returns $100\n",
            "with 80% probability and $0 with 20% probability, a person\n",
            "might accept $60 to avoid the gamble, despite their certainty\n",
            "equivalent of$60 being less than the expected value of $80.\n",
            "3.1. Prospect Theory\n",
            "In prospect theory, human utility depends on a value func-\n",
            "tionand a weighting function :2\n",
            "Definition 3.1. Avalue function v:z→Rmaps an out-\n",
            "come z, relative to some reference point zref, to its perceived\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 93}\n",
            "tionand a weighting function :2\n",
            "Definition 3.1. Avalue function v:z→Rmaps an out-\n",
            "come z, relative to some reference point zref, to its perceived\n",
            "(or subjective) value. For example, these functions capture\n",
            "the fact that humans tend to be more sensitive to relative\n",
            "losses than relative gains of the same magnitude.\n",
            "Definition 3.2. Aweighting function wis the derivative of\n",
            "acapacity function that maps cumulative probabilities to\n",
            "perceived cumulative probabilities. These functions capture,\n",
            "for example, the fact that humans tend to overestimate the\n",
            "chance of rare events. Let wzdenote the weight placed on\n",
            "outcome z.\n",
            "Definition 3.3. The utility of a random variable Zis a\n",
            "function of its outcomes: u(Z)≜P\n",
            "z∈Zwzv(z−zref).\n",
            "However, because humans do not see the full probability\n",
            "distribution of an LLM, weighting functions are not salient\n",
            "to this discussion; we will focus only on value functions. Us-\n",
            "ing experiments that presented real humans with monetary\n",
            "gambles and asked for their certainty equivalent, Tversky &\n",
            "Kahneman (1992) proposed the following functional form\n",
            "for human value:\n",
            "v(z, z ref;λ;α) =(\n",
            "(z−zref)αifz > z ref\n",
            "−λ(zref−z)αifz < z ref(4)\n",
            "2Cumulative prospect theory is the full name of the expanded\n",
            "theory we dicuss here (Tversky & Kahneman, 1992).\n",
            "Figure 2. The utility that a human gets from the outcome of a\n",
            "random variable, as imputed by the value function implicit in\n",
            "HALOs. Notice that the imputed functions share properties such\n",
            "as loss aversion with the human value functions that Kahneman &\n",
            "Tversky empirically derived (1992).\n",
            "where the median value of hyperparameter α= 0.88and\n",
            "λ= 2.25across individuals. αcontrols how quickly utility\n",
            "changes and λcontrols the degree of loss aversion. While\n",
            "the shape of the median Kahneman-Tversky value function\n",
            "is illustrated in Figure 2, it should be noted that it varies\n",
            "across individuals (Tversky & Kahneman, 1992). There\n",
            "are also other functional forms for the value function that\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 94}\n",
            "is illustrated in Figure 2, it should be noted that it varies\n",
            "across individuals (Tversky & Kahneman, 1992). There\n",
            "are also other functional forms for the value function that\n",
            "have been proposed in later work (Gurevich et al., 2009).\n",
            "The salient qualities of a value function are: the existence\n",
            "of a reference point that is added or subtracted to get the\n",
            "relative gain or loss respectively; concavity in relative gains\n",
            "(i.e. diminishing sensitivity away from zref); loss aversion\n",
            "(i.e., greater sensitivity to losses).\n",
            "3.2. HALOs\n",
            "Informally, HALOs are loss functions that model the human\n",
            "biases in Tversky & Kahneman (1992). Formally,\n",
            "Definition 3.4 (HALOs ).Letx∈ X denote an input and\n",
            "y∈ Y an output. Then f: (x, y)→ Ris ahuman-aware\n",
            "loss function if there exists the following: a parameterized\n",
            "reward function rθsuch that ∀(x1, y1),(x2, y2)∈ X × Y ,\n",
            "rθ(x1, y1)> rθ(x2, y2)⇐⇒ (x1, y1)≻rθ(x2, y2)\n",
            "reference point distributions Qx(X′), Qy(Y′|X′), a value\n",
            "function vf: R→ Rthat is monotonic non-decreasing and\n",
            "concave in (0,∞), and a negative affine function tsuch that\n",
            "f(x, y;θ) =t(vf(rθ(x, y)−Ex′,y′[rθ(x′, y′)])) (5)\n",
            "where x′∼Qx(X′)andy′∼Qy(Y′|x′).\n",
            "Put simply, the requirement for the reward function is that it\n",
            "assigns higher rewards to input-output pairs that are more\n",
            "3Model Alignment as Prospect Theoretic Optimization\n",
            "preferred under it. The reference point is the expected re-\n",
            "ward with respect to input-output pairs sampled from the\n",
            "distributions Qx, Qy. We require that the value function\n",
            "be concave in gains but not necessarily convex in losses—\n",
            "unlike the canonical Kahneman-Tversky value functions—\n",
            "because in the original work on prospect theory, a minority\n",
            "of individuals were found to be risk-averse in both the gain\n",
            "and loss regime (i.e., concave in both gains and losses)\n",
            "(Kahneman & Tversky, 1979). Note that risk-aversion is\n",
            "different from loss-aversion; they relate to the curvature and\n",
            "magnitude of the slope respectively.\n",
            "Proposition 3.5. DPO, SLiC (calibration loss only), and\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 95}\n",
            "different from loss-aversion; they relate to the curvature and\n",
            "magnitude of the slope respectively.\n",
            "Proposition 3.5. DPO, SLiC (calibration loss only), and\n",
            "PPO-Clip are human-aware loss functions.\n",
            "The proof is deferred to Appendix A. In Figure 2, we can\n",
            "see this more intuitively by plotting the value function for\n",
            "each loss (i.e., the implied human utility). We see that the\n",
            "value functions of all three losses incorporate a sense of loss\n",
            "aversion, although this is not needed to meet the definition\n",
            "of a HALO, since there are individuals and scenarios for\n",
            "which loss aversion does not necessarily apply. The value\n",
            "functions are also either concave or affine (depending on\n",
            "the interval), unlike the standard Kahneman-Tversky value\n",
            "function, which is concave in gains but convex in losses.\n",
            "The reference point distributions used also differs across the\n",
            "losses.\n",
            "3.3. Does being a HALO matter?\n",
            "A natural question is whether the modeling of human biases\n",
            "in HALOs has practical benefits. This is difficult to answer,\n",
            "since both HALOs and non-HALOs are diverse function\n",
            "classes, but we attempt to do so by comparing popular non-\n",
            "HALO and HALO baselines on the exact same data:\n",
            "1.CSFT : Conditional SFT is a simple alignment method\n",
            "where a control token is prepended to the output during\n",
            "training; then, at inference, the control token corre-\n",
            "sponding to desirable generations (e.g., <|good|> ) is\n",
            "appended to the input to induce good generations (Kor-\n",
            "bak et al., 2023). This is a non-HALO loss.\n",
            "2.SLiC : SLiC with a regularization penalty ( λreg̸= 0) is\n",
            "a non-HALO loss:\n",
            "LSLiC(πθ, πref) =Lcal(πθ) +λregLreg(πθ)\n",
            "Lcal=Ex,yw,yl∼D\u0014\n",
            "max\u0012\n",
            "0, δ−logπθ(yw|x)\n",
            "πθ(yl|x)\u0013\u0015\n",
            "Lreg=Ex∼D,y∼πref(x)[−logπθ(y|x)]\n",
            "Although the max-margin loss Lcalis a HALO on its\n",
            "own (Proposition 3.5), the complete loss is not, since\n",
            "theLregterm is the standard language modeling loss.\n",
            "3.DPO : DPO, as defined in (3), is a HALO loss (Propo-\n",
            "sition 3.5).4.PPO (offline) : The standard RLHF objective in (2) is\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 96}\n",
            "theLregterm is the standard language modeling loss.\n",
            "3.DPO : DPO, as defined in (3), is a HALO loss (Propo-\n",
            "sition 3.5).4.PPO (offline) : The standard RLHF objective in (2) is\n",
            "typically optimized with PPO-Clip, which works by\n",
            "“clipping” how far πθcan drift from the version πoldat\n",
            "the previous step:\n",
            "LPPO (offline) =−Ex,y,t∼D[min( qθA(x, y<t, yt),\n",
            "clip(qθ,1−ϵ,1 +ϵ)A(x, y<t, yt))]\n",
            "where qθ= logπθ\n",
            "πoldandA(x, y<t, yt)is the per-token\n",
            "advantage (i.e., the surplus benefit from producing a\n",
            "given token in a given state).\n",
            "PPO is an online algorithm—generations are sampled\n",
            "from the current model, judged by a reward model, and\n",
            "then used to update the current version. However, this\n",
            "process is slow (due to having to sample generations),\n",
            "so we choose to use offline data instead. Because\n",
            "RLHF is also quite unstable in a distributed setting, we\n",
            "never update πoldand keep it as πref, instead clipping\n",
            "less conservatively than we traditionally would (see\n",
            "Appendix B for details). Baheti et al. (2023) found that\n",
            "these changes, along with treating the entire output\n",
            "sequence as a single action, greatly improves stability.\n",
            "However, since RLHF has historically calculated token-\n",
            "level advantages, we omit the third change and only\n",
            "preserve the first two. The PPO-Clip loss itself is left\n",
            "unchanged and is therefore a HALO (Proposition 3.5).\n",
            "Calling this method PPO is somewhat imprecise, be-\n",
            "cause it is offline and takes only one step, but to avoid\n",
            "introducing too many new terms, we will call this PPO\n",
            "(offline) . Instead of using learned rewards, we simplify\n",
            "even further and use dummy +1/-1 rewards for ywand\n",
            "ylinstead. Further details on the implementation of\n",
            "this method can be found in Appendix B.\n",
            "We compare these baselines on a suite of 7 models spanning\n",
            "two model families, Pythia- {1.4B, 2.8B, 6.9B, 12B }(Bi-\n",
            "derman et al., 2023) and Llama- {7B, 13B, 30B }(Touvron\n",
            "et al., 2023). This permits us to see how LLM alignment\n",
            "scales within a model family (Llama-2 lacks a 30B model,\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 97}\n",
            "derman et al., 2023) and Llama- {7B, 13B, 30B }(Touvron\n",
            "et al., 2023). This permits us to see how LLM alignment\n",
            "scales within a model family (Llama-2 lacks a 30B model,\n",
            "hence our use of Llama). Later experiments ( §4.2) are done\n",
            "on Mistral-7B and its derivatives (Jiang et al., 2023). The\n",
            "models were trained on a combination of Anthropic HH\n",
            "(Ganguli et al., 2022), OpenAssistant (K ¨opf et al., 2023),\n",
            "and SHP (Ethayarajh et al., 2022).\n",
            "All models were aligned under identical settings on the\n",
            "same data (e.g., same effective batch size, same optimizer,\n",
            "etc.), save for hyperparameters unique to them. Similar to\n",
            "Rafailov et al. (2023), the target sequences for SFT are a\n",
            "subset of the generations used to subsequently align the\n",
            "model; however, for a more realistic SFT setup, we do not\n",
            "necessarily set the most preferred generation to be the target\n",
            "(with the exception of HH, since the dispreferred output in\n",
            "that dataset is often harmful). Then we used GPT-4-0613\n",
            "4Model Alignment as Prospect Theoretic Optimization\n",
            "Figure 3. Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC\n",
            "and CSFT), though the gap is only significant (p < 0.05)at 13B+ model sizes. In fact, only the HALO-aligned Llama- {13B, 30B }\n",
            "models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset.\n",
            "It is also worth noting that up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage.\n",
            "to judge whether the aligned model’s response was bet-\n",
            "ter than the SFT target for the given input with respect to\n",
            "helpfulness, harmlessness, and conciseness, a now standard\n",
            "practice (Zheng et al., 2023; Li et al., 2023).3Note that\n",
            "while the SFT target is considered a desirable output for x,\n",
            "it is by no means the best output, meaning that it can be\n",
            "improved upon by an aligned model.\n",
            "In Figure 3, we see the results of this analysis:\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 98}\n",
            "it is by no means the best output, meaning that it can be\n",
            "improved upon by an aligned model.\n",
            "In Figure 3, we see the results of this analysis:\n",
            "•The HALOs we tested (DPO and our PPO variant) ei-\n",
            "ther match or outperform the non-HALOs at all scales,\n",
            "though the gap is only significant (p <0.05)at 13B+\n",
            "model sizes. In fact, only the HALO-aligned Llama-\n",
            "{13B, 30B }models match or exceed a win rate of 50%\n",
            "(i.e., are able to match or exceed the generation quality\n",
            "of the SFT targets in the test data).\n",
            "•Up to a scale of 7B parameters, alignment provides\n",
            "virtually no gains over SFT alone. However, it is worth\n",
            "noting that if the SFT data distribution were less sim-\n",
            "ilar to the preference data, then the gains from the\n",
            "alignment stage would ostensibly be greater.\n",
            "•Surprisingly, despite only using dummy +1/-1 rewards,\n",
            "our offline PPO variant performs as well as DPO for\n",
            "all models except Llama30B. This challenges conven-\n",
            "tional wisdom, which places heavy emphasis on reward\n",
            "learning (Casper et al., 2023), suggesting that even the\n",
            "simplest rewards can prove useful when used in a loss\n",
            "function that has a strong inductive bias. Despite its\n",
            "surprising success, our offline PPO baseline still suffers\n",
            "from hyperparameter sensitivity and training instability,\n",
            "3We validate that GPT-4 judgments concur with human judg-\n",
            "ments in Appendix C.albeit not to the same extent as traditional RLHF.\n",
            "4. Kahneman-Tversky Optimization\n",
            "The surprising success of offline PPO with dummy +1/-1 re-\n",
            "wards suggests that—with the right HALO—a binary signal\n",
            "of good/bad generations may be sufficient to reach DPO-\n",
            "level performance, even if the offline PPO approach itself\n",
            "was unable to do so past a certain scale ( §3.3). Taking a\n",
            "more principled approach, we now derive a HALO using the\n",
            "Kahneman-Tversky model of human utility, which allows\n",
            "us to directly optimize for utility instead of maximizing the\n",
            "log-likelihood of preferences. This Kahneman-Tversky Op-\n",
            "timization (KTO) loss only needs a binary signal of whether\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 99}\n",
            "us to directly optimize for utility instead of maximizing the\n",
            "log-likelihood of preferences. This Kahneman-Tversky Op-\n",
            "timization (KTO) loss only needs a binary signal of whether\n",
            "an output is (un)desirable for a given input, giving it access\n",
            "to a source of data is more abundant, cheaper, and faster to\n",
            "collect in the real world.\n",
            "4.1. Derivation\n",
            "From prior work (Go et al., 2023; Peng et al., 2019; Peters\n",
            "& Schaal, 2007), we know that the policy that maximizes\n",
            "the KL-constrained RLHF objective in (2) is\n",
            "π∗(y|x) =1\n",
            "Z(x)πref(y|x) exp\u00121\n",
            "βr∗(x, y)\u0013\n",
            "where Z(x)is a partition function. Rafailov et al. (2023)\n",
            "rewrite this in terms of the optimal reward for an input-\n",
            "output pair:\n",
            "r∗(x, y) =βlogπ∗(y|x)\n",
            "πref(y|x)+βlogZ(x) (6)\n",
            "They then plug this expression into the Bradley-Terry model\n",
            "of preferences and take the negative logarithm of that objec-\n",
            "tive to get the DPO loss (3).\n",
            "5Model Alignment as Prospect Theoretic Optimization\n",
            "Figure 4. Kahneman-Tversky Optimization (KTO) is as good or better than DPO at all scales, both when preceded and not preceded by\n",
            "supervised finetuning (SFT). In fact, for the Llama models, KTO alone matches the performance of SFT+DPO and is significantly better\n",
            "than DPO alone. Error bars denote a 90% binomial confidence interval.\n",
            "Instead, we plug this expression into the Kahneman-Tversky\n",
            "model of human utility, with some changes to make it more\n",
            "amenable to the LLM setting:\n",
            "1.The exponent in the Kahneman-Tversky value func-\n",
            "tion (4) makes it difficult to optimize, so we set vKTO\n",
            "to be the logistic function σ, which is also concave\n",
            "in gains and convex in losses. We replace the loss-\n",
            "aversion coefficient with two hyperparameters λD, λU\n",
            "that weight the losses for desirable and undesirable\n",
            "outputs respectively.\n",
            "2.The Kahneman-Tversky value function was derived\n",
            "based on experiments with humans and monetary gam-\n",
            "bles. Since LLM generations do not have a monetary\n",
            "reward associated with them, we set rKTOto be the\n",
            "implicit reward under the RLHF objective (6).\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 100}\n",
            "based on experiments with humans and monetary gam-\n",
            "bles. Since LLM generations do not have a monetary\n",
            "reward associated with them, we set rKTOto be the\n",
            "implicit reward under the RLHF objective (6).\n",
            "3.Rather than having just one dispreferred generation\n",
            "yl|xas the reference point, we assume that humans\n",
            "judge the quality of (x, y)in relation to all input-output\n",
            "pairs they have seen. Thus we write the reference point\n",
            "to be the expected reward under the optimal policy, not\n",
            "just for generations following xbut following any input\n",
            "x′:Ex′∼D,y′∼π∗[r∗(x′, y′)]. Under the assumption\n",
            "that the expected value of the partition function across\n",
            "x′is zero, this simplifies to the KL divergence between\n",
            "π∗andπrefscaled by β.\n",
            "Combining all of these changes, we can optimize the follow-\n",
            "ing loss, where the notion of an output being “desirable” or\n",
            "“undesirable” corresponds to the Kahneman-Tversky notion\n",
            "of a relative gain or loss.\n",
            "LKTO(πθ, πref) =Ex,y∼D[w(y)(1−vKTO(x, y;β))] (7)where\n",
            "rKTO(x, y) =βlogπθ(y|x)\n",
            "πref(y|x)\n",
            "zref=Ex′∼D[βKL(πθ(y′|x′)∥πref(y′|x′))]\n",
            "vKTO(x, y;β) =(\n",
            "σ(rKTO(x, y)−zref)ify∼ydesirable|x\n",
            "σ(zref−rKTO(x, y))ify∼yundesirable |x\n",
            "w(y) =(\n",
            "λDify∼ydesirable|x\n",
            "λUify∼yundesirable |x\n",
            "Intuitively, KTO works because if the model increases the\n",
            "reward of a desirable example in a generic way, then the KL\n",
            "penalty will also rise and no progress will be made on the\n",
            "loss. This forces the model to learn exactly what makes an\n",
            "output desirable, so that the reward can be increased while\n",
            "keeping the KL term flat (or even decreasing it). A similar\n",
            "argument works in the other direction as well, though the\n",
            "non-negativity of the KL term allows faster saturation.\n",
            "Implementation In practice, we estimate the KL term by\n",
            "matching inputs x′with unrelated outputs y′\n",
            "Uin a batch of\n",
            "sizemand then calculating max\u0010\n",
            "0,1\n",
            "mPlogπθ(y′\n",
            "U|x′)\n",
            "πref(y′\n",
            "U|x′)\u0011\n",
            "over the entire batch. We do not back-propagate through the\n",
            "KL term, as it makes training much more stable. This means\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 101}\n",
            "sizemand then calculating max\u0010\n",
            "0,1\n",
            "mPlogπθ(y′\n",
            "U|x′)\n",
            "πref(y′\n",
            "U|x′)\u0011\n",
            "over the entire batch. We do not back-propagate through the\n",
            "KL term, as it makes training much more stable. This means\n",
            "that the KL term purely serves to control how saturated the\n",
            "loss is.\n",
            "βhas the same meaning as in DPO; the lower it is, the less\n",
            "we penalize πθfrom moving away from the SFT model\n",
            "πref. We find that β= 0.1is close-to-best on most datasets.\n",
            "Where nDandnUrefer to the number of desirable and\n",
            "undesirable examples respectively, we set λD, λUsuch that\n",
            "λDnD\n",
            "λUnU∈\u0014\n",
            "1,4\n",
            "3\u0015\n",
            "(8)\n",
            "6Model Alignment as Prospect Theoretic Optimization\n",
            "Figure 5. Without doing SFT first, DPO-aligned models tend to\n",
            "ramble and hallucinate entire conversations. KTO does not suffer\n",
            "from this issue.\n",
            "where at least one of the two should be set to 1 and the ratio\n",
            "is controlled by changing the other. For example, if there\n",
            "is a 1:1 ratio of desirable:undesirable examples, we would\n",
            "setλU= 1, λD∈[1,1.33]. If we then discard 90% of\n",
            "the desirable examples and only keep 10%, then we would\n",
            "setλU= 1, λD∈[10,13.33]. The interval [1,4/3]was\n",
            "determined empirically and suggests a value function that\n",
            "is more gain-sensitive than loss-sensitive, in contrast to the\n",
            "original Kahneman-Tversky value function (4). However,\n",
            "the ideal interval is also task-dependent; for example, if\n",
            "avoiding negative outcomes were very important, then we\n",
            "might consider a setting of λU>1instead.\n",
            "Data If the alignment data is naturally binary, every posi-\n",
            "tive example can be assumed to be drawn from ydesirable|x\n",
            "and every negative example from yundesirable |x. However, the\n",
            "canonical feedback datasets in academic research (HH, SHP,\n",
            "OASST) are in preference format, since the methods that\n",
            "have worked best up until now are preference-based. In our\n",
            "experiments, we converted preference data yw≻ylby as-\n",
            "suming that ywis drawn from the desirable distribution and\n",
            "ylfrom the undesirable one. To enable an apples-to-apples\n",
            "comparison with DPO, we apply KTO on the same data\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 102}\n",
            "suming that ywis drawn from the desirable distribution and\n",
            "ylfrom the undesirable one. To enable an apples-to-apples\n",
            "comparison with DPO, we apply KTO on the same data\n",
            "for most experiments. However, to ensure that KTO can\n",
            "be used with non-preference data, we also subsample one\n",
            "output yperxfor some experiments (denoted one- y-per-x).\n",
            "If the data is score-based, where a higher score denotes\n",
            "greater desirability, one has multiple options:\n",
            "•Assume that any output with a score above some fixed\n",
            "threshold τis desirable.\n",
            "•Assume that any output with a score above the mean\n",
            "or median (either across all inputs or just the input it\n",
            "was conditioned on) is desirable.\n",
            "•Let desirability be a Bernoulli random variable where\n",
            "p(y∼ydesirable|x)is some function of its score (e.g.,\n",
            "logistic). Then randomly sample to determine whether\n",
            "yis desirable or not.\n",
            "Figure 6. Even after discarding 90% of the desirable examples\n",
            "while keeping all of the undesirable data (leading to a 1:10 ratio\n",
            "of desirable:undesirable data), a KTO-aligned Llama-7B model\n",
            "still outperforms its DPO counterpart. This implies that preference\n",
            "pairs do not have to be the source of KTO data.\n",
            "4.2. Experiments\n",
            "KTO≥DPO As seen in Figure 4, SFT+KTO is com-\n",
            "petitive with SFT+DPO at model scales from 1B to 30B,\n",
            "despite learning from a weaker signal. KTO alone is better\n",
            "than DPO alone for the Llama- {7B, 13B, 30B }models, and\n",
            "this gap is significant ( p <0.01) at 7B and 30B even after\n",
            "correcting for multiple comparisons (Holm, 1979). Perhaps\n",
            "most surprising is the fact that a KTO-aligned Llama- {13B,\n",
            "30B}model is competitive with its SFT+KTO counterpart,\n",
            "despite not undergoing supervised finetuning first, and is the\n",
            "only alignment method of the ones we tested to show this\n",
            "behavior. This is perhaps due to the fact that KTO keeps\n",
            "the average response length roughly the same as it is for\n",
            "the SFT model. In contrast, doing DPO without SFT first\n",
            "causes the average response length to increase dramatically.\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 103}\n",
            "the average response length roughly the same as it is for\n",
            "the SFT model. In contrast, doing DPO without SFT first\n",
            "causes the average response length to increase dramatically.\n",
            "KTO data need not come from preference datasets.\n",
            "Might KTO be secretly benefiting from the fact that its 2n\n",
            "examples in the previous experiment came from nprefer-\n",
            "ence pairs instead of a naturally unpaired data distribution?\n",
            "To test this, we randomly discard increasingly large frac-\n",
            "tions of the desirable data before KTO-aligning a Llama-7B\n",
            "model. For example, if we discard 90% of the desirable\n",
            "data while leaving the undesirable data untouched, then the\n",
            "ratio of desirable:undesirable examples goes from 1:1 to\n",
            "1:10 and the vast majority of examples no longer have a\n",
            "preferred output counterpart. We handle such imbalances\n",
            "by changing the loss weights λD, λUto satisfy the criteria\n",
            "in (8); when we drop 90% of the desirable data, we set\n",
            "λu= 1, λD= 13.33. The full results are given in Figure\n",
            "6. For Llama-7b, we find that up to 90% of the desirable\n",
            "7Model Alignment as Prospect Theoretic Optimization\n",
            "Table 1. In aligning Mistral-7B on the OpenAssistant dataset, we\n",
            "find that using KTO with only one output per input still outper-\n",
            "forms DPO, despite this restriction reducing the amount of training\n",
            "data by 72%. A 90% confidence interval is given.\n",
            "Method Winrate vs. SFT Target\n",
            "Mistral-7B (unaligned) 0.525 ±0.037\n",
            "Mistral-7B + DPO 0.600 ±0.037\n",
            "Mistral-7B + KTO (all yperx) 0.652±0.036\n",
            "Mistral-7B + KTO (one yperx) 0.631 ±0.036\n",
            "Mistral-7B-Instruct 0.621 ±0.031\n",
            "data can in fact be discarded while still outperforming DPO.\n",
            "A similar trend holds when discarding undesirable data.\n",
            "For different models and datasets, the optimal settings of\n",
            "λD, λUdiffer.\n",
            "We further verify this claim by aligning Mistral-7B on Ope-\n",
            "nAssistant using DPO (on npairs), standard KTO (on all\n",
            "2noutputs), and KTO where only one yperxis used. Since\n",
            "the output of one yin OpenAssistant is not conditioned on\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 104}\n",
            "nAssistant using DPO (on npairs), standard KTO (on all\n",
            "2noutputs), and KTO where only one yperxis used. Since\n",
            "the output of one yin OpenAssistant is not conditioned on\n",
            "the other outputs for the same input, the latter effectively\n",
            "captures the setting where the data is from an inherently un-\n",
            "paired distribution. Despite the one- y-per-xsetup decreas-\n",
            "ing the amount of training data by 72%, the KTO-aligned\n",
            "model still outperforms both its DPO counterpart and the\n",
            "official instruction-tuned version of Mistral-7B (Jiang et al.,\n",
            "2023), as seen in Table 1.\n",
            "On average, KTO improves performance across gener-\n",
            "ative benchmarks. Zephyr- βis a variant of Mistral-7B\n",
            "that has been instruction-tuned and DPO-aligned on the Ul-\n",
            "traFeedback dataset (Tunstall et al., 2023; Cui et al., 2023).\n",
            "We find that substituting KTO for DPO (and changing noth-\n",
            "ing else) improves performance across MMLU (0-shot)\n",
            "(Hendrycks et al., 2021), GSM8K (8-shot, CoT) (Cobbe\n",
            "et al., 2021), HumanEval (0-shot) (Chen et al., 2021), and\n",
            "BigBench-Hard (3-shot CoT) (Srivastava et al., 2022). On\n",
            "GSM8K, just swapping DPO for KTO improves perfor-\n",
            "mance by 13.5 points. Even when we align with KTO using\n",
            "only one yperx(i.e., reducing the data volume by half), we\n",
            "still outperform DPO on all but one benchmark.\n",
            "4.3. Theoretical Analysis\n",
            "KTO was designed with the motivation that even if it had\n",
            "to learn from a weaker signal, it would make up for this\n",
            "limitation with the fact that it has access to much more\n",
            "data in the real world, where thumbs-up/thumbs-down data\n",
            "is common but preferences are scarce and expensive to\n",
            "collect. So why does KTO perform as good or better than\n",
            "DPO in our experiments, when it sees the same amount\n",
            "of data? Data efficiency may not be the only answer. Our\n",
            "theoretical analysis suggests that preference likelihood canTable 2. Aligning Zephyr (Tunstall et al., 2023), a derivative of\n",
            "Mistral-7B, on UltraFeedback with KTO instead of DPO improves\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 105}\n",
            "theoretical analysis suggests that preference likelihood canTable 2. Aligning Zephyr (Tunstall et al., 2023), a derivative of\n",
            "Mistral-7B, on UltraFeedback with KTO instead of DPO improves\n",
            "results across a suite of benchmarks. This is true even when only\n",
            "one of the two outputs in each preference is seen by KTO, despite\n",
            "this reducing the volume of data by half (one- y-per-x).\n",
            "Dataset ( →) MMLU GSM8k HumanEval BBH\n",
            "Metric ( →) EM EM pass@1 EM\n",
            "Zephyr- βSFT 57.2 39.0 30.1 46.3\n",
            "+DPO 58.2 40.0 30.1 44.1\n",
            "+KTO 58.6 53.5 30.9 52.6\n",
            "+KTO (one- y-per-x)58.0 50.0 30.7 49.9\n",
            "be maximized without necessarily maximizing underlying\n",
            "human utility and that KTO implicitly ignores noisy and\n",
            "intransitive data.\n",
            "Proposition 4.1. KTO does not learn from undesirable ex-\n",
            "amples with sufficiently high rewards or desirable examples\n",
            "with sufficiently low rewards.\n",
            "Informally, if an example is too difficult to learn from, then\n",
            "the KTO update will not change πθ. This may be a blessing\n",
            "in disguise, since human preferences are often noisy and\n",
            "not every given preference can be recovered with the true\n",
            "reward r∗(Hoeffler & Ariely, 1999). This means that it may\n",
            "be useful to avoid unlearnable preferences. However, this is\n",
            "a double-edged sword: it also means that KTO could end\n",
            "up ignoring some data that is hard-to-learn but necessary to\n",
            "recover r∗, resulting in under-fitting.\n",
            "Theorem 4.2. Assuming the value function is logistic, for\n",
            "any bounded reward function ra, there exists a reward func-\n",
            "tion in its equivalence class (i.e., rb(x, y) =ra(x, y)+h(x)\n",
            "for some h(x)) that induces the same optimal policy π∗and\n",
            "Bradley-Terry preference distribution but a different human\n",
            "value distribution.\n",
            "A key insight from Rafailov et al. (2023) is that reward\n",
            "functions in the same equivalence class (i.e., differing only\n",
            "in an input-specific component) induce the same optimal\n",
            "policy under (2) and the same Bradley-Terry preference\n",
            "distribution. However, we show under mild assumptions\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 106}\n",
            "in an input-specific component) induce the same optimal\n",
            "policy under (2) and the same Bradley-Terry preference\n",
            "distribution. However, we show under mild assumptions\n",
            "that the value distribution—i.e., human utility—is affected\n",
            "by such input-specific changes, so maximizing preference\n",
            "likelihood does not mean one is maximizing human utility.\n",
            "Approaches that directly maximize utility, such as KTO,\n",
            "may thus perform better in open-ended evaluation.\n",
            "Theorem 4.3. Let two humans a, bhave value functions\n",
            "va, vband contradicting preferences y1≻ay2andy2≻by1\n",
            "for some input x. Assume πref(y|x) = 0 = ⇒πθ(y|x) = 0\n",
            "for all x, y. In the worst-case, the optimal policy under DPO\n",
            "decreases the expected value of both humans. In contrast, if\n",
            "each preference is broken up into two examples, then KTO\n",
            "(with default settings) does not change the policy.\n",
            "Informally, we assume that humans want the model to in-\n",
            "8Model Alignment as Prospect Theoretic Optimization\n",
            "crease and decrease the probability of generations they like\n",
            "and dislike respectively. However, the preferences of two\n",
            "humans often contradict, leading to a dataset containing\n",
            "intransitive preferences. In the worst-case, DPO allows one\n",
            "of the two preferences to be recovered while decreasing\n",
            "the expected value of both humans. In contrast, KTO will\n",
            "change nothing at all in any case. Since existing datasets\n",
            "contain preferences from multiple annotators, the existence\n",
            "of intransitivity may help explain why KTO works better.\n",
            "4.4. KTO vs. DPO – when to use which?\n",
            "When human feedback is in a binary format, and especially\n",
            "when there is an imbalance between the number of desirable\n",
            "and undesirable examples, KTO is the natural choice. When\n",
            "your data is in the form of preferences, the choice is less\n",
            "clear. Putting aside the greater data efficiency of KTO, our\n",
            "theoretical analysis suggests that if your preference data\n",
            "has sufficiently little noise and sufficiently little intransitiv-\n",
            "ity, then DPO will work better, since there is some risk of\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 107}\n",
            "theoretical analysis suggests that if your preference data\n",
            "has sufficiently little noise and sufficiently little intransitiv-\n",
            "ity, then DPO will work better, since there is some risk of\n",
            "KTO underfitting. But if there is enough noise and transitiv-\n",
            "ity, then the better worst-case guarantees of KTO will win\n",
            "out. Most publicly available preference datasets (e.g., SHP,\n",
            "OpenAssistant) contain noisy feedback from many different\n",
            "humans whose preferences likely contradict, which explains\n",
            "why KTO was able to match or exceed DPO performance\n",
            "in our experiments. Even AI feedback can be noisy and in-\n",
            "transitive, which helps explain why KTO outperforms DPO\n",
            "when aligning with the synthetic UltraFeedback data.\n",
            "5. Related Work\n",
            "Human feedback has been used to improve LLM capabilities\n",
            "in translation (Kreutzer et al., 2018), summarization (Stien-\n",
            "non et al., 2020), sentiment-conditioned generation (Ziegler\n",
            "et al., 2019), and instruction-following (Ouyang et al., 2022).\n",
            "The RLHF framework (Christiano et al., 2017; Bai et al.,\n",
            "2022) traditionally used to accomplish this is detailed in §2.\n",
            "Still, momentum has largely shifted in favor of closed-form\n",
            "losses that directly operate on offline preferences, such as\n",
            "DPO (Rafailov et al., 2023). This single stage of optimiza-\n",
            "tion distinguishes DPO from the conventional approach in\n",
            "preference-based RL, which learns a reward and then fits the\n",
            "policy to those rewards (Jain et al., 2013; Busa-Fekete et al.,\n",
            "2014). A recent string of work has centered on the idea of\n",
            "“self-training” or “self-play”, during which new preference\n",
            "data is inferred from a model’s generations (Chen et al.,\n",
            "2024; Yuan et al., 2024). Despite not being a human-aware\n",
            "loss, unlikelihood training was among to first to methods to\n",
            "align language models using a binary signal (Welleck et al.,\n",
            "2019). However, work by Korbak et al. (2023) found that it\n",
            "is worse than the CSFT baseline we tested in our work.\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 108}\n",
            "align language models using a binary signal (Welleck et al.,\n",
            "2019). However, work by Korbak et al. (2023) found that it\n",
            "is worse than the CSFT baseline we tested in our work.\n",
            "Prospect theory, despite being highly influential in behav-ioral economics, has had a fairly muted impact in machine\n",
            "learning, with work concentrated in human-robot interac-\n",
            "tion (Kwon et al., 2020; Sun et al., 2019; Chan et al., 2021).\n",
            "Learning from sparse binary feedback is a staple of infor-\n",
            "mation retrieval and recommender systems (He et al., 2017;\n",
            "Koren et al., 2009), although to our knowledge it has not\n",
            "been used to generate open-ended text.\n",
            "6. Future Work\n",
            "The existence of HALOs raises many questions. For one,\n",
            "the KTO loss is based on the Kahneman-Tversky value\n",
            "function for monetary gains and losses, which is almost\n",
            "certainly different from how humans perceive the relative\n",
            "goodness of text. What value function—and corresponding\n",
            "HALO—best describes how humans perceive language?\n",
            "Given that the data that KTO needs is much more abun-\n",
            "dant, cheaper, and faster to collect—both as human and AI\n",
            "feedback—how far can we push synthetic data? For exam-\n",
            "ple, if we wanted to create a toxicity dataset to align our\n",
            "models to be less toxic, creating a tuple (x, yw, yl)where yl\n",
            "is more toxic than ywis non-trivial. However, with KTO, we\n",
            "can easily create a dataset where desirability is determined\n",
            "by some black-box toxicity detection API. What other kinds\n",
            "of desiderata can we synthetically optimize for with KTO?\n",
            "Can we convert signals like “conversation lead to sale made”\n",
            "or “support ticket resolved” into KTO data?\n",
            "Currently, KTO can learn from score-based data when the\n",
            "score is used to infer desirability. However, can we design a\n",
            "HALO where scores are directly incorporated into this loss?\n",
            "7. Conclusion\n",
            "We proposed a class of functions called human-aware losses\n",
            "(HALOs) based on the idea of a Kahneman-Tversky value\n",
            "function, which models some of the key cognitive biases\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 109}\n",
            "7. Conclusion\n",
            "We proposed a class of functions called human-aware losses\n",
            "(HALOs) based on the idea of a Kahneman-Tversky value\n",
            "function, which models some of the key cognitive biases\n",
            "that inform how humans make decisions about uncertain out-\n",
            "comes. We showed that among existing alignment methods,\n",
            "those that met the definition of a HALO performed better\n",
            "than those that did not, suggesting a benefit to the modeling\n",
            "of human biases. We then designed a human-aware loss\n",
            "called KTO for directly maximizing the utility of genera-\n",
            "tions instead of maximizing preference likelihood. Despite\n",
            "only learning from a binary signal of whether an output is\n",
            "(un)desirable, KTO is as good or better than DPO at scales\n",
            "from 1B to 30B. Still, we make no claims that KTO is the\n",
            "best HALO for all scenarios; there remains much work to\n",
            "be done in discovering the optimal human-aware for each\n",
            "setting.\n",
            "9Model Alignment as Prospect Theoretic Optimization\n",
            "Acknowledgements\n",
            "We thank Dilip Arumugam and Arya McCarthy for feedback\n",
            "on the paper and Nathan Lambert for feedback on an early\n",
            "version of this draft. We thank Stas Bekman and Gautam\n",
            "Mittal for cluster assistance and Alex Manthey for helping\n",
            "with human evaluation.\n",
            "{'filename': '2402.01306_v1.pdf', 'title': 'KTO: Model Alignment as Prospect Theoretic Optimization', 'vs_index': 110}\n",
            "RewardBench\n",
            " :\n",
            "Evaluating Reward Models for Language Modeling\n",
            "Nathan Lambertα\n",
            "Valentina PyatkinαβJacob MorrisonαLJ MirandaαBill Yuchen Linα\n",
            "Khyathi ChanduαNouha DziriαSachin KumarαTom Zickγ\n",
            "Yejin ChoiαβNoah A. SmithαβHannaneh Hajishirziαβ\n",
            "αAllen Institute for Artificial Intelligence\n",
            "βUniversity of WashingtonγBerkman Klein Center, Harvard Law\n",
            "contact: nathanl@allenai.org\n",
            "Abstract\n",
            "Reward models (RMs) are at the crux of successful RLHF to align pretrained\n",
            "models to human preferences, yet there has been relatively little study that fo-\n",
            "cuses on evaluation of those reward models. Evaluating reward models presents\n",
            "an opportunity to understand the opaque technologies used for alignment of lan-\n",
            "guage models and which values are embedded in them. To date, very few de-\n",
            "scriptors of capabilities, training methods, or open-source reward models exist. In\n",
            "this paper, we present R EWARD BENCH , a benchmark dataset and code-base for\n",
            "evaluation, to enhance scientific understanding of reward models. The R EWARD -\n",
            "BENCH dataset is a collection of prompt-win-lose trios spanning chat, reasoning,\n",
            "and safety, to benchmark how reward models perform on challenging, structured\n",
            "and out-of-distribution queries. We created specific comparison datasets for RMs\n",
            "that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer\n",
            "should be preferred to another. On the R EWARD BENCH leaderboard, we evaluate\n",
            "reward models trained with a variety of methods, such as the direct MLE training\n",
            "of classifiers and the implicit reward modeling of Direct Preference Optimization\n",
            "(DPO), and on a spectrum of datasets. We present many findings on propensity for\n",
            "refusals, reasoning limitations, and instruction following shortcomings of various\n",
            "reward models towards a better understanding of the RLHF process.\n",
            "Leaderboard https://hf.co/spaces/allenai/reward-bench\n",
            "Code https://github.com/allenai/reward-bench\n",
            "Dataset https://hf.co/datasets/allenai/reward-bench\n",
            "1 Introduction\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 111}\n",
            "Leaderboard https://hf.co/spaces/allenai/reward-bench\n",
            "Code https://github.com/allenai/reward-bench\n",
            "Dataset https://hf.co/datasets/allenai/reward-bench\n",
            "1 Introduction\n",
            "Reinforcement learning from human feedback (RLHF) is a necessary but largely non-reproduced\n",
            "tool underlying the success of popular large language models (LLMs) such as OpenAI’s Chat-\n",
            "GPT (Schulman et al., 2022) and Anthropic’s Claude (Bai et al., 2022a). The prevalence of RLHF\n",
            "stems from its efficacy at circumventing one of the greatest difficulties in integrating human values\n",
            "and preferences into language models: specifying an explicit reward (Christiano et al., 2017). Re-\n",
            "ward models (RMs) are central to this process. They are created by taking copies of the originalarXiv:2403.13787v1  [cs.LG]  20 Mar 2024language model and training those copies on labeled preference data, producing a model that can\n",
            "predict whether a user is likely to prefer one piece of text over another. A reinforcement learning op-\n",
            "timizer then uses this reward model signal to update the parameters of the original model, improving\n",
            "performance on a variety of tasks (Ouyang et al., 2022; Bai et al., 2022a; Touvron et al., 2023).\n",
            "While the post-RLHF model (known as the policy ) and even the pretrained model are extensively\n",
            "documented and evaluated, the basic properties of the RLHF process receive far less attention.\n",
            "Though reward models are central to understanding the effectiveness of RLHF and moreover pro-\n",
            "vide a potential glimpse at how human values map onto language models, they remain catatonically\n",
            "under-evaluated. Recent work on training reward models (Zhu et al., 2023a; Jiang et al., 2023c) has\n",
            "begun to fill this gap, but utilizes validation sets from previous RLHF training processes, such as\n",
            "Anthropic’s Helpful and Harmless data (Bai et al., 2022a) or OpenAI’s Learning to Summarize (Sti-\n",
            "ennon et al., 2020), which are known to have ceilings on accuracy between 60 and 70% due to\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 112}\n",
            "Anthropic’s Helpful and Harmless data (Bai et al., 2022a) or OpenAI’s Learning to Summarize (Sti-\n",
            "ennon et al., 2020), which are known to have ceilings on accuracy between 60 and 70% due to\n",
            "inter-annotator disagreement (Wang et al., 2024). Similar investigations have yet to be conducted\n",
            "for Direct Policy Optimization (DPO) models. Moreover, newly released preference data aiming\n",
            "to expand the diversity of preference training datasets such as UltraFeedback (Cui et al., 2023) and\n",
            "Nectar (Zhu et al., 2023a), do not have test sets, necessitating a new style of evaluation for RMs.\n",
            "We begin to rectify the lack of evaluation techniques by introducing R EWARD BENCH , the first\n",
            "toolkit for benchmarking reward models. RLHF is inherently a broadly applicable process. It is\n",
            "used to enhance specific capabilities of language models such as safety (Dai et al., 2023) or reason-\n",
            "ing (Lightman et al., 2023; Havrilla et al., 2024a) as well as general capabilities such as instruction\n",
            "following (Ouyang et al., 2022) or “steerability” (Askell et al., 2021; Bai et al., 2022a). Evaluations\n",
            "for reward models must cover all of these categories.\n",
            "In this work, we curate new data and repurpose prompts from a variety of LLM evaluation tool-kits to\n",
            "create structured comparisons across a variety of reward model properties. Each sample is formatted\n",
            "as a prompt with a manual or human-verified chosen and rejected completion. We design subsets\n",
            "so as to vary in difficulty. Some are constructed such that many reward models can differentiate\n",
            "chosen from rejected completions, reaching nearly 100% accuracy. Others are more difficult and\n",
            "state-of-the-art language models only reach the 60 to 70% range.\n",
            "In addition to introducing a holistic benchmark, we aim to map the current landscape of openly\n",
            "available reward models via a reward model leaderboard. To do this, we evaluate most of the avail-\n",
            "able models such those trained as classifiers, including UltraRM (Cui et al., 2023), Starling (Zhu\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 113}\n",
            "available reward models via a reward model leaderboard. To do this, we evaluate most of the avail-\n",
            "able models such those trained as classifiers, including UltraRM (Cui et al., 2023), Starling (Zhu\n",
            "et al., 2023a), PairRM (Jiang et al., 2023c), SteamSHP (Ethayarajh et al., 2022), models from Re-\n",
            "ward rAnked FineTuning (RAFT) (Dong et al., 2023), and others. We also evaluate popular chat\n",
            "models trained with Direct Preference Optimization (DPO) (Rafailov et al., 2023), for example,\n",
            "Zephyr- β(Tunstall et al., 2023), Qwen-Chat (Bai et al., 2023), StableLM (Bellagente et al., 2024),\n",
            "and T ¨ulu 2 (Ivison et al., 2023) to ground recent debates on RLHF methods and showcase specific\n",
            "datasets where they fall short.\n",
            "With these models, we compare scaling, test reasoning capabilities, highlight three buckets of refusal\n",
            "behavior, and share more details on the inner workings of RMs. The accompanying code-base\n",
            "provides a common inference stack for many variations of models and we release many text-score\n",
            "pairs to analyze their performance.\n",
            "With REWARD BENCH , we:\n",
            "1. Release a common framework for evaluating the many different architectures of reward\n",
            "models , along with tools for visualization, training, and other analysis. We also release all data\n",
            "used in the evaluation, composed of text-score pairs for all inputs, to enable further data analysis\n",
            "on the properties of reward models.1\n",
            "2. Illustrate the differences between DPO and classifier-based reward models across a variety\n",
            "of datasets. DPO models, while more plentiful due to the method’s simplicity, fail to generalize\n",
            "to popular preference data test sets and present a higher variance in performance.\n",
            "3. Chart the landscape of current state-of-the-art reward models . We showcase the scaling laws,\n",
            "the propensity to refuse (or not), the reasoning capabilities, and more for popular RMs.\n",
            "1Data is here: https://huggingface.co/datasets/allenai/reward-bench-results .\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 114}\n",
            "the propensity to refuse (or not), the reasoning capabilities, and more for popular RMs.\n",
            "1Data is here: https://huggingface.co/datasets/allenai/reward-bench-results .\n",
            "24. Show the limitations of existing preference data test sets for evaluating these models, show-\n",
            "casing common pitfalls of RMs on subtle, but challenging instruction pairs (e.g. intentionally\n",
            "modified rejected responses, which superficially look high quality but answer the wrong prompt).\n",
            "We hope this benchmark enables more advanced reward model training, scientific understanding of\n",
            "the integration of human preferences in LMs, and ultimately better aligned, open language models.\n",
            "2 Related Works\n",
            "2.1 Reinforcement Learning from Human Feedback\n",
            "Using Reinforcement Learning to align language models with human feedback or preferences\n",
            "(Christiano et al., 2017; Ziegler et al., 2019) has led to improved chat models such as ChatGPT\n",
            "(Schulman et al., 2022) and Llama2 (Touvron et al., 2023). Incorporating human feedback into\n",
            "models in this way has been used to improve summarization (Stiennon et al., 2020; Wu et al., 2021),\n",
            "question answering (Nakano et al., 2021), image models (Lee et al., 2023) and instruction following\n",
            "in general (Ouyang et al., 2022).\n",
            "RLHF for alignment has been operationalized beyond general preference by comparing aspect-based\n",
            "preference, where aspects could be more general concepts like helpfulness orharmlessness (Bai\n",
            "et al., 2022a), or more fine-grained ones (Wu et al., 2023), among others. In general terms, RLHF\n",
            "involves training a reward model on preference data collected from crowdworkers (Wang et al.,\n",
            "2024) (or via using an LLM as a judge of responses, denoted RL from AI Feedback (Bai et al.,\n",
            "2022b)). Given a reward model, a policy can be learned using RL algorithms like PPO (Schulman\n",
            "et al., 2017), a method that has been shown to work well for language policies (Ramamurthy et al.,\n",
            "2022). Another option is to directly optimize a model with chosen and rejected pairs, using DPO\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 115}\n",
            "et al., 2017), a method that has been shown to work well for language policies (Ramamurthy et al.,\n",
            "2022). Another option is to directly optimize a model with chosen and rejected pairs, using DPO\n",
            "(Rafailov et al., 2023). Some reward modeling extensions include process reward models (Luo et al.,\n",
            "2023; Lightman et al., 2023) and step-wise reward models (Havrilla et al., 2024b), which are used\n",
            "for reasoning tasks to provide a correctness label for each of the steps in a reasoning chain.\n",
            "Despite RLHF’s impressive results, the approach has also been shown to lead to overoptimiza-\n",
            "tion (Gao et al., 2023) and divergence from the initial data distribution (Marks et al., 2024). Such\n",
            "reward hacking might be partially, but not fully, mitigated using RM ensembles (Coste et al.,\n",
            "2023; Eisenstein et al., 2023), weight averaging (Ram ´e et al., 2024), or constrained optimiza-\n",
            "tion (Moskovitz et al., 2023).\n",
            "2.2 Reward Model & RLHF Evaluation\n",
            "Preference tuned models can be evaluated using downstream evaluations, for example using the\n",
            "AlpacaFarm (Dubois et al., 2024) framework. In AlpacaFarm, LLMs are used to simulate human\n",
            "preferences by comparing a model generated output with the output from a reference model. The\n",
            "reported metric is the win-rate of the model over the reference model, which is being calculated over\n",
            "a set of 805 instructions. Similarly, MT-Bench (Zheng et al., 2023), evaluates chatbots on multi-turn\n",
            "conversations that are judged by LLMs as proxy for human judgments. Chatbot Arena (Zheng et al.,\n",
            "2023) is an evaluation benchmark that crowdsources the preferences between two different model\n",
            "outputs. These types of setups do not directly evaluate the reward model.\n",
            "Other works, on the other hand, analyze the reward model, such as Singhal et al. (2023), who look\n",
            "at the training dynamics of RMs. In their study they found a strong correlation between output\n",
            "length and rewards. Another analysis looked at reward inconsistencies, by creating a benchmark\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 116}\n",
            "at the training dynamics of RMs. In their study they found a strong correlation between output\n",
            "length and rewards. Another analysis looked at reward inconsistencies, by creating a benchmark\n",
            "of contrasting instructions (Shen et al., 2023). Most importantly they found that reward model\n",
            "inconsistency also affects the RLHF training and resulting RLHF’ed model.\n",
            "3 Background\n",
            "3.1 Reward Modeling\n",
            "The first step of training a reward model, and therefore doing RLHF, is collecting preference data\n",
            "from a group of human labelers. Individuals are presented with prompts ,x, akin to a question or\n",
            "task, and asked to choose between a set of completions ,yi, answering the request. The most common\n",
            "3Category Subset N Short Description\n",
            "Chat AlpacaEval Easy 100 GPT4-Turbo vs. Alpaca 7bB from Li et al. (2023b)\n",
            "358 total AlpacaEval Length 95 Llama 2 Chat 70B vs. Guanaco 13B completions\n",
            "AlpacaEval Hard 95 Tulu 2 DPO 70B vs. Davinici003 completions\n",
            "MT Bench Easy 28 MT Bench ratings 10s vs. 1s from Zheng et al. (2023)\n",
            "MT Bench Medium 40 MT Bench completions rated 9s vs. 2-5s\n",
            "Chat Hard MT Bench Hard 37 MT Bench completions rated 7-8s vs. 5-6\n",
            "456 total LLMBar Natural 100 LLMBar chat comparisons from Zeng et al. (2023)\n",
            "LLMBar Adver. Neighbor 134 LLMBar challenge comparisons via similar prompts\n",
            "LLMBar Adver. GPTInst 92 LLMBar comparisons via GPT4 similar prompts\n",
            "LLMBar Adver. GPTOut 47 LLMBar comparisons via GPT4 unhelpful response\n",
            "LLMBar Adver. Manual 46 LLMBar manually curated challenge completions\n",
            "Safety Refusals Dangerous 100 Preferring refusal to elicit dangerous responses\n",
            "740 total Refusals Offensive 100 Preferring refusal to elicit offensive responses\n",
            "XSTest Should Refuse 154 Prompts that should be refused R ¨ottger et al. (2023)\n",
            "XSTest Should Respond 250 Preferring responses to queries with trigger words\n",
            "Do Not Answer 136 Questions that LLMs should refuse (Wang et al., 2023)\n",
            "Reasoning PRM Math 447 Human vs. buggy LLM answers (Lightman et al., 2023)\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 117}\n",
            "Do Not Answer 136 Questions that LLMs should refuse (Wang et al., 2023)\n",
            "Reasoning PRM Math 447 Human vs. buggy LLM answers (Lightman et al., 2023)\n",
            "1431 total HumanEvalPack CPP 164 Correct CPP vs. buggy code (Muennighoff et al., 2023)\n",
            "HumanEvalPack Go 164 Correct Go code vs. buggy code\n",
            "HumanEvalPack Javascript 164 Correct Javascript code vs. buggy code\n",
            "HumanEvalPack Java 164 Correct Java code vs. buggy code\n",
            "HumanEvalPack Python 164 Correct Python code vs. buggy code\n",
            "HumanEvalPack Rust 164 Correct Rust code vs. buggy code\n",
            "Prior Sets Anthropic Helpful 6192 Helpful split from test set of Bai et al. (2022a)\n",
            "17.2k total Anthropic HHH 221 HHH validation data (Askell et al., 2021)\n",
            "SHP 1741 Partial test set from Ethayarajh et al. (2022)\n",
            "Summarize 9000 Test set from Stiennon et al. (2020)\n",
            "Table 1: Summary of the dataset used in R EWARD BENCH . Note: Adver. is short for Adverserial.\n",
            "case is for only two completions to be shown with measurement of preference, such as win-loss-tie\n",
            "or a Likert scale indicating the magnitude of preference between completions (Bai et al., 2022a),\n",
            "though other methods for labeling exist, such as ranking in a batch of 4 to 7 answers (Ouyang et al.,\n",
            "2022). The resulting data is transformed into a set of prompt-chosen-rejected trios, where the chosen\n",
            "completion is preferred over the rejected completion for training.\n",
            "Training a reward model involves training a classifier to predict the human preference probability,\n",
            "p∗, between two answers, as modeled by a Bradley-Terry model (Bradley and Terry, 1952):\n",
            "p∗(y1≻yx∣x)=exp(r∗(x, y 1))\n",
            "exp(r∗(x, y 1))+exp(r∗(x, y 2)). (1)\n",
            "Then, estimate the parameters of the reward model by optimizing the maximum likelihood loss as\n",
            "follows:\n",
            "L(θ,D)=E(x,y chosen,yrejected)∼D[log(1+erθ(x,y rejected)−rθ(x,y chosen))]. (2)\n",
            "For language models, the RM is often implemented by appending a linear layer to predict one logit\n",
            "or removing the final decoding layers and replacing them with a linear layer. At inference time,\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 118}\n",
            "For language models, the RM is often implemented by appending a linear layer to predict one logit\n",
            "or removing the final decoding layers and replacing them with a linear layer. At inference time,\n",
            "a trained reward model returns a scalar, such that P(y1≻y2∣x)∝er(x,y1)(which intuitively is\n",
            "the probability that the completion would be a preferred response, but is trained indirectly via the\n",
            "pairwise loss). Thus, a win between completions y1andy2is achieved when r(x, y 1)>r(x, y 2).\n",
            "3.2 Direct Preference Optimization\n",
            "Direct Preference Optimization solves the RLHF problem without needing to learn a separate reward\n",
            "model. It arranges an reward function from the model probabilities, directly optimizes the RM, and\n",
            "extracts a language model from it (Rafailov et al., 2023). The implicit reward used in DPO is a\n",
            "function of the policy model probabilities (i.e. the model being trained), π(y∣x), a regularization\n",
            "4Figure 1: The scoring method of the R EWARD BENCH evaluation suite. Each prompt is accompanied\n",
            "by a chosen and rejected completion which are independently rated by a reward model.\n",
            "constant, β, the base model probabilities, πref(y∣x), and a partition function Z(x):\n",
            "r(x, y)=βlogπ(y∣x)\n",
            "πref(y∣x)+βlogZ(x). (3)\n",
            "Given two completions to a prompt, we compare the rewards r(x, y 1)andr(x, y 2)as follows, where\n",
            "the score is computed via the log ratios of π:\n",
            "logπ(y1∣x)\n",
            "πref(y1∣x)>logπ(y2∣x)\n",
            "πref(y2∣x). (4)\n",
            "4 The R EWARD BENCH Benchmark\n",
            "In this section, we detail the design philosophy and construction of the evaluation dataset. The\n",
            "dataset is designed to provide a broad set of basic evaluations for reward models, covering chat, in-\n",
            "struction following, coding, safety, and other important metrics for fine-tuned language models. The\n",
            "REWARD BENCH dataset contains a combination of existing evaluation prompt-completion pairs,\n",
            "and those curated for this project.\n",
            "A good reward function, and therefore a good RM broadly, is one that stably assigns credit to the\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 119}\n",
            "and those curated for this project.\n",
            "A good reward function, and therefore a good RM broadly, is one that stably assigns credit to the\n",
            "classes of good or bad content.2Given one verified answer that is better than another for factual\n",
            "or clear qualitative reasons (e.g. typos), a good reward model will choose the correct one 100%\n",
            "of the time. To evaluate this, each datapoint consists of a prompt and two completions, chosen\n",
            "andrejected . For each prompt, the score of the reward model is computed. The prompt is then\n",
            "categorized as a win if the score of the prompt with the verified chosen completion is higher than that\n",
            "of the verified rejected completion, as shown in Fig. 1. Finally, we report accuracy for each subset as\n",
            "the percentage of wins. For all the section scores of R EWARD BENCH (e.g. Chat orSafety ) except\n",
            "Prior Sets , the average score is weighted per-prompt in the requisite subsets.\n",
            "4.1 R EWARD BENCH Dataset\n",
            "The benchmark is broken down into five sections from different subsets – the first four compose\n",
            "the R EWARD BENCH dataset described in this section. We have broken down the dataset into these\n",
            "subsections to create one final R EWARD BENCH score in order to reasonably weigh different aspects\n",
            "of an RM’s performance. The summary of the dataset is shown in Tab. 1 (see appendix B for full\n",
            "details) At a high level, the subsets consist of the following:\n",
            "1.Chat : Testing a reward model’s basic ability to distinguish a thorough and correct chat response\n",
            "in open-ended generation. Prompts and chosen, rejected pairs are selected from AlpacaEval (Li\n",
            "2There are more considerations on how to usea RM, but the initial notion of quality should be one that\n",
            "agrees with curated data. Next, we can evaluate which RMs are best for downstream tasks such as RLHF.\n",
            "5Reward Model Avg ChatChat\n",
            "Hard Safety ReasonPrior\n",
            "Sets\n",
            "berkeley-nest/Starling-RM-34B 81.5 96.9 59.0 89.9 90.3 71.4\n",
            "allenai/tulu-2-dpo-70b 77.0 97.5 60.8 85.1 88.9 52.8\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 120}\n",
            "5Reward Model Avg ChatChat\n",
            "Hard Safety ReasonPrior\n",
            "Sets\n",
            "berkeley-nest/Starling-RM-34B 81.5 96.9 59.0 89.9 90.3 71.4\n",
            "allenai/tulu-2-dpo-70b 77.0 97.5 60.8 85.1 88.9 52.8\n",
            "mistralai/Mixtral-8x7B-Instruct-v0.1 75.8 95.0 65.2 76.5 92.1 50.3\n",
            "berkeley-nest/Starling-RM-7B-alpha 74.7 98.0 43.5 88.6 74.6 68.6\n",
            "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO 73.9 91.6 62.3 81.7 81.2 52.7\n",
            "HuggingFaceH4/zephyr-7b-alpha 73.6 91.6 63.2 70.0 89.6 53.5\n",
            "NousResearch/Nous-Hermes-2-Mistral-7B-DPO 73.5 92.2 59.5 83.8 76.7 55.5\n",
            "allenai/tulu-2-dpo-13b 72.9 95.8 56.6 78.4 84.2 49.5\n",
            "openbmb/UltraRM-13b 71.3 96.1 55.2 45.8 81.9 77.2\n",
            "HuggingFaceH4/zephyr-7b-beta 70.7 95.3 62.6 54.1 89.6 52.2\n",
            "allenai/tulu-2-dpo-7b 70.4 97.5 54.6 74.3 78.1 47.7\n",
            "stabilityai/stablelm-zephyr-3b 70.1 86.3 58.2 74.0 81.3 50.7\n",
            "HuggingFaceH4/zephyr-7b-gemma-v0.1 66.6 95.8 51.5 55.1 79.0 51.7\n",
            "Qwen/Qwen1.5-72B-Chat 66.2 62.3 67.3 71.8 87.4 42.3\n",
            "allenai/OLMo-7B-Instruct 66.1 89.7 48.9 64.1 76.3 51.7\n",
            "IDEA-CCNL/Ziya-LLaMA-7B-Reward 66.0 88.0 41.3 62.5 73.7 64.6\n",
            "stabilityai/stablelm-2-zephyr-1 6b 65.9 96.6 46.6 60.0 77.4 48.7\n",
            "Qwen/Qwen1.5-14B-Chat 65.8 57.3 67.4 77.2 85.9 41.2\n",
            "Qwen/Qwen1.5-7B-Chat 65.6 53.6 69.8 75.3 86.4 42.9\n",
            "OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 65.1 88.5 47.8 62.1 61.4 65.8\n",
            "Random 50.0 50.0 50.0 50.0 50.0 50.0\n",
            "Table 2: Top-20 Leaderboard results in R EWARD BENCH . Evaluating many RMs shows that there is\n",
            "still large variance in RM training and potential for future improvement across the more challeng-\n",
            "ing instruction and reasoning tasks. Icons refer to model types: Sequence Classifier (\n",
            " ), Direct\n",
            "Preference Optimization (\n",
            " ), and a random model (\n",
            " ).\n",
            "et al., 2023b) and MT Bench (Zheng et al., 2023) completions, two popular open-ended chat\n",
            "evaluation tools.\n",
            "2.Chat Hard : Testing a reward model’s abilities to understand trick questions and subtly different\n",
            "instruction responses. Prompts and chosen, rejected pairs are selected from MT Bench exam-\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 121}\n",
            "2.Chat Hard : Testing a reward model’s abilities to understand trick questions and subtly different\n",
            "instruction responses. Prompts and chosen, rejected pairs are selected from MT Bench exam-\n",
            "ples with similar ratings and adversarial data specifically for fooling LLM-as-a-judge tools from\n",
            "LLMBar’s evaluation set (Zeng et al., 2023) (reformatted for RMs).\n",
            "3.Safety : Testing the models’ tendencies to refuse dangerous content and to avoid incorrect re-\n",
            "fusals to similar trigger words. Prompts and chosen, rejected pairs are selected from custom\n",
            "versions of the datasets XSTest (R ¨ottger et al., 2023), Do-Not-Answer (Wang et al., 2023), and\n",
            "examples from an in-development refusals dataset at AI2, where the chosen response is a refusal\n",
            "and the rejected is harmful text of either dangerous or offensive nature.\n",
            "4.Reasoning : Evaluating the models code and reasoning abilities. Code prompts are created by\n",
            "reformatting HumanEvalPack examples with correct code as chosen and rejected as one with\n",
            "bugs (Muennighoff et al., 2023). Reasoning prompts pair reference answers with incorrect model\n",
            "generations from the PRM800k dataset (Lightman et al., 2023).\n",
            "5.Prior Sets : For consistency with recent work on training reward models, we average perfor-\n",
            "mance over test sets from existing preference datasets. We use the Anthropic Helpful split (Bai\n",
            "et al., 2022a) (the only multi-turn data), the Anthropic HHH subset of BIG-Bench (Askell et al.,\n",
            "2021), a curated subset of the test set from the Stanford Human Preferences (SHP) Dataset (Etha-\n",
            "yarajh et al., 2022), and OpenAI’s Learning to Summarize Dataset (Stiennon et al., 2020).3\n",
            "3The dataset with more test sets and details is found here: https://huggingface.co/datasets/\n",
            "allenai/preference-test-sets\n",
            "6Reward Model Avg ChatChat\n",
            "Hard Safety Reason.Prior\n",
            "Sets\n",
            "allenai/tulu-2-dpo-70b 77.0 97.5 60.8 85.1 88.9 52.8\n",
            "allenai/tulu-2-dpo-13b 72.9 95.8 56.6 78.4 84.2 49.5\n",
            "allenai/tulu-2-dpo-7b 70.4 97.5 54.6 74.3 78.1 47.7\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 122}\n",
            "Hard Safety Reason.Prior\n",
            "Sets\n",
            "allenai/tulu-2-dpo-70b 77.0 97.5 60.8 85.1 88.9 52.8\n",
            "allenai/tulu-2-dpo-13b 72.9 95.8 56.6 78.4 84.2 49.5\n",
            "allenai/tulu-2-dpo-7b 70.4 97.5 54.6 74.3 78.1 47.7\n",
            "Qwen/Qwen1.5-72B-Chat 66.2 62.3 67.3 71.8 87.4 42.3\n",
            "Qwen/Qwen1.5-14B-Chat 65.8 57.3 67.4 77.2 85.9 41.2\n",
            "Qwen/Qwen1.5-7B-Chat 65.6 53.6 69.8 75.3 86.4 42.9\n",
            "Qwen/Qwen1.5-4B-Chat 52.2 38.8 59.4 63.4 54.6 44.7\n",
            "Qwen/Qwen1.5-1.8B-Chat 55.9 56.1 60.1 50.9 67.9 44.5\n",
            "Qwen/Qwen1.5-0.5B-Chat 52.8 35.5 59.7 70.1 52.5 46.3\n",
            "Table 3: R EWARD BENCH results for two model groups, T ¨ulu and Qwen-Chat, with a broad range\n",
            "of model sizes with fixed datasets, showcasing the scaling performance of DPO reward models.\n",
            "Scaling reward models, at least those trained with DPO, shows clear improvements in performance.\n",
            "4.2 R EWARD BENCH Scoring\n",
            "The primary scoring metric for R EWARD BENCH is accuracy. For each prompt-chosen-rejected trio,\n",
            "we infer the score the reward model assigns for the prompt-chosen and prompt-rejected pairs4then\n",
            "assign a true classification label when the chosen score is higher than rejected. This technique is\n",
            "highlighted in Fig. 1. More details on scoring, including for DPO models, is included in Sec. 3.\n",
            "Given the binary classification of correct or not, a random model achieves a result of 50 on our\n",
            "benchmark. On many subsets, models achieve at or well below the random baseline, indicating\n",
            "substantial areas of progress in reward models.\n",
            "In order to create a representative, single evaluation score, we perform a limited mixture of averaging\n",
            "across results. For all the subsets detailed in Sec. 4.1 except for Reasoning, we perform per-prompt\n",
            "weighted averaging across all the prompts in the subset to get the section score to normalize by\n",
            "the size of each category. For example, in Chat we take a weighted average of the AlpacaEval and\n",
            "MT Bench sets based on the number of prompts. For Reasoning, we increase the weight of the\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 123}\n",
            "the size of each category. For example, in Chat we take a weighted average of the AlpacaEval and\n",
            "MT Bench sets based on the number of prompts. For Reasoning, we increase the weight of the\n",
            "PRM-Math subset so code and math abilities are weighed equally in the final number, rather than\n",
            "increasing the relevance of code. For Prior Sets, we take an unweighted average over the subsets\n",
            "due to the large disparity in dataset sizes. Once all subsets weighted averages are achieved, the\n",
            "final R EWARD BENCH score is the average across the subset scores.\n",
            "5 Evaluation Results\n",
            "REWARD BENCH includes evaluation of many public reward models, ranging in parameter count\n",
            "from 400 million (PairRM) to 70 billion (T ¨ulu 2), trained as classifiers or with Direct Preference\n",
            "Optimization (when the reference model is available). In this section, we detail the core findings\n",
            "of R EWARD BENCH and more results are available in Appendix A. In particular, we study the state-\n",
            "of-the-art reward models (Tab. 2), results of similar-size models at 7B (Tab. 4), and a demonstration\n",
            "of the impact of scaling DPO reward models on performance in Tab. 3. We further study the limits\n",
            "of current reward models (Section 5.2) and prior test sets (Section 5.3).5\n",
            "5.1 Comparing State-of-the-art Reward Models\n",
            "Tab. 2 summarizes results for the top 20 models across different model sizes large, medium,\n",
            "and small . The large models are the only models capable of consistent high performance on\n",
            "the Chat Hard and Reasoning sections, with the model Starling-RM-34B (81.5) being state-of-\n",
            "the-art. These models are not accessible for many people to use, so we define two other cat-\n",
            "4For some reward models, such as PairRM and SteamSHP, their intended use is with pairwise inputs, so we\n",
            "evaluate in that manner following the original source code.\n",
            "5https://huggingface.co/datasets/allenai/reward-bench-results\n",
            "7Reward Model Avg ChatChat\n",
            "Hard Safety ReasonPrior\n",
            "Sets\n",
            "HuggingFaceH4/zephyr-7b-alpha 73.6 91.6 63.2 70.0 89.6 53.5\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 124}\n",
            "5https://huggingface.co/datasets/allenai/reward-bench-results\n",
            "7Reward Model Avg ChatChat\n",
            "Hard Safety ReasonPrior\n",
            "Sets\n",
            "HuggingFaceH4/zephyr-7b-alpha 73.6 91.6 63.2 70.0 89.6 53.5\n",
            "HuggingFaceH4/zephyr-7b-beta 70.7 95.3 62.6 54.1 89.6 52.2\n",
            "allenai/tulu-2-dpo-7b 70.4 97.5 54.6 74.3 78.1 47.7\n",
            "HuggingFaceH4/zephyr-7b-gemma-v0.1 66.6 95.8 51.5 55.1 79.0 51.7\n",
            "allenai/OLMo-7B-Instruct 66.1 89.7 48.9 64.1 76.3 51.7\n",
            "berkeley-nest/Starling-RM-7B-alpha 74.7 98.0 43.5 88.6 74.6 68.6\n",
            "NousResearch/Nous-Hermes-2-Mistral-7B-DPO 73.5 92.2 59.5 83.8 76.7 55.5\n",
            "IDEA-CCNL/Ziya-LLaMA-7B-Reward 66.0 88.0 41.3 62.5 73.7 64.6\n",
            "Qwen/Qwen1.5-7B-Chat 65.6 53.6 69.8 75.3 86.4 42.9\n",
            "OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1 64.0 94.4 36.6 59.4 70.0 59.4\n",
            "PKU-Alignment/beaver-7b-v1.0-reward 52.9 87.4 32.0 29.6 57.3 58.2\n",
            "Table 4: Comparing all 7B class models, with various base models, supervised fine-tuning, or prefer-\n",
            "ence optimization methods. Top is all of the Zephyr-style models (Tunstall et al., 2023), showcasing\n",
            "the potential of a strong fine-tuning recipe. Future work can involve ablating all the base models and\n",
            "fine-tuning recipes to find the best reward models Icons refer to model types: Sequence Classifier\n",
            "(\n",
            ") or Direct Preference Optimization (\n",
            " ).\n",
            "egories of state-of-the-art, 7 billion parameters and 1.5 billion parameters or less. The lead-\n",
            "ingmedium-sized 7B models are Starling-RM-7B-alpha (74.7), zephyr-7b-alpha (73.6),\n",
            "andNous-Hermes-2-Mistral-7B-DPO (73.5) given the similar scores and no formal notion\n",
            "of error bars on the benchmark. The final category is comprised of the small , most ac-\n",
            "cessible models, where the state-of-the-art models are stablelm-2-zepyhr-1 6b(65.9) and\n",
            "oasst-rm-2.1-pythia-1.4b-epoch-2.5 (65.1). There are striations in performance with\n",
            "changes in base model size and quality, mirroring the benchmark performance of models such as\n",
            "OLMo, Llama 2, Mistral 7B, Yi-34B, and others.\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 125}\n",
            "changes in base model size and quality, mirroring the benchmark performance of models such as\n",
            "OLMo, Llama 2, Mistral 7B, Yi-34B, and others.\n",
            "The Impacts of Different Base Models In our evaluation there are multiple models trained either\n",
            "with the same or very similar fine-tuning approaches on different base models. We show the impact\n",
            "of scaling across different Llama 2 and Qwen 1.5 versions in Tab. 3. In general, Llama 2 shows a\n",
            "clear improvement with scaling across all sections of R EWARD BENCH , but Qwen 1.5 shows less\n",
            "monotonic improvement (and even regression on Prior Sets ).\n",
            "Tab. 4 compares the impact of different base models and subtle changes of fine-tuning meth-\n",
            "ods via the Zephyr-class models (Tunstall et al., 2023). zephyr-7b-beta ,zephyr-7b-alpha ,\n",
            "zephyr-7b-gemma-v0.1 , and tulu-2-dpo-7b are all trained with the same target method and\n",
            "different base models or datasets. zephyr-7b-alpha andzephyr-7b-beta differ by filtering of\n",
            "the UltraFeedback preference dataset only, and this is reflected in zephyr-7b-alpha ’s higher score\n",
            "onSafety (as refusals were removed from the dataset) and lower score on Chat .tulu-2-dpo-7b\n",
            "shows the difference from the Mistral 7B to the Llama 2 7B base models and a different supervised\n",
            "fine-tuning dataset, as regressions on Chat Hard andReasoning , but improvements on Safety .\n",
            "zephyr-7b-gemma-v0.1 shows the regression when switching to Gemma base model across many\n",
            "categories.\n",
            "Different Shapes of Reward Functions The per-prompt scores demonstrate the different magni-\n",
            "tudes and distributions of rewards assigned to each reward model over the R EWARD BENCH eval-\n",
            "uation dataset. In Fig. 2 these distributions are shown for the reward models trained as classifiers\n",
            "we evaluated, with results for DPO models and on prior preference test sets in Appendix A.2. Only\n",
            "some reward models are Gaussian in their scores, only some reward models are centered around 0 re-\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 126}\n",
            "we evaluated, with results for DPO models and on prior preference test sets in Appendix A.2. Only\n",
            "some reward models are Gaussian in their scores, only some reward models are centered around 0 re-\n",
            "ward, and few are both. While much reward model research focuses on mitigating overoptimization,\n",
            "future work should identify a practical RM output distribution for downstream RL training.\n",
            "810\n",
            " 5\n",
            " 0 50200400600800Ziya-LLaMA-7B-Reward\n",
            "0 5 10hh_rlhf_rm_open_llama_3b\n",
            "20\n",
            " 0 20beaver-7b-v1.0-reward\n",
            "5\n",
            " 0 5Starling-RM-7B-alpha\n",
            "10.0\n",
            " 7.5\n",
            " 5.0\n",
            "0100200300400500Starling-RM-34B\n",
            "5\n",
            " 0 5reward-model-\n",
            "deberta-v3-large-v2\n",
            "0 10oasst-rm-2.1-pythia-1.4b\n",
            "-epoch-2.5\n",
            "10\n",
            " 0 10UltraRM-13bDensity\n",
            "Reward Model Score\n",
            "Chosen RejectedFigure 2: The distribution of rewards outputted by reward models for the chosen and rejected re-\n",
            "sponses in the R EWARD BENCH dataset. A large variety of model behaviors exist among open reward\n",
            "models. Some top scoring models, such as Starling and UltraRM show an increased margin between\n",
            "the mean of the chosen and rejected samples.\n",
            "MTBench LLMBar LLMBar Adversarial\n",
            "Reward Model Avg. Hard Natural Neighbor GPTInst GPTOut Manual\n",
            "Qwen/Qwen1.5-14B-Chat 70.2 67.6 71.0 83.6 62.0 46.8 71.7\n",
            "Qwen/Qwen1.5-7B-Chat 69.1 64.9 65.0 81.3 59.8 53.2 80.4\n",
            "mistralai/Mixtral-8x7B-Instruct-v0.1 64.0 75.7 77.0 67.9 41.3 55.3 69.6\n",
            "HuggingFaceH4/zephyr-7b-beta 62.7 83.8 83.0 70.9 27.2 51.1 60.9\n",
            "openbmb/UltraRM-13b 58.6 86.5 85.0 48.5 43.5 53.2 43.5\n",
            "allenai/tulu-2-dpo-13b 58.3 70.3 75.0 71.6 25.0 51.1 47.8\n",
            "berkeley-nest/Starling-RM-34B 57.2 91.9 91.0 31.3 39.1 76.6 47.8\n",
            "Random 50.0 50.0 50.0 50.0 50.0 50.0 50.0\n",
            "HuggingFaceH4/zephyr-7b-gemma-v0.1 49.6 83.8 74.0 44.0 17.4 53.2 45.7\n",
            "IDEA-CCNL/Ziya-LLaMA-7B-Reward 46.5 67.6 77.0 36.6 32.6 40.4 26.1\n",
            "berkeley-nest/Starling-RM-7B-alpha 45.8 78.4 80.0 31.3 23.9 48.9 28.3\n",
            "Table 5: Different categories of performance on the Chat Hard category, where only a few models\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 127}\n",
            "berkeley-nest/Starling-RM-7B-alpha 45.8 78.4 80.0 31.3 23.9 48.9 28.3\n",
            "Table 5: Different categories of performance on the Chat Hard category, where only a few models\n",
            "obtain strong results ( top).Middle shows where some of the top overall reward models land on\n",
            "the subset and bottom shows how some average-overall RMs struggling on this section (perform-\n",
            "ing worse than random). Icons refer to model types: Sequence Classifier (\n",
            " ), Direct Preference\n",
            "Optimization (\n",
            " ), and random (\n",
            " ).\n",
            "5.2 Limits of Current Reward Models\n",
            "A summary of performance is shown in Tab. 9. Current reward models can solve some subsets\n",
            "of R EWARD BENCH reliably, approaching 100% accuracy, but many subsets experience a combina-\n",
            "tion of low ceilings on performance or high variance of performance. The subsets with low ceilings,\n",
            "mostly in the Chat Hard andReasoning sections indicate areas where preference datasets and re-\n",
            "ward modeling methods can be extended to improve performance, and subsets with high variability,\n",
            "such as many of the Safety subsets, indicate areas where best practices can be converged upon.\n",
            "9Refusals XSTest ShouldDo Not\n",
            "Reward Model Avg. Dang. Offen. Refuse Respond Answer\n",
            "berkeley-nest/Starling-RM-34B 88.2 84.0 97.0 97.4 93.6 61.8\n",
            "allenai/tulu-2-dpo-70b 83.9 82.0 89.0 85.7 90.4 70.6\n",
            "NousResearch/Nous-Hermes-2-Mistral-7B-DPO 82.3 86.0 88.0 82.5 83.6 73.5\n",
            "Qwen/Qwen1.5-14B-Chat 76.3 93.0 83.0 80.5 41.6 90.4\n",
            "Qwen/Qwen1.5-7B-Chat 74.8 87.0 81.0 82.5 39.2 87.5\n",
            "Qwen/Qwen1.5-0.5B-Chat 66.1 76.0 91.0 87.0 16.8 58.1\n",
            "IDEA-CCNL/Ziya-LLaMA-7B-Reward 60.2 39.0 69.0 61.0 90.4 33.8\n",
            "openbmb/UltraRM-13b 54.3 18.0 21.0 66.2 94.8 37.5\n",
            "HuggingFaceH4/zephyr-7b-gemma-v0.1 52.9 25.0 61.0 51.3 92.4 25.7\n",
            "Table 6: A subset of R EWARD BENCH results for the Safety category grouped by behavior type.\n",
            "Top: Example reward models that correctly refuse sensitive prompts and do not refuse prompts with\n",
            "potential trigger words. Middle: Example reward models that refuse every request, including those\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 128}\n",
            "Top: Example reward models that correctly refuse sensitive prompts and do not refuse prompts with\n",
            "potential trigger words. Middle: Example reward models that refuse every request, including those\n",
            "that they should respond to. Bottom: Example reward models that respond to every request, even\n",
            "those they should refuse. Icons refer to model types: Sequence Classifier (\n",
            " ) and Direct Preference\n",
            "Optimization (\n",
            " ).\n",
            "Evaluating across Chat Hard Categories Tab. 5 compares different rewards models across Chat\n",
            "Hard categories (full results are shown in Tab. 9). The adversarial subsets from LLMBar are crucial\n",
            "to understanding RMs because they show examples where two answers are written in a similar style\n",
            "(e.g. the same GPT-4 model version), but with slightly different subjects. The difference between\n",
            "asking a factual question about a related but different object or slightly changing the context of a\n",
            "prompt, is hard to pick up with most reward models. The Chat Hard section (and to some extent\n",
            "Reasoning ) is the mirror of the Prior Sets section, where the hard prompts are dominated by\n",
            "DPO models – even those with low average performance overall, such as the Qwen Chat Models.\n",
            "The performance gain of DPO models can be caused by many aspects, ranging from better base\n",
            "models to more aligned training datasets, but closing the gap with standard reward models trained\n",
            "as classifiers is an important step.\n",
            "Evaluating across Reasoning Categories TheReasoning section of R EWARD BENCH has the\n",
            "widest, smooth variation in performance – e.g. models populate many levels, from 35% accuracy\n",
            "(well below random) all the way to 90% accuracy. Though, the ceiling on reasoning models is much\n",
            "harder than the adversarially designed data, indicating RMs can reliably identify known bugs in\n",
            "reasoning or code. Full reasoning results are included in Tab. 11.\n",
            "Evaluating across Safety Metrics Tab. 6 (full results in Tab. 10 in Appendix) compares dif-\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 129}\n",
            "reasoning or code. Full reasoning results are included in Tab. 11.\n",
            "Evaluating across Safety Metrics Tab. 6 (full results in Tab. 10 in Appendix) compares dif-\n",
            "ferent reward models across different safety categories, indicating challenges on striking a bal-\n",
            "ance between refusing too much or not refusing. Models, such as zephyr-7b-beta and\n",
            "zephyr-7b-gemma-v0.1 show how a model focused on helpfulness without a strong notion of\n",
            "safety will score poorly on the should-refuse subsets of the safety section, but highly on XSTest\n",
            "Should Respond . Other models, namely those at the top of the overall leaderboard, clearly in-\n",
            "clude safety information in the training process andmaintain strong performance on trick questions\n",
            "that could induce false refusals ( XSTest Should Respond ). Finally, the third option is also repre-\n",
            "sented in models – those that score highly on prompts that they should refuse and poorly on those\n",
            "they should not, indicating a model that is likely to falsely refusal queries (for example, the Qwen\n",
            "chat models). These three behavior modes being represented indicates that R EWARD BENCH can\n",
            "be used as a quick check of the safety behavior of a candidate model, especially when trained with\n",
            "DPO (as it will not need further RL training like the classifier models).\n",
            "Evaluating Length Bias Given the results showing length bias in RLHF and reward models (Sing-\n",
            "hal et al., 2023), we designed R EWARD BENCH so that the chosen responses are either a similar\n",
            "length or shorter than the rejected responses. For example, the AlpacaEval Length subset is de-\n",
            "signed to differentiate between other Chat subsets by having notably different models capabilities\n",
            "with the same average length (results in Tab. 8). In this case, the results are lower than other easy\n",
            "10chat subsets, but 90% plus accuracy is achieved by over 10 models – far above random for most\n",
            "models. Though, more detailed statistical tests are needed to fully understand this, as this only tests\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 130}\n",
            "10chat subsets, but 90% plus accuracy is achieved by over 10 models – far above random for most\n",
            "models. Though, more detailed statistical tests are needed to fully understand this, as this only tests\n",
            "the reward models’ abilities to discern information without the help of length as a proxy. More\n",
            "details on the length distributions of R EWARD BENCH are found in Appendix D.2.\n",
            "5.3 Limitations of Prior Test Sets\n",
            "Many popular models trained with RLHF use new preference datasets such as UltraFeedback (Cui\n",
            "et al., 2023) or Nectar (Zhu et al., 2023a), which don’t have publicly available validation sets. Given\n",
            "this, when training reward models, common practice is to compare model agreement with a variety\n",
            "of existing test sets from earlier work in RLHF.\n",
            "Some models scoring strongly on the Prior Sets section of R EWARD BENCH , such as\n",
            "UltraRM-13b andPairRM-hf were trained on the training splits of Anthropic HH, Stanford Human\n",
            "Preferences (SHP), and OpenAI’s Learning to Summarize, but other top classifier models, such as\n",
            "the Starling models were not. Combining this with the very low average score of DPO models on\n",
            "these test sets indicates that substantial research is needed to understand the full limitations of these\n",
            "datasets. Full results are detailed in Tab. 12.\n",
            "Additional data is included in the code-base, but not included in the evaluation score due to noisy\n",
            "results or lack of clear use instructions (e.g. could be easy for unintentional test-set contamination).\n",
            "In this vein, results on SafeRLHF (Dai et al., 2023) data and MT Bench labels6(from humans and\n",
            "GPT-4) are supported within the methodology, but not included in this analysis.\n",
            "6 Discussions\n",
            "DPO Models vs Classifiers Since DPO-trained LLMs are implicit reward models largely used for\n",
            "their generative abilities, the question of how they compare to RMs trained as classifiers is unstudied.\n",
            "There are currently more DPO models released to the public, partially due to DPO requiring notably\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 131}\n",
            "There are currently more DPO models released to the public, partially due to DPO requiring notably\n",
            "fewer computational resources among other factors such as existing implementations and relevant\n",
            "datasets. We see that the results on R EWARD BENCH flatter the recent DPO methods, except for the\n",
            "Prior Sets section. For how the DPO reward is computed, see Sec. 3.\n",
            "The same inference code of popular DPO training implementations can easily be used for evaluation\n",
            "as an RM by not propagating gradients through the models. The simplest implementations requires\n",
            "more GPU memory to run evaluation of DPO-trained models given the two models needed to com-\n",
            "pute the reward, but this can be avoided by computing the probabilities over the policy and base\n",
            "models sequentially. Though, some of the released DPO models do not clearly document which\n",
            "reference model is used in training (e.g. if it is a base model or a model obtained via supervised\n",
            "fine-tuning), which can result in unclear benchmarking.7When a reference model is unavailable or\n",
            "compute is constrained, an alternative approach in such cases would be to obtain a reference free\n",
            "reward: π(y1∣x)>π(y2∣x), which could be normalized using different approaches. Without nor-\n",
            "malization, the loss has a length penalty by summing over probabilities of each token which are all\n",
            "negative numbers. We will explore the impacts of reference free inference in future work.\n",
            "We also experimentedwith using the “wrong” reference model, i.e. a similar but different base\n",
            "model, and found that this reduced the DPO trained RM performance to similar levels as the random\n",
            "baseline.\n",
            "There is still a lot that is unknown about the best practices of training RMs: trained with DPO they\n",
            "are regularized by KL distance, but the classifiers are not. Additionally, a common practice for\n",
            "training RMs via classification is to train for 1 epoch (Ouyang et al., 2022), while DPO models are\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 132}\n",
            "are regularized by KL distance, but the classifiers are not. Additionally, a common practice for\n",
            "training RMs via classification is to train for 1 epoch (Ouyang et al., 2022), while DPO models are\n",
            "usually trained for more than 1 epoch (Tunstall et al., 2023; Ivison et al., 2023). Other future work\n",
            "ideas therefore include analyzing the role of the training hyperparameters in DPO training and RM\n",
            "classification performance (such as Beta KL regularization on generated text, number of training\n",
            "epochs, etc.).\n",
            "6https://huggingface.co/datasets/lmsys/mt_bench_human_judgments\n",
            "7Examples include Mixtral-8x7B-Instruct-v0.1 or the Qwen chat models, which just say “trained\n",
            "with DPO,” yet they achieve solid performance.\n",
            "11Generative Reward Modeling Given LLM-as-a-judge’s prevalent use for evaluation, recent\n",
            "works have emerged using LLMs as feedback mechanisms very similar to reward models. Some\n",
            "works have fine-tuned models specifically for the task of rating or choosing responses from\n",
            "LLMs (Jiang et al., 2023b; Kim et al., 2023; Zhu et al., 2023b). Other work has proposed gen-\n",
            "erative reward modeling (Li et al., 2023a)– using a generative language model to provide scores\n",
            "via output tokens. While similar to the reward computation of DPO models, this mode of score\n",
            "calculation often involves specific prompting per-model and more computation per sample, such as\n",
            "explaining reasoning before or after the score. Given these differences, we decided not to include\n",
            "them in the R EWARD BENCH leaderboard, but they are worth exploring in future work.\n",
            "Values Represented in Reward Models Reward models inhabit an important normative role in\n",
            "the RLHF process being the primary artifact where human preferences or values are encoded in\n",
            "the final policy. The R EWARD BENCH infrastructure enables asking basic questions when study-\n",
            "ing reward models such as whose orwhich values are embedded as the sense of reward (Lambert\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 133}\n",
            "the final policy. The R EWARD BENCH infrastructure enables asking basic questions when study-\n",
            "ing reward models such as whose orwhich values are embedded as the sense of reward (Lambert\n",
            "et al., 2023). Initial work is studying this question for LLMs broadly, such as measuring represen-\n",
            "tation (Durmus et al., 2023; Ryan et al., 2024) or moral foundations of LMs (Abdulhai et al., 2023),\n",
            "but this work should be extended to reward models. This can involve the study of different base\n",
            "models which RMs are trained from, tweaking fine-tuning techniques, if synthetic datasets amplify\n",
            "bias in RMs as well (Wyllie et al., 2024), and datasets.\n",
            "Safety In or After RLHF An emerging trend in LLMs is the shift from chat systems being\n",
            "only a model to being a system of models, with small models used as classifiers for tasks such\n",
            "as safety (Mozes et al., 2023). If some LLMs or RMs are designed to be used with additional safety\n",
            "classifiers after the fact, evaluating them on R EWARD BENCH may not be a fair comparison. For sys-\n",
            "tems such as this, each classifier for a specific task should be evaluated on the sections it controls.\n",
            "The most common area where this is handled is safety, where a small reward model can be used to\n",
            "permit or block all outputs from a larger generating model.\n",
            "7 Conclusion\n",
            "We present R EWARD BENCH , and show the variety of performance characteristics of current reward\n",
            "models in order to improve understanding of the RLHF process. While covering a wide variety\n",
            "of topics important to alignment of language models, a crucial next step is needed to correlate\n",
            "performance in R EWARD BENCH to downstream performance of a model trained with RLHF. We\n",
            "have taken a first step to understanding which values are embedded in the RLHF training and data,\n",
            "showing trends across many base models and preference datasets. The toolkit we have released can\n",
            "easily be expanded include new custom dataset to specifically audit a certain property of the RLHF\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 134}\n",
            "showing trends across many base models and preference datasets. The toolkit we have released can\n",
            "easily be expanded include new custom dataset to specifically audit a certain property of the RLHF\n",
            "process. R EWARD BENCH is one of many tools which will help us understand the science of whose\n",
            "and what values are embedded in our language models.\n",
            "Acknowledgements\n",
            "The authors would like to thank Thomas Gilbert for early discussions that helped motivate this\n",
            "project. Thanks to Prasann Singhal for discussing similar and complimentary concurrent work when\n",
            "building this project. Thanks to Hamish Ivision for helping with the math data filtering code. Thanks\n",
            "to Matt Latzke for help with the logo and design artifacts.\n",
            "{'filename': '2403.13787_v1.pdf', 'title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'vs_index': 135}\n",
            "SequenceMatch: Imitation Learning for\n",
            "Autoregressive Sequence Modelling with Backtracking\n",
            "Chris Cundy1Stefano Ermon1\n",
            "1Department of Computer Science, Stanford University\n",
            "{cundy, ermon }@cs.stanford.edu\n",
            "Abstract\n",
            "In many domains, autoregressive models can attain high likelihood on the task\n",
            "of predicting the next observation. However, this maximum-likelihood (MLE)\n",
            "objective does not necessarily match a downstream use-case of autoregressively\n",
            "generating high-quality sequences. The MLE objective weights sequences propor-\n",
            "tionally to their frequency under the data distribution, with no guidance for the\n",
            "model’s behaviour out of distribution (OOD): leading to compounding error during\n",
            "autoregressive generation. In order to address this compounding error problem, we\n",
            "formulate sequence generation as an imitation learning (IL) problem. This allows\n",
            "us to minimize a variety of divergences between the distribution of sequences\n",
            "generated by an autoregressive model and sequences from a dataset, including\n",
            "divergences with weight on OOD generated sequences. The IL framework also\n",
            "allows us to incorporate backtracking by introducing a backspace action into\n",
            "the generation process. This further mitigates the compounding error problem by\n",
            "allowing the model to revert a sampled token if it takes the sequence OOD. Our\n",
            "resulting method, SequenceMatch, can be implemented without adversarial training\n",
            "or architectural changes. We identify the SequenceMatch- χ2divergence as a more\n",
            "suitable training objective for autoregressive models which are used for generation.\n",
            "We show that empirically, SequenceMatch training leads to improvements over\n",
            "MLE on text generation with language models.\n",
            "1 Introduction\n",
            "Autoregressive models such as the GPT series of causally masked transformers [ 7,25] are able to\n",
            "perform a variety of downstream tasks such as question-answering, translation, and summarization,\n",
            "after simply training on a large corpus of text with the objective of predicting the next token given\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 136}\n",
            "perform a variety of downstream tasks such as question-answering, translation, and summarization,\n",
            "after simply training on a large corpus of text with the objective of predicting the next token given\n",
            "the previous tokens. However, autoregressive language models suffer from a variety of pathological\n",
            "behavior when deployed on the task of free-form text generation [ 15,37], particularly at lower\n",
            "generation temperatures or with smaller models. These include generating the same token or series\n",
            "of token repeatedly, or generating gibberish outputs. This phenomenon of degeneration for longer\n",
            "continuations has also been observed in autoregressive models for video [ 36]. This problem of neural\n",
            "text degeneration has been linked to the training objective for LLMs, which trains a conditional\n",
            "distribution for the next token given a (partial) sentence [ 10]. When deployed in an autoregressive\n",
            "fashion, the model has its own outputs as inputs, resulting in a compounding error problem that\n",
            "rapidly takes the model out of distribution (OOD). This compounding error problem is also a key\n",
            "issue in the imitation learning subfield of reinforcement learning, where the goal is to learn a\n",
            "policy (a distribution over next actions given the past) which results in trajectories similar to a set\n",
            "of provided expert trajectories. The approach of directly matching the expert’s actions leads to\n",
            "a compounding error [ 27], which has led to several works proposing to address this problem by\n",
            "minimizing alternative divergences [ 3,28]. These alternative divergences encourage the policy to\n",
            "return to expert states if the generated trajectory starts to diverge from them. We argue that two key\n",
            "Preprint. Under review.arXiv:2306.05426v2  [cs.LG]  19 Jun 2023Figure 1: A toy model of an autoregressive generation problem, such as language modelling. Our task\n",
            "is to learn a set of conditional distributions that continue the sequence similarly to those sequences in\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 137}\n",
            "is to learn a set of conditional distributions that continue the sequence similarly to those sequences in\n",
            "the dataset (green arrows), and avoid incorrect next tokens (red arrows). Our method trains against\n",
            "divergences that more heavily punish out-of-distribution sequences. We additionally introduce a\n",
            "<backspace> action which can backtrack from an erroneous token (dashed purple arrows).\n",
            "issues can prevent autoregressive models trained with maximum-likelihood from generating fluent\n",
            "sequences at evaluation time. First is the divergence measure used to evaluate the difference between\n",
            "the model and the data distribution. Because the MLE loss does not have any contribution from OOD\n",
            "sequences, the behavior of the model on OOD sequences (such as those generated autoregressively)\n",
            "is not constrained. We address this by minimizing the χ2-divergence between a mixture of the data\n",
            "and autoregressively generated sequences. This divergence is known to perform much better than\n",
            "MLE in imitiation learning [11, 1].\n",
            "Secondly, if a model generates an OOD token, there may be no natural continuation which is similar to\n",
            "sequences from the data distribution, and so the model may be unable to return to the data distribution\n",
            "even if our χ2-divergence encourages this. To address this, we augment the generation process\n",
            "with a <backspace> action, which deletes the previous token, and allows the model to correct for\n",
            "erroneous generations. By incorporating recent work in non-adversarial imitation learning [ 11],\n",
            "our method, SequenceMatch , is able to train autoregressive models against alternative divergences\n",
            "such as the χ2-mixture divergence while augmenting the policy with a <backspace> action. The\n",
            "SequenceMatch loss is a fully supervised loss without adversarial training, and can be applied on top\n",
            "of pretrained models as a finetuning step. To summarize our contributions:\n",
            "•We formulate the sequence generation problem as an imitation learning (IL) problem, and\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 138}\n",
            "of pretrained models as a finetuning step. To summarize our contributions:\n",
            "•We formulate the sequence generation problem as an imitation learning (IL) problem, and\n",
            "formulate a general non-adverarial objective for minimizing divergences between occupancy\n",
            "measures based on [ 11], handling (among others) the forward and reverse KL-divergence,\n",
            "JS-divergence, and χ2divergence.\n",
            "•We develop a novel masking scheme allowing training of a transformer-based autoregressive\n",
            "model with a <backspace> action with no additional overhead vs MLE.\n",
            "•Finally we evaluate the empirical performance of SequenceMatch-trained models, showing\n",
            "improved performance over the maximum likelihood objective in general text generation.\n",
            "2 Preliminaries: Training Objectives for Autoregressive Models\n",
            "2.1 KL-Divergence\n",
            "Typically, autoregressive models are trained against a maximum-likelihood objective. This objective\n",
            "can be motivated by treating our dataset as consisting of sequences of random variables (x1, . . . , x N),\n",
            "with a corresponding probability distribution Pdata(x1, . . . , x N), with a fixed length N. The goal is\n",
            "to learn a parameterized model Pθ(x1, . . . , x N)that is close to Pdata. The KL-divergence between\n",
            "2the data distribution and the model has a useful decomposition:\n",
            "DKL(Pdata∥Pθ) =Ex1:N∼Pdata[logPdata(x1:N)−logPθ(x1:N)] (1)\n",
            "=−Ex1:N∼Pdata\"NX\n",
            "ilogPθ(xi|x<i)#\n",
            "+C, (2)\n",
            "where Cis a constant that does not depend on θ. For a dataset D={xj\n",
            "1:N}Ndata\n",
            "j=0of se-\n",
            "quences drawn i.i.d. from Pdata, this can be approximated with an estimator ˆDKL(Pdata∥Pθ) =\n",
            "1\n",
            "NdataP\n",
            "jPN\n",
            "ilogPθ(xj\n",
            "i|xj\n",
            "<i) +C′. Hence, minimizing the KL-divergence is equivalent to maximiz-\n",
            "ing the model’s (log-) likelihood of the next element in the sequence given the previous elements.\n",
            "This formulation is statistically well-motivated and particularly convenient for architectures such as\n",
            "the transformer [ 35], where all conditionals over a sequence can be computed in parallel. The density\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 139}\n",
            "the transformer [ 35], where all conditionals over a sequence can be computed in parallel. The density\n",
            "under the data distribution Pdatais not required, which is convenient as it is typically unknown. In\n",
            "some domains, the length of the sequences njdiffers in each example j, which can be incorporated\n",
            "by choosing an effective length N= max ni, and treating all sequences shorter than Nas having\n",
            "a sequence of padding tokens appended1. In the sequel with some abuse of notation we will write\n",
            "Pdata(sn)forPn\n",
            "ilogPθ(xi|x<i), for partial sequences that may not terminate until after n, and\n",
            "Pdata(x|sn)to signify the conditional probability of the next token given a partial sequence.\n",
            "2.1.1 Limitations of the KL-divergence\n",
            "However, while it is clear that minimizing the KL-divergence in equation (1)will result in Pθ=Pdata\n",
            "(for a sufficiently flexible parameterization Pθ), it is not obvious what the behaviour is of models\n",
            "which approximately minimize the KL-divergence. In figure 1, a chain distribution is shown, with\n",
            "sequences of length N. The model Pθhas an ϵerror on each conditional, where an error leads to an\n",
            "OOD sequence which has no support under the data distribution. This leads to an error in the KL\n",
            "metric of order nϵ. However, the probability of getting to the end of the chain before an incorrect\n",
            "token is picked is 1−(1−ϵ)n, and so the value of the KL-divergence is not a good metric if our\n",
            "main quantity of interest is how often generated sequences are in-distribution. Furthermore, the\n",
            "KL-divergence weights the loss by the frequency under the data distribution, so if sequences are rarely\n",
            "seen under the data distribution they are not very important in the total loss. One particular implication\n",
            "is that the model’s behavior out-of-distribution from the data is essentially undetermined. In other\n",
            "words, there is no optimization pressure on the model to learn how to ‘recover’ from mistakes.\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 140}\n",
            "is that the model’s behavior out-of-distribution from the data is essentially undetermined. In other\n",
            "words, there is no optimization pressure on the model to learn how to ‘recover’ from mistakes.\n",
            "In non-autoregressive generative modelling, a wide variety of different divergences are commonly\n",
            "used, such as the Wasserstein distance [ 2] and Fisher divergence [ 30]. Particularly interesting is the\n",
            "χ2divergence Dχ2(Pθ, Pdata) =Ex∼Pdata\u0002\n",
            "(Pθ(x)/Pdata(x)−1)2\u0003\n",
            ". Indeed we see in figure 1 that\n",
            "theχ2-divergence in this case is equal to the squared probability of staying in the data distribution\n",
            "of sequences. We can further penalize out-of-distribution behavior by considering the divergence\n",
            "between mixtures Dχ2(Pθ,(Pdata+Pθ)/2), as we do in our practical algorithm. However, it is\n",
            "generally difficult in practice to compute any divergence involving the density of the data, which\n",
            "must be substituted for with an approximation from a discriminator.\n",
            "In the field of reinforcement learning, several methods have been discovered which can minimize\n",
            "divergences such as Wasserstein and JS between the distribution of trajectories from an expert and\n",
            "a learned policy. The approaches are non-adversarial, even though the expert density is not known\n",
            "[11,32,4,1]. A key feature of these methods is that they operate on occupancy measures instead of\n",
            "joint distributions, a concept which we introduce in the next section 3.\n",
            "3 Method\n",
            "3.1 Sequence Modelling as a Markov Decision Process\n",
            "We consider a sequence model represented as a Markov decision process (MDP), defined by a tuple\n",
            "(S,A,P, r, γ).S,Arepresent state and action spaces, P(s′|s, a)represents the dynamics, r(s, a)\n",
            "represents the reward function, and γ∈(0,1)represents the discount factor. In our case, the state\n",
            "1Some care is required here, as each averaging the loss of each example over its length leads to an inconsistent\n",
            "estimator.\n",
            "3spaceSis the set of all sequences (of all lengths) with elements in a finite set X(the vocabulary plus\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 141}\n",
            "estimator.\n",
            "3spaceSis the set of all sequences (of all lengths) with elements in a finite set X(the vocabulary plus\n",
            "a<begin-sentence> token if not already present in the vocabulary). The set of actions Ais a finite\n",
            "set. For concreteness, we can assume that X⊆ A (i.e. we have an insert-token action for each\n",
            "token), as well as additional editing actions ( <backspace> , etc). In our case, we will assume that the\n",
            "initial state is deterministically given by a special <begin-sentence> token, while the dynamics for\n",
            "aninsert-token action in a state (sequence) sleads deterministically to the sequence s′consisting\n",
            "ofswith the given token appended to the end.\n",
            "Combined with a policy pθ(a|s), the MDP defines a distribution over (possibly infinite-length)\n",
            "sequences, following the generative process of sampling an action a∼pθ(·|s), then sampling\n",
            "the next state s′∼ P(s′|s, a), etc. Finally, we assume that a special end of sequence token\n",
            "<end-of-sentence> induces a terminal state: in any state swith <end-of-sentence> as the\n",
            "final element, all actions cause a self-transition to swith probability 1. We note that this proba-\n",
            "bilistic process reduces exactly to the autoregressive formulation of sequence modelling when the\n",
            "action set is the same as the vocabulary, but is different when editing actions are introduced. We\n",
            "incorporate a single editing action, <backspace> . A backspace action in a state sdeterministically\n",
            "transitions to a state s′with the final token in the sequence sremoved.2An example of the states\n",
            "and actions can be seen in figure 2. As an additional example, the actions [a; <backspace>] have\n",
            "corresponding states [<begin-sentence>; <begin-sentence> a; <begin-sentence>] . The\n",
            "MDP framework formalizes the intuitive picture in figure 1: the task of language modelling can be\n",
            "viewed as the traversal of a tree where the nodes are (partial) sequences.\n",
            "A central quantity of interest is the occupancy measure. We denote by stthe random variable\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 142}\n",
            "viewed as the traversal of a tree where the nodes are (partial) sequences.\n",
            "A central quantity of interest is the occupancy measure. We denote by stthe random variable\n",
            "consisting of the state at time tunder a policy p(a|s)and the MDP defined above. Then, the\n",
            "occupancy measure ρ(s, a) :S × A → [0,1]is the (discounted) probability of observing a particular\n",
            "sentence sat time tand taking action agiven that sentence:\n",
            "ρ(s, a) = (1 −γ)p(a|s)X\n",
            "tγtP(st=s) (3)\n",
            "In other words, the occupancy measure is proportional to the observed frequency of a particular\n",
            "(sentence, next-action) pair occurring, with occurrances discounted in time by a factor of γ. In the\n",
            "absence of editing actions, A=Xand the occupancy measure is a discounted probability over\n",
            "(partial) sequences: for a sequence snof length n,ρdata(sn, x) = (1 −γ)γnPdata(s′), where s′is\n",
            "the sequence obtained by appending xtos. Given editing actions which can reduce the length of a\n",
            "sequence, the occupancy measure becomes more complicated, as the same sequence can occur at\n",
            "multiple times. For instance, if a sequence has length nat time nand the <backspace> action is used,\n",
            "the sequence at time n+1will have length n−1. We note that the occupancy measure is a normalized\n",
            "probability distribution, even if editing actions are included. For a function r, the expectation with\n",
            "respect to ρhas the usual meaning: E(s,a)∼ρh\n",
            "r(s, a) =P\n",
            "S,Aρ(s, a)r(s, a)i\n",
            ", where the sum is\n",
            "over the discrete action space and (countably infinite) state space. Occupancy measures provide\n",
            "an alternative way of modelling sequences, allowing us to impose a measure over all sequences,\n",
            "even in the presence of editing actions. As such, we avoid the somewhat artificial treatment with\n",
            "a maximum length cut-off discussed in section 2.1, and can handle infinite sequences in a natural\n",
            "manner. Furthermore, the next section illustrates that we can non-adversarially minimize a large\n",
            "variety of divergences between occupancy measures, compared to only the KL divergence in the\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 143}\n",
            "manner. Furthermore, the next section illustrates that we can non-adversarially minimize a large\n",
            "variety of divergences between occupancy measures, compared to only the KL divergence in the\n",
            "typical joint probability formulation.\n",
            "3.2 Minimizing Occupancy Divergences\n",
            "Our aim is to learn a policy pθ(a|s)which induces an occupancy measure pθsuch that it is close to\n",
            "the data occupancy measure pdata. We define the data occupancy measure by forming the data policy\n",
            "pdata(a|s)corresponding to the conditionals Pdata(x|sn)and setting the probability of editing actions\n",
            "to zero. It is known that matching occupancy measures implies matching policies: if ρθ=ρdatafor a\n",
            "valid occupancy ρθ, then the corresponding pθ(a|s) =Pdata(a|s)[33]. Therefore, it is reasonable to\n",
            "minimize divergences between occupancy measures. We extend the derivations in Garg et al. [11] to\n",
            "the case with infinite-dimensional state space. We consider distances between occupancy divergences\n",
            "2In the case of s=<begin-sentence> and<backspace> is used, s′=<begin-sentence> also\n",
            "4parameterized by the following form:\n",
            "dψ(ρθ, ρdata) = sup\n",
            "r∈RE(s,a)∼ρθ[r(s, a)]−E(s,a)∼ρdata[r(s, a)]−ψ(r),\n",
            "where ψis a convex regularizer. The critic rpicks out any differences between the occupancies,\n",
            "while if ρθ=ρdata, the difference in expectations will be zero for any r. This family of divergences\n",
            "might seem restrictive, but show in the appendix that it includes all f-divergences such as the KL and\n",
            "JS-divergence, as well as the Wasserstein distance and MMD. The divergence minimization problem\n",
            "can be made tractable by adding an entropy term:\n",
            "inf\n",
            "θdψ(ρθ, ρdata)−αH[ρθ], (4)\n",
            "with the entropy H[ρθ] =−E(s,a)∼logρθ[logρθ(s, a)], and αa chosen regularization strength. Sub-\n",
            "stituting in the definition of dψ, we obtain the min-max problem infρθsuprL(θ, r) = inf ρθsuprr·\n",
            "(ρθ−ρdata)−ψ(r)−αH[ρθ]. We prove in the appendix that the saddle-point property in [ 14] extends\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 144}\n",
            "stituting in the definition of dψ, we obtain the min-max problem infρθsuprL(θ, r) = inf ρθsuprr·\n",
            "(ρθ−ρdata)−ψ(r)−αH[ρθ]. We prove in the appendix that the saddle-point property in [ 14] extends\n",
            "to our infinite-dimensional case, so infρθsuprL(θ, r) = suprinfρθL(θ, r). We can interpret the\n",
            "outer maximization as finding a critic [ 20]rfor sequences and actions s, asuch that the model has\n",
            "high values on examples from the dataset and low values on the examples from the learned model:\n",
            "i.e. it is a discriminator. The inner minimization over θis an entropy-regularized minimization of the\n",
            "KL-divergence between ρθandr. If we were to approach this directly by explicitly learning randρθ,\n",
            "we would have an objective similar to a GAN [ 12]. This is known to be difficult to train [ 16]. Instead,\n",
            "we are able to solve the problem with optimization over a single variable by a transformation of\n",
            "variables. In the following section, we recover an objective Jwhich is equivalent to the objective in\n",
            "equation (4), but only involves optimization over the logits of a policy. We will set α= 1in the next\n",
            "section, with the understanding that we can recover the original objective by scaling randψby1/α.\n",
            "3.2.1 Reformulating the Occupancy Divergence Minimization Problem\n",
            "We first introduce the Q-function, corresponding to the discounted rewards obtained in state s\n",
            "by taking action a. Formally we define it as the unique fixed point of the soft Bellman oper-\n",
            "atorBθ\n",
            "r, where Bθ\n",
            "rQ(s, a) = r(s, a) +γEs′∼P(s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            ", for the value function Vθ(s) =\n",
            "Ea∼pθ(·|s)[Q(s, a)−logpθ(a|s)]. The inverse Bellman operator Tθis the inverse of this operator,\n",
            "given by (TθQ)(s, a) =Q(s, a)−γEs′∼P(s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            ". For a fixed policy θ, there is a one-to-one\n",
            "correspondence between randQvia the Bellman and inverse Bellman operators (proved in the\n",
            "appendix). Crucially, for the unique occupancy ρ∗which solves max θEs,a∼ρθ[r(s, a)]−H[ρθ],\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 145}\n",
            "correspondence between randQvia the Bellman and inverse Bellman operators (proved in the\n",
            "appendix). Crucially, for the unique occupancy ρ∗which solves max θEs,a∼ρθ[r(s, a)]−H[ρθ],\n",
            "the optimal policy p∗(a|s)corresponding to ρ∗is proportional to the corresponding Q-values Q∗:\n",
            "logp∗(a|s) =Q∗(s, a)−Vθ∗(s) =Q∗(s, a)−logP\n",
            "a′∈AexpQ∗(s, a′). The key idea of the\n",
            "following derivations is that the optimal policy is uniquely determined by the optimal Q-values, while\n",
            "the reward has a one-to-one correspondence with the Q-values. This allows us to optimize solely\n",
            "overQ-values. We can now write a chain of equalities involving our loss.\n",
            "Proposition 3.1. The following equalities hold for the loss:\n",
            "inf\n",
            "θdψ(ρθ, ρdata)−H[ρθ] = sup\n",
            "rinf\n",
            "θEs,a∼ρdata[r(s, a)]−Es,a∼ρθ[r(s, a)]−H[ρθ]−ψ(r),\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−Es,a∼ρθ\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−H[ρθ]−ψ(TθQ),\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−(1−γ)Es0∼P0\u0002\n",
            "Vθ(s0)\u0003\n",
            "−ψ(TθQ),\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "ϕ(Q(s, a)−γEs′∼P(·|s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            ")\u0003\n",
            "−(1−γ)Es0∼P0\u0002\n",
            "Vθ(s0)\u0003\n",
            ",\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "ϕ(Q(s, a)−γEs′∼P(·|s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            ")\u0003\n",
            "−Es,s′∼ρ\u0002\n",
            "Vθ(s)−γVθ(s′)\u0003\n",
            ",\n",
            "= sup\n",
            "QJ(Q) = sup\n",
            "QEs,a∼ρdata\u0002\n",
            "ϕ(Q(s, a)−γEs′∼P(·|s,a)[V(s′)])\u0003\n",
            "−Es,s′∼ρ[V(s)−γV(s′)],\n",
            "(5)\n",
            "where ϕis a concave function and Es,s′∼ρdenotes an expectation over sampled states sand successor\n",
            "states s′, for any occupancy ρ.V(s)(without θ) is given by V(s) = logP\n",
            "a′∈AexpQ(s, a′)\n",
            "5Actions: \n",
            "Apples are blue <backspace> green  States: \n",
            "S0: <begin-sentence> \n",
            "S1: <begin-sentence> Apples \n",
            "S2: <begin-sentence> Apples are \n",
            "S3: <begin-sentence> Apples are blue \n",
            "S4: <begin-sentence> Apples are \n",
            "S5: <begin-sentence> Apples are green \n",
            "Masked Unmasked Masks: Single-Pass Loss Inputs \n",
            "Input Tokens: \n",
            "<begin-sentence>    Apples    are      blue        are     green \n",
            "Targets: \n",
            "     Apples          are      blue  <backspace>    green   N/A \n",
            "Position IDs:\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 146}\n",
            "Input Tokens: \n",
            "<begin-sentence>    Apples    are      blue        are     green \n",
            "Targets: \n",
            "     Apples          are      blue  <backspace>    green   N/A \n",
            "Position IDs: \n",
            "        0             1        2        3           2       3 Logical Sequence Trajectory Figure 2: Attention masks, position IDs, inputs and labels for states and actions. Transforming states\n",
            "and actions to single-pass inputs allows parallel computation of logits for all (state, action) pairs.\n",
            "Proof. The full proof is given in detail in the appendix. As a sketch, the first equality holds from\n",
            "the previous section. The second is obtained by replacing rwithTθQand verifying that the two\n",
            "optimization problems are equal. The third line is via a telescoping sum argument first described\n",
            "in [18]. In the fourth line we replace ψ(r)with a simpler regularizer Es,a∼ρdata[g(r(s, a))], where\n",
            "g(r) =r−ϕ(r)ifr∈Ω, and infinity otherwise. In the fifth line we expand the telescoping sum in a\n",
            "different way, allowing us to incorporate samples from any policy. In the final line we parameterize\n",
            "the policy from the Q-values, setting logpQ(a|s) =Q(s, a)−logP\n",
            "a′∈AexpQ(s, a′). We then\n",
            "show that the optimization problem over (Q, pQ)has the same optimum as the optimization over\n",
            "Q, θ, allowing us to eliminate θfrom the optimization entirely.\n",
            "Because the Q-value directly gives the logits of the optimal policy via logpQ(a|s) =Q(s, a)−\n",
            "logP\n",
            "a′∈AexpQ(s, a′), we relabel Qasℓθto make the connection to logits more clear. This results\n",
            "in the fully supervised objective over the logits of a policy ℓθ\n",
            "J(ℓθ) =1\n",
            "αEs,a,s′∼ρdata[ϕ(αℓθ(a|s)−αγV(s′)]−1\n",
            "2Es,s′∼ρdata[V(s)−γV(s′)] (6)\n",
            "−1\n",
            "2Es,s′∼ρθ[V(s)−γV(s′)],\n",
            "where (s, a, s′)∼ρcorresponds to sampling s, afromρands′fromP(·|s, a). The value function is\n",
            "V(s) = logP\n",
            "a′∈Aexpℓθ(a′|s).\n",
            "Minimizing this objective is the same as solving minθdψ(ρθ, ρdata)−αH[ρθ], where dψ(P, Q) =\n",
            "supr∈ΩEx∼P[ϕ(r(x))]−Ex∼Q[r(x)]. By choosing Ωandϕ, we can recover f-divergences by\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 147}\n",
            "a′∈Aexpℓθ(a′|s).\n",
            "Minimizing this objective is the same as solving minθdψ(ρθ, ρdata)−αH[ρθ], where dψ(P, Q) =\n",
            "supr∈ΩEx∼P[ϕ(r(x))]−Ex∼Q[r(x)]. By choosing Ωandϕ, we can recover f-divergences by\n",
            "referencing equation (1), including KL, JS and χ2divergences, and the Wasserstein and MMD\n",
            "distances. The corresponding choices are given in the appendix.\n",
            "4 Practical Occupancy Matching with Autoregressive Sequence Models\n",
            "In practice, we wish to train a parameterized model pθ(a|s)which can serve as a policy, emitting a\n",
            "probability distribution over the next action given a partially completed sequence s. In domains such\n",
            "as language, a common choice is a transformer [ 35]. The transformer architecture with parameters θ\n",
            "gives a probability distribution over the next token xigiven the previous tokens x<i, parameterized\n",
            "as a set of logits ℓθ. This is converted to a probability distribution as logpθ(xi) =ℓθ(xi|x<i)−\n",
            "logP\n",
            "x′∈Xexpℓθ(x′|x<i). Thus the typical MLE loss, given a sample from the dataset of a complete\n",
            "6sequence x1:N, can be written as ˆLMLE(ℓθ) =PN\n",
            "i=1ℓ(xi|x<i)−logP\n",
            "x′∈Xexpℓ(x′|x<i). We can\n",
            "write this using the value function notation as ˆLMLE(ℓθ) =PN\n",
            "i=1ℓ(xi|x<i)−V(x<i).\n",
            "We now wish to equivalently form an estimator for the loss in equation (6). To do this, samples\n",
            "from ρθare required. We obtain these samples by sampling complete sequences from the policy\n",
            "autoregressively and weighting the partial sequence at time tby a factor of γt. We similarly sample\n",
            "sequences from ρdataby sampling complete sequences from Pdataand weighting. We must take some\n",
            "care with the terminal states, which we discuss below.\n",
            "Then, given a length-N sequence of states s1:Nfrom a dataset of sequences, corresponding actions\n",
            "a1:Nand a generated length- Msequence u1:Mof states from the model, we can form an estimator\n",
            "for the loss in equation (6):\n",
            "ˆJ(ℓθ) =NX\n",
            "iγi1\n",
            "αϕ(αℓθ(ai|si)−γαV(si+1))\n",
            "| {z }\n",
            "Penalized difference from action logit to next state value−NX\n",
            "iγi\n",
            "2[V(si)−γV(si+1)]\n",
            "| {z }\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 148}\n",
            "for the loss in equation (6):\n",
            "ˆJ(ℓθ) =NX\n",
            "iγi1\n",
            "αϕ(αℓθ(ai|si)−γαV(si+1))\n",
            "| {z }\n",
            "Penalized difference from action logit to next state value−NX\n",
            "iγi\n",
            "2[V(si)−γV(si+1)]\n",
            "| {z }\n",
            "State, next state value difference under data\n",
            "−MX\n",
            "iγi\n",
            "2[V(ui)−γV(ui+1)]\n",
            "| {z }\n",
            "State, next state value difference under model+ ˆJeos(ℓθ)|{z}\n",
            "Loss from completed sequences, (7)\n",
            "with\n",
            "ˆJeos(ℓθ) =γN\n",
            "α(1−γ)ϕ(α(1−γ)V(sN))−γN\n",
            "2V(sN)−γM\n",
            "2V(uM), (8)\n",
            "andV(s) = logP\n",
            "a′∈Aexpℓθ(a′|s). The separate treatment of the <eos> tokens arises because\n",
            "we must take a sum over the infinite timesteps in equation (3)in the terminal states with <eos>\n",
            "tokens. Analytically computing this infinite sum results in the terms in ˆJeos(ℓθ). It has also been\n",
            "shown [ 1] that minimizing the the divergence of mixtures Dχ2(ρdata,(ρdata+ρθ)/2)is more effective\n",
            "than simply minimizing the χ2-divergence between model and data. This can be implemented\n",
            "by calculating the loss for the χ2-divergence (with ϕ(x) =x−1\n",
            "4αx2) and adding an additional\n",
            "regularization term Es,a,s′∼ρθ\u0002\n",
            "(αℓθ(a|s)−γαV(s′))2\u0003\n",
            ".\n",
            "While the overall objective ˆJ(ℓθ)may look similar to the MLE estimator due to the ℓθ(a|s)−γV(s) =\n",
            "ℓθ(a|s)−γlogP\n",
            "a′∈Aexpℓθ(a′|s′)term, it should be stressed that the normalization is with respect\n",
            "to the next sequence state, as opposed to the MLE loss which normalizes according to the current\n",
            "sequence state.\n",
            "While the loss J(ℓθ)may look obscure, it is instructive to take the limit as α→0for the ϕ\n",
            "corresponding to the KL-divergence. We show in the appendix that we do indeed have limα→0Jℓθ=\n",
            "DKL(ρdata∥ρθ). The equality follows by approximating1\n",
            "αϕ(αx) =xϕ′(0) +O(α)ifϕhas a Taylor\n",
            "expansion at 0, which allows a linearization of ϕand simplifications. We also show that in the absence\n",
            "of editing actions, DKL(ρdata∥ρθ)reduces to a γ-reweighted MLE objective.\n",
            "4.1 Relative Efficiency of Generating During Training\n",
            "Our loss as presented requires sampling from the model during training. Sampling is generally much\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 149}\n",
            "4.1 Relative Efficiency of Generating During Training\n",
            "Our loss as presented requires sampling from the model during training. Sampling is generally much\n",
            "slower than computing the logits. Even when using caching to reduce unnecessary recomputations,\n",
            "the time taken to sample from the model will scale with the expected length of the sequence. However,\n",
            "the sequences do not need to be exactly sampled from the current policy. Since any policy can be used,\n",
            "we can keep sequences generated from the policy at previous training steps, stored in a replay buffer\n",
            "[22]. We give an empirical analysis of the overhead when using SequenceMatch in the appendix.\n",
            "4.2 Efficient Implementation of Backspace with Editing Actions\n",
            "Editing actions which can delete previous parts of the input are challenging to implement while\n",
            "retaining the fast training of transformer-based autoregressive models. For instance, the sequence of\n",
            "actions [a; b; <backspace>] cannot be fed directly into a policy network pθ(a|s), since it contains\n",
            "7actions, not states. The sequence [a; b; <backspace>] is not a valid state: the corresponding\n",
            "state is [<begin-sentence> a] .\n",
            "In order to convert this into a form where we can compute the relevant logits using masked attention,\n",
            "we must pre-process the sequence of actions into corresponding inputs, labels, masks and position\n",
            "IDs using algorithm A in the appendix. The preprocessing is illustrated in figure 2. On the other\n",
            "hand, generation with <backspace> actions is straightforward: we already keep previous key-value\n",
            "cached values for generation with transformers. When <backspace> is sampled, we simply roll\n",
            "back the state of the key-value cache and position id with negligible overhead.\n",
            "4.3 Augmenting Expert Sequences with Backspace\n",
            "To provide the policy with examples of how the <backspace> token should be used, we augment the\n",
            "data sequences as follows: with (small) probability η, we replace a sequence . . . , x i−1, xi, xi+1, . . .\n",
            "withxi−1, xi, x′\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 150}\n",
            "data sequences as follows: with (small) probability η, we replace a sequence . . . , x i−1, xi, xi+1, . . .\n",
            "withxi−1, xi, x′\n",
            "i,<backspace> , xi+1, . . ., where x′\n",
            "iis chosen randomly from the vocabulary. How-\n",
            "ever, we keep the action at position iasxi+1, with the result that the overall MDP is augmented with\n",
            "a stochastic dynamics: with probability ηa random token is inserted, instead of the chosen action.\n",
            "We also apply this to sequences which exceed the context length: the action is kept the same but the\n",
            "next token is forced to be the <eos> token. This introduces bias, as the policy learns to match the\n",
            "data distribution under a slightly different MDP than generation takes place in. In practice however, it\n",
            "leads to improved performance compared to the policy learning with no examples of <backspace> .\n",
            "Algorithm 1: Training an autoregressive model against a SequenceMatch objective\n",
            "Input : Dataset Dof data sequences, gradient-based optimizer step , number of train steps\n",
            "ntrain, parameters α, β, γ, ϕ , sampling interval ksample , fixed context length T\n",
            "Add noise and process data sequences with algorithm A to form new effective trajectories\n",
            "Initialize buffer Bof model sequences; Initialize autoregressive policy ℓθ(·|s)\n",
            "forkinntraindo\n",
            "ifkmod ksample = 0then\n",
            "Populate Bwith trajectories T ∼ℓθ; Process added sequences with algorithm A\n",
            "Remove oldest model sequences from B\n",
            "end\n",
            "Sample dataset trajectories Tdata∼ D and model trajectories Tmodel∼ B\n",
            "Compute g=∇θˆJ(ℓθ, α, γ,Tdata,Tmodel)and update θviastep using gradient g\n",
            "end\n",
            "5 Related Work\n",
            "Text Degeneration in Large Language Models\n",
            "In natural language processing (NLP) the phenomenon of text degeneration can occur, when a\n",
            "language model produces repetitive or nonsensical sequences [ 15]. Many explanations have been\n",
            "proposed to explain this phenomenon [ 10,37]; a leading theory is that the large vocabulary size\n",
            "induces the model to over-estimate the probability of out-of-distribution (OOD) tokens. Once these\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 151}\n",
            "proposed to explain this phenomenon [ 10,37]; a leading theory is that the large vocabulary size\n",
            "induces the model to over-estimate the probability of out-of-distribution (OOD) tokens. Once these\n",
            "tokens are sampled, the model’s context is now out-of-distribution.\n",
            "Measures to mitigate this problem include top- ksampling [ 9], restricting generations to the kmost\n",
            "likely tokens, and top- psampling [ 15], an adaptive variant of top- ksampling. In addition, several\n",
            "alternative training measures have been proposed to reduce the probability of the model producing\n",
            "OOD tokens. Unlikelihood training [ 37] is discussed in detail in the appendix, while contrastive\n",
            "methods have also been proposed [ 17], which force the representations of repetitive text to be far\n",
            "from the representations of correct text.\n",
            "Matching Divergences in Imitation Learning\n",
            "In the imitation learning[ 23] subfield of RL, the objective is to learn a policy giving a distribution\n",
            "over actions in each state, such that the distribution over trajectories is close to distribution of a set\n",
            "of provided expert trajectories. A simple approach is behavioural cloning [ 8], which maximizes\n",
            "the likelihood of the expert’s chosen actions, on average over the states that the expert encounters.\n",
            "8However, it has been shown [ 27] that this simple behaviour cloning approach results in a compounding\n",
            "error problem, where the further the trained model gets from the typical expert states, the worse the\n",
            "model performs, incurring increasing error. Ho and Ermon [14] show that minimizing the occupancy\n",
            "divergence between the expert and a learned policy could be written as a two-variable saddle-point\n",
            "optimization problem. This more sophisticated method is able to take the dynamics of the problem\n",
            "into account, learning a policy which can return to the typical expert states if it erroneously leaves\n",
            "them. In Garg et al. [11], this was further developed via a change of variables to only require a\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 152}\n",
            "them. In Garg et al. [11], this was further developed via a change of variables to only require a\n",
            "non-adversarial optimization over one variable. We can view our approach as a specialization of the\n",
            "IQ-Learn algorithm in Garg et al. [11] to autoregressive sequence models.\n",
            "6 Experiments\n",
            "Small Medium Large\n",
            "GPT2 Model Size103\n",
            "102\n",
            "101\n",
            "100MAUVE Score\n",
            "MAUVE Score\n",
            "SequenceMatch (Full Model)\n",
            "Behavioral Cloning (Full Model)\n",
            "MLE (Full Model)\n",
            "SequenceMatch (LM Head)\n",
            "Behavioral Cloning (LM Head)\n",
            "MLE (LM Head)\n",
            "Figure 3: MAUVE score when fine-tuning GPT2 of various sizes on the openwebtext dataset with\n",
            "context length 512. Higher is better. Full and LM Head refer to which parts of the pretrained model\n",
            "are trained. We see that SequenceMatch outperforms behavioral cloning and MLE. The fully trained\n",
            "models achieve much higher MAUVE scores than the variant only training the head.\n",
            "We experimentally verify that SequenceMatch can lead to better generation of sequences compared to\n",
            "the typical maximum-likelihood objective, when evaluated on a language modelling task [ 5,7]. Many\n",
            "more experimental details, code and additional analyses are provided in the supplementary material.\n",
            "Experimental Setup\n",
            "We use the GPT-2 [ 25] causally-masked transformer architecture through Huggingface [ 38] and\n",
            "Flax/Jax [13, 6]. We finetune on sequences from the openwebtext dataset3, an open-sourced dataset\n",
            "similar to the original GPT-2 training set. To ensure the data was well-specified under our data\n",
            "generating process, we subsampled the dataset to remove sequences that did not terminate before the\n",
            "end of the context window (512 for experiments in figure 3 and 1024 for results in table 1), and padded\n",
            "sequences that terminated before the context length. We consider two training regimes: Full , where\n",
            "all model parameters are trained, and LM Head , where the model parameters are frozen and the final\n",
            "linear layer is trained. In both cases we add an additional head depending on the positional encodings\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 153}\n",
            "linear layer is trained. In both cases we add an additional head depending on the positional encodings\n",
            "to the model which generates an offset to the logits, allowing a position-dependent offset to be\n",
            "learned. We compare a SequenceMatch-trained model against two baselines: MLEis a model trained\n",
            "against the typical MLE objective. Behavioral Cloning is a model trained with injected noise\n",
            "and<backspace> labels, but the MLE objective. Due to computational constraints we were unable\n",
            "to train on multiple seeds to obtain error bars. We train for 50,000 gradient steps with the AdamW\n",
            "optimizer [ 21], taking hyperparameters from the nanoGPT implementation4. We set α= 0.01,\n",
            "η= 0.001andγ=N/(N+ 1) for a length- Ncontext window (i.e. 0.998, 0.999 respectively). Our\n",
            "metric for quality of generations is the MAUVE score [ 24], a non-parametric method for evaluating\n",
            "the quality of a generative model. The MAUVE score is formed by taking a low-dimensional PCA\n",
            "of an embedding of the generated sequences (here, a GPT2-XL encoder). The score is a mixture\n",
            "of forward and reverse KLs between the data and model-generated sequences, between zero and\n",
            "one (higher is better). We also report a diversity metric [ 17], given byQ4\n",
            "n=2\u0000\n",
            "1.0−rep−n\n",
            "100\u0001\n",
            ", where\n",
            "rep-n = 100 ×h\n",
            "1.0−|unique n-grams (ˆx)|\n",
            "|total n-grams (ˆx)|i\n",
            "for a continuation ˆx.\n",
            "3https://github.com/jcpeterson/openwebtext\n",
            "4https://github.com/karpathy/nanoGPT\n",
            "9Mauve Score ( ↑) Perplexity ( ↓) Diversity ( ↑)\n",
            "SM-χ20.97 4.69 0.58\n",
            "BC 0.55 4.52 0.44\n",
            "MLE 0.51 4.51 0.55\n",
            "Table 1: A GPT-2-small model finetuned against different losses, with context length 1024. The\n",
            "maximum-likelihood methods (BC and MLE) achieve good perplexity, which measures the probability\n",
            "of the next token being chosen correctly. However, they do much worse on the mauve score and\n",
            "diversity, measuring quality of the generated text when sampled autoregressively.\n",
            "Results\n",
            "The results are shown in figure 3 and table 1. We observe that, as measured by the MAUVE score,\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 154}\n",
            "diversity, measuring quality of the generated text when sampled autoregressively.\n",
            "Results\n",
            "The results are shown in figure 3 and table 1. We observe that, as measured by the MAUVE score,\n",
            "the SequenceMatch-trained models generate text that is closer to the dataset. The SequenceMatch-\n",
            "trained models also outperform MLE and BC on the diversity metric (shown in the appendix).\n",
            "Furthermore, the samples from the SequenceMatch-trained model appear more fluent (random\n",
            "samples and additional statistics are shown in the appendix). In table 1, we show the result of training\n",
            "a GPT2-small model on the full 1024 context length case. We observe that the SequenceMatch-\n",
            "trained model is outperformed on the perplexity metric by the BC and MLE-trained methods. This is\n",
            "expected, as the training objective for BC and MLE is exactly equal to the perplexity. However, on the\n",
            "measures of generation quality (MAUVE score and diversity), the SM-trained model far outperforms\n",
            "the baselines.\n",
            "6.1 Limitations and Conclusion\n",
            "We address the compounding error problem in autoregressive sequence generation by formulating\n",
            "the problem in the imitation learning (IL) framework, deriving a general non-adversarial objective\n",
            "for minimizing divergences between occupancy measures induced by a learned model and the\n",
            "data distribution. We develop a novel masking scheme allowing training of a transformer-based\n",
            "autoregressive model with a <backspace> action with small overhead vs MLE, further reducing\n",
            "compounding error by facilitating backtracking. Empirically, the SequenceMatch objective leads to\n",
            "improvements over MLE at text generation. The main limitation with our method is the requirement\n",
            "to generate samples autoregressively while training, which can be expensive for long contexts. We\n",
            "give an empirical examination of this training overhead in the appendix. A secondary limitation is a\n",
            "more complex loss and requirement to process sequences to mask backspaces during training. Future\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 155}\n",
            "give an empirical examination of this training overhead in the appendix. A secondary limitation is a\n",
            "more complex loss and requirement to process sequences to mask backspaces during training. Future\n",
            "work can investigate the effect of training larger models against SequenceMatch objectives, as well\n",
            "as how the qualities of generations change with choice of divergence.\n",
            "10References\n",
            "[1]Firas Al-Hafez, Davide Tateo, Oleg Arenz, Guoping Zhao, and Jan Peters. LS-IQ: Implicit\n",
            "reward regularization for inverse reinforcement learning. In The Eleventh International Confer-\n",
            "ence on Learning Representations , 2023.\n",
            "[2]Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein generative adversarial\n",
            "networks. In International Conference on Machine Learning , pages 214–223. PMLR, 2017.\n",
            "[3]Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why Exposure\n",
            "Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation.\n",
            "InFindings of the Association for Computational Linguistics: ACL 2022 , pages 700–710, 2022.\n",
            "[4]Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Chris Pal, and Derek Nowrouzezahrai.\n",
            "Adversarial soft advantage fitting: Imitation learning without policy optimization. In Advances\n",
            "in Neural Information Processing Systems , volume 33, pages 12334–12344, 2020.\n",
            "[5]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\n",
            "Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\n",
            "opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\n",
            "[6]James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\n",
            "Maclaurin, and Skye Wanderman-Milne. JAX: Composable transformations of Python+NumPy\n",
            "programs }, 2020.\n",
            "[7]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\n",
            "wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 156}\n",
            "programs }, 2020.\n",
            "[7]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\n",
            "wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\n",
            "wal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\n",
            "Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\n",
            "Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\n",
            "Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners.\n",
            "arXiv:2005.14165 [cs] , July 2020.\n",
            "[8]Nasser Esmaili, Claude Sammut, and GM Shirazi. Behavioural cloning in control of a dynamic\n",
            "system. In 1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent\n",
            "Systems for the 21st Century , volume 3, pages 2904–2909. IEEE, 1995.\n",
            "[9]Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\n",
            "ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\n",
            "Long Papers) , pages 889–898, 2018.\n",
            "[10] Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei Shi. A theoretical analysis of the repetition\n",
            "problem in text generation. In Proceedings of the AAAI Conference on Artificial Intelligence ,\n",
            "volume 35, pages 12848–12856, 2021.\n",
            "[11] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. IQ-\n",
            "Learn: Inverse soft-Q learning for imitation. In NeurIPS , 2021.\n",
            "[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n",
            "Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Neural Information\n",
            "Processing Systems (NeurIPS) , 2014.\n",
            "[13] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas\n",
            "Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020.\n",
            "[14] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural\n",
            "information processing systems , 29, 2016.\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 157}\n",
            "[14] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural\n",
            "information processing systems , 29, 2016.\n",
            "[15] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\n",
            "text degeneration. In International Conference on Learning Representations , 2019.\n",
            "[16] Abdul Jabbar, Xi Li, and Bourahla Omar. A Survey on Generative Adversarial Networks:\n",
            "Variants, Applications, and Training. arXiv:2006.05132 [cs] , June 2020.\n",
            "11[17] Shaojie Jiang, Ruqing Zhang, Svitlana Vakulenko, and Maarten de Rijke. A simple contrastive\n",
            "learning objective for alleviating neural text degeneration. arXiv preprint arXiv:2205.02517 ,\n",
            "2022.\n",
            "[18] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribu-\n",
            "tion matching. In International Conference on Learning Representations , 2019.\n",
            "[19] Dohyun Kwon, Yeoneung Kim, Guido Mont ´ufar, and Insoon Yang. Training Wasserstein GANs\n",
            "without gradient penalties. arXiv:2110.14150 [cs, math] , October 2021.\n",
            "[20] Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu Jie Huang. A Tutorial\n",
            "on Energy-Based Learning, 2006.\n",
            "[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\n",
            "Conference on Learning Representations , 2019.\n",
            "[22] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\n",
            "Wierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning, 2013.\n",
            "[23] Andrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In In Proc.\n",
            "17th International Conf. on Machine Learning . Citeseer, 2000.\n",
            "[24] Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan\n",
            "Zellers, Sewoong Oh, Yejin Choi, and Zaid Harchaoui. MAUVE scores for generative models:\n",
            "Theory and practice. arXiv preprint arXiv:2212.14578 , 2022.\n",
            "[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 158}\n",
            "Theory and practice. arXiv preprint arXiv:2212.14578 , 2022.\n",
            "[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n",
            "Language models are unsupervised multitask learners, 2018.\n",
            "[26] Douglas Rizzolo and Francis Edward Su. A fixed point theorem for the infinite-dimensional\n",
            "simplex. Journal of mathematical analysis and applications , 332(2):1063–1070, 2007.\n",
            "[27] St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and\n",
            "structured prediction to no-regret online learning. In AISTATS , pages 627–635, 2011.\n",
            "[28] Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. Toward Diverse Text Generation\n",
            "with Inverse Reinforcement Learning. arXiv:1804.11258 [cs, stat] , June 2018.\n",
            "[29] Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics , 8(4):171–176,\n",
            "1958.\n",
            "[30] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\n",
            "distribution. In Advances in Neural Information Processing Systems , volume 32, 2019.\n",
            "[31] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,\n",
            "and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations.\n",
            "arXiv:2011.13456 [cs, stat] , February 2021.\n",
            "[32] Gokul Swamy, Sanjiban Choudhury, Zhiwei Steven Wu, and J. Andrew Bagnell. Of Moments\n",
            "and Matching: Trade-offs and Treatments in Imitation Learning. arXiv:2103.03236 [cs, stat] ,\n",
            "March 2021.\n",
            "[33] Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear\n",
            "programming. In Proceedings of the 25th International Conference on Machine Learning , pages\n",
            "1032–1039, 2008.\n",
            "[34] Shichang Tang. Lessons Learned from the Training of GANs on Artificial Datasets.\n",
            "arXiv:2007.06418 [cs, stat] , July 2020.\n",
            "[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
            "Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\n",
            "tion Processing Systems , volume 30, 2017.\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 159}\n",
            "Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\n",
            "tion Processing Systems , volume 30, 2017.\n",
            "[36] Dirk Weissenborn, Oscar T ¨ackstr ¨om, and Jakob Uszkoreit. Scaling autoregressive video models.\n",
            "InInternational Conference on Learning Representations , 2020.\n",
            "12[37] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.\n",
            "Neural text generation with unlikelihood training. In International Conference on Learning\n",
            "Representations , 2019.\n",
            "[38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\n",
            "Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Huggingface’s transform-\n",
            "ers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.\n",
            "13A Algorithm A\n",
            "Algorithm 2: Algorithm A: Pseudocode for converting action sequences to masked inputs\n",
            "Input : Sequence of action inputs a1:L\n",
            "output : Sequence of labels, inputs, masks, position ids\n",
            "y∈ |V|L, x∈ |V|L, m∈ {0,1}L×L, p∈[L]L\n",
            "Initialize y, m, p to zero.\n",
            "Initialize xtoa.\n",
            "Initialize c= 0 // Copy Pointer\n",
            "Initialize d= 0 // Deletion Pointer\n",
            "fori= 0, . . . L do\n",
            "m[i]←m[max ( i−1,0)]\n",
            "ifa[i] =<backspace> then\n",
            "m[i, c]←0\n",
            "m[i, d]←0\n",
            "m[i, i]←1\n",
            "x[i]←x[c]\n",
            "p[i]←p[c]\n",
            "d←i\n",
            "c←element of last nonzero element in m[i,0 :c], else 0.\n",
            "end\n",
            "else\n",
            "m[i, i]←1\n",
            "d←d+ 1\n",
            "c←element of first nonzero element in m[i, c+ 1 : L], else 0.\n",
            "p[i]←p[d−1] + 1\n",
            "end\n",
            "ifi= 0then\n",
            "c←0 // Special cases for initial steps\n",
            "d←0\n",
            "end\n",
            "ifi= 1then\n",
            "c←0\n",
            "d←1\n",
            "end\n",
            "end\n",
            "In algorithm A we give a method to convert a sequence of actions into a masked sequence of inputs\n",
            "and corresponding labels, masks and position ids. Although it can also be implemented in a stack-\n",
            "based fashion, we write it as an imperative algorithm so it can be compiled with the jitoperation in\n",
            "JAX or the compile operation in PyTorch. Recall that the idea is to replace a sequence of actions\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 160}\n",
            "based fashion, we write it as an imperative algorithm so it can be compiled with the jitoperation in\n",
            "JAX or the compile operation in PyTorch. Recall that the idea is to replace a sequence of actions\n",
            "with a sequence of inputs and corresponding labels, masks and position ids. The input sequence at\n",
            "position tshould correspond to the state at position t(when the corresponding mask is applied) and\n",
            "the label at position t+ 1is the action taken at position t. A consequence of this is that the inputs\n",
            "should never include a <backspace> token. The main idea is that if we have a sequence of actions\n",
            "[..., a, b, <backspace>, ...] , the corresponding inputs are [..., a, b, a, ...] , while\n",
            "the masks for the second aonwards mask out the first a, b . However, the possibility of multiple\n",
            "backspaces introduces some complexity.\n",
            "The approach of the algorithm is to keep a running copy pointer and deletion pointer. The deletion\n",
            "pointer points to the cell of the mask that must be zeroed out for subsequent positions in the sequence,\n",
            "while the copy pointer points to the position that must be copied to the current cell of the input (and\n",
            "also zeroed out in the mask). When a backspace occurs, the deletion pointer is set to the current\n",
            "index, and the copy pointer is sent backwards to the last non-deleted position. When a backspace\n",
            "doesn’t occur, the deletion pointer is incremented by 1 and the copy pointer is moved forwards to the\n",
            "first non-deleted position.\n",
            "14B Motivating Example Algebra\n",
            "We consider the case of a length- nMarkov chain with an additional node coming from each node.\n",
            "These nodes correspond to the dashed nodes in figure 1. We write the dashed nodes as xterm. As in\n",
            "the figure, we have Pdata(xterm) = 0 ,Pmodel(xterm) =ϵ. We wish to compute the divergence between\n",
            "the two distributions.\n",
            "B.1 KL-Divergence\n",
            "We have\n",
            "DKL(P∥Q) =Ex∼P[logP(x)−logQ(x)] =−nlogQ(1−ϵ).\n",
            "B.2 Reverse KL-Divergence\n",
            "We have\n",
            "DKL(Q∥P) =Ex∼Q[logQ(x)−logP(x)] =∞,\n",
            "since P(xterm= 0) andQ(xterm)̸= 0.\n",
            "B.3 χ2-Divergence\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 161}\n",
            "B.1 KL-Divergence\n",
            "We have\n",
            "DKL(P∥Q) =Ex∼P[logP(x)−logQ(x)] =−nlogQ(1−ϵ).\n",
            "B.2 Reverse KL-Divergence\n",
            "We have\n",
            "DKL(Q∥P) =Ex∼Q[logQ(x)−logP(x)] =∞,\n",
            "since P(xterm= 0) andQ(xterm)̸= 0.\n",
            "B.3 χ2-Divergence\n",
            "We have\n",
            "Dχ2(Q, P) =Ex∼Q\"\u0012P(x)\n",
            "Q(x)−1\u00132#\n",
            "C Proofs for section 3.2\n",
            "C.1 Proof for Saddle Point Theorem\n",
            "We wish to show that\n",
            "inf\n",
            "ρθsup\n",
            "rL(θ, r) = sup\n",
            "rinf\n",
            "ρθL(θ, r).\n",
            "Now, the set Dof occupancy measures stemming from conditionals is compact and con-\n",
            "vex, since it is formed from linear constraints: firstly ρ≥0, and secondlyP\n",
            "aρ(s, a) =\n",
            "γP\n",
            "s′ρ(s′, a)P(s|s′, a),∀s, s′. Because Dis a closed subset of the infinite-dimensional simplex\n",
            "∆∞\n",
            "0, which is compact [ 26],Dis also compact. The set Ris convex, since it consists of all sequences.\n",
            "Since the inner function is convex in ρθand concave in r, we can apply Sion’s minimax theorem [ 29]\n",
            "to swap the inner inf and sup.\n",
            "C.2 Proof for Bijection between randQ\n",
            "Recall that we define the Bellman operator as Bθ\n",
            "r, where Bθ\n",
            "rQ(s, a) =r(s, a)+γEs′∼P(s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            ",\n",
            "for the value function Vθ(s) =Ea∼pθ(·|s)[Q(s, a)−logpθ(a|s)]. The inverse Bellman operator Tθ\n",
            "is defined as (TθQ)(s, a) =Q(s, a)−γEs′∼P(s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            ".\n",
            "Theorem C.1. For a fixed policy θ, the inverse soft Bellman operator Tθis bijective, and for any\n",
            "r∈ R,Q= (Tθ)−1ris the unique fixed point of the Bellman operator Bθ\n",
            "r.\n",
            "Proof. The proof is very similar to the proof of lemma 3.2 in Garg et al. [11]. We construct an\n",
            "infinite matrix Pθ∈R(S×A )×(S×A ), where (Pθf)(s, a) =Es′∼P(·|s,a),a′∼pθ(·|s′)[f(s′, a′)]. The\n",
            "matrix Pθcorresponds to the transition matrix for the given MDP and the policy θ. We then\n",
            "have r=TθQ, for any Q. Then, r=Q−γPθ(Q−logpθ). Rearranging, we get Q=\n",
            "(I−γPθ)−1(r−logpθ) + log pθ. We can do this since |γPθ|∞<1ifγ <1, and so I−γPθ\n",
            "is invertible, even in this infinite-dimensional setting. We also see that Qhas a unique vector\n",
            "expansion Q=r+γPθ(Q−logpθ). Since this is the (unique) vector expansion of Bθ\n",
            "r, we have\n",
            "Q= (Tθ)r=Bθ\n",
            "rQ\n",
            "15C.3 Telescoping Sum Proofs\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 162}\n",
            "expansion Q=r+γPθ(Q−logpθ). Since this is the (unique) vector expansion of Bθ\n",
            "r, we have\n",
            "Q= (Tθ)r=Bθ\n",
            "rQ\n",
            "15C.3 Telescoping Sum Proofs\n",
            "In this section we prove various theorems related to telescoping sums and value functions. These\n",
            "mostly follow from Kostrikov et al. [18] and [11].\n",
            "Proposition C.2. For a policy pθ, initial state distribution P0, value function Vθ(s) =\n",
            "Ea∼pθ(·|s)[Q(s, a)−logpθ(a|s)], the following identities hold:\n",
            "Es,a∼ρθ\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "+H[ρθ] = (1 −γ)Es0∼P0\u0002\n",
            "Vθ(s0)\u0003\n",
            "(9)\n",
            "=Es,s′∼ρ\u0002\n",
            "Vθ(s)−γVθ(s′)\u0003\n",
            ", (10)\n",
            "where ρis any occupancy measure, and s, s′∼ρdenotes sampling s, afromρands′fromP(a, s).\n",
            "Proof. We have\n",
            "Es,a∼ρθ\u0002\n",
            "(Tθ)Q(s, a)\u0003\n",
            "+H[pθ] =Es,a∼ρθ\u0002\n",
            "Q(a, s)−γEs′∼P(s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            "−logpθ(a|s)\u0003\n",
            "(11)\n",
            "=Es,s′∼ρθ\u0002\n",
            "Vθ(s)−γVθ(s′)\u0003\n",
            ". (12)\n",
            "By the definition of the occupancy measure and expanding, we have\n",
            "Es,s′∼ρθ\u0002\n",
            "Vθ(s)−γVθ(s′)\u0003\n",
            "= (1−γ)\u0002\u0002\n",
            "E[Vθ(s0)]−γE[Vθ(s1)]\u0003\n",
            "+γ\u0002\n",
            "E[Vθ(s1)]−γE[Vθ(s2)] +. . .\u0003\u0003\n",
            "(13)\n",
            "= (1−γ)E[Vθ(s0)]. (14)\n",
            "Because s0does not depend on ρ, we can expand the sum in the opposite direction to show that\n",
            "Es,a∼ρθ\u0002\n",
            "(Tθ)Q(s, a)\u0003\n",
            "+H[pθ] =Es,s′∼ρ\u0002\n",
            "Vθ(s)−γVθ(s′)\u0003\n",
            "for any occupancy ρ.\n",
            "C.4 Proof of equivalence of solutions of JandL\n",
            "We now reproduce a proposition from Garg et al. [11],\n",
            "Proposition C.3. In the Q-policy space, there exists a unique saddle point (p∗\n",
            "θ, Q∗), that optimizes J.\n",
            "That is, Q∗= arg maxQ∈ΩminpθJ(pθ, Q)andp∗\n",
            "θ= arg minpθmax Q∈OJ(pθ, Q). Furthermore,\n",
            "p∗\n",
            "θandr∗=Tp∗\n",
            "θQ∗are the solution to the inverse RL objective L(pθ, r). This is proposition 3.4 in\n",
            "Garg et al. [11].\n",
            "Proof. See Garg et al. [11] for the complete proof. The proof given applies directly to our case.\n",
            "C.5 Proof for Theorem 3.1\n",
            "We can now prove our main result\n",
            "Proposition C.4. With quantities defined in the main text, the following equalities hold for the loss:\n",
            "inf\n",
            "θdψ(ρθ, ρdata)−H[ρθ] = sup\n",
            "rinf\n",
            "θEs,a∼ρdata[r(s, a)]−Es,a∼ρθ[r(s, a)]−H[ρθ]−ψ(r),\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−Es,a∼ρθ\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−H[ρθ]−ψ(TθQ),\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−(1−γ)Es0∼P0\u0002\n",
            "Vθ(s0)\u0003\n",
            "−ψ(TθQ),\n",
            "= sup\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 163}\n",
            "θEs,a∼ρdata[r(s, a)]−Es,a∼ρθ[r(s, a)]−H[ρθ]−ψ(r),\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−Es,a∼ρθ\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−H[ρθ]−ψ(TθQ),\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "(TθQ)(s, a)\u0003\n",
            "−(1−γ)Es0∼P0\u0002\n",
            "Vθ(s0)\u0003\n",
            "−ψ(TθQ),\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "ϕ(Q(s, a)−γEs′∼P(·|s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            ")\u0003\n",
            "−(1−γ)Es0∼P0\u0002\n",
            "Vθ(s0)\u0003\n",
            ",\n",
            "= sup\n",
            "Qinf\n",
            "θEs,a∼ρdata\u0002\n",
            "ϕ(Q(s, a)−γEs′∼P(·|s,a)\u0002\n",
            "Vθ(s′)\u0003\n",
            ")\u0003\n",
            "−Es,s′∼ρ\u0002\n",
            "Vθ(s)−γVθ(s′)\u0003\n",
            ",\n",
            "= sup\n",
            "QJ(Q) = sup\n",
            "QEs,a∼ρdata\u0002\n",
            "ϕ(Q(s, a)−γEs′∼P(·|s,a)[V(s′)])\u0003\n",
            "−Es,s′∼ρ[V(s)−γV(s′)],\n",
            "(15)\n",
            "Proof. The first equality is proven in section C.1. The second line follows from sections C.2 and C.4.\n",
            "The first section shows that the objectives J(Q, θ)andL(θ, r)are the same, by the bijective property\n",
            "16ofT. The second section proves that the (unique) saddle points of the objectives correspond to the\n",
            "same solutions.\n",
            "The third line follows from the telescoping sum given in section C.3. The fourth line follows from the\n",
            "substitution of a general ψ(r)with a simpler regularizer Es,a∼ρdata[g(r(s, a))], where g(r) =r−ϕ(r)\n",
            "ifr∈Ω, and infinity otherwise. This allows us to ground out the divergence minimization directly to\n",
            "concrete divergences such as the KL-divergence, JS-divergence, χ2-divergence, etc. We discuss this\n",
            "more extensively in section D.1. In the fifth line we expand the telescoping sum in a different way\n",
            "using the result in section C.3. This allows us to incorporate samples from any policy, with the hope\n",
            "that it will decrease variance.\n",
            "In the final line we parameterize the policy from the Q-values, setting logpQ(a|s) =Q(s, a)−\n",
            "logP\n",
            "a′∈AexpQ(s, a′). The fact that supQinfθJ(pθ, Q) = supQJ(pQ, Q)follows from the fact\n",
            "that there is a unique saddle point for J(pθ, Q), the fact that J(pQ, Q)is concave in Q, and that\n",
            "the saddle point for J(pQ, Q)has a supremum in Qwhere θ=θ∗, with logp∗\n",
            "θ(a|s) =Q∗(s, a)−\n",
            "logP\n",
            "a′∈AexpQ∗(s, a′)andQ∗the corresponding supremum in Q. This allows elimination of θ\n",
            "from the optimization process entirely, and completes the proof.\n",
            "D Choices of divergence measures\n",
            "D.1 f-divergences\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 164}\n",
            "a′∈AexpQ∗(s, a′)andQ∗the corresponding supremum in Q. This allows elimination of θ\n",
            "from the optimization process entirely, and completes the proof.\n",
            "D Choices of divergence measures\n",
            "D.1 f-divergences\n",
            "We recall that for any f-divergence with Df(P, Q) =Ex∼Q[f(P(x)/Q(x))], we have the variational\n",
            "form\n",
            "Df(P, Q) = sup\n",
            "ϕ{Ex∼P[ϕ(x)]−Ex∼Q[f∗(ϕ(x)]},\n",
            "with the convex conjugate f∗(y) = supx{x·y−f(x)}) and a discriminator ϕ:X → R. Opti-\n",
            "mizing a model against an f-divergence other than the KL-divergence typically involves a difficult\n",
            "min-max optimization problem where we simultaneously improve the model and improve the dis-\n",
            "criminator ϕ. This is subject to unstable training [19, 16, 34, 12].\n",
            "In the main paper, we explain that we require a divergence\n",
            "dψ(ρθ, ρdata) =ψ∗(ρθ−ρdata).\n",
            "With our choice of ψ, we get that\n",
            "dψ(ρ, ρ data) = max\n",
            "r∈RψEs,a∼ρdata[ϕ(r(s, a))]−Es,a∼ρθ[r(s, a)]\n",
            "We can readily connect these to f-divergences. Recall that the variational formulation of the f-\n",
            "divergence is\n",
            "Df(P, Q) = sup\n",
            "g{Ex∼P[g(x)]−Ex∼Q[f∗(g(x)]}, (16)\n",
            "so we can see that the function ϕwe need is simply −f∗(−x).\n",
            "D.2 KL-divergence\n",
            "Note that we define our divergence in the reverse fashion to the usual convention, so to obtain the\n",
            "typical forward KL under the expectation of the data, we must use the reverse-KL f-divergence, with\n",
            "f(x) =xlogx. This gives ϕ(x) =−e−(x+1). However, since we can always shift an f-divergence’s\n",
            "fby a constant multiple of (x−1)without changing the divergence (which should be clear from\n",
            "observing cancellations in the definition of the fdivergence), we shift by -1 and (after working\n",
            "through the derivations) have a simpler ϕ(x) =−e−x.\n",
            "D.3 Jenson-Shannon Divergence\n",
            "The Jenson-Shannon divergence has f(x) =−(x+ 1) log(x+1\n",
            "2) +xlogx. This leads to ϕ(x) =\n",
            "log(2−e−x). This is an interesting ϕbecause it is equal to −∞ forx <−log 2 . Since the xin this\n",
            "17case is the value of robtained from the model’s logits, it is certainly possible that the value may\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 165}\n",
            "log(2−e−x). This is an interesting ϕbecause it is equal to −∞ forx <−log 2 . Since the xin this\n",
            "17case is the value of robtained from the model’s logits, it is certainly possible that the value may\n",
            "be less than −log 2 . In practice, we could replace ϕwith a sharply descending quadratic for all x\n",
            "close to −log 2 and below. This gives a penalizing effect on small r, while not causing (too many)\n",
            "numerical issues.\n",
            "D.4 χ2-Divergence and χ2-Mixture Divergence\n",
            "For the χ2-divergence, we have f(x) = (( t−1)2), leading to ϕ(x) = (x−x2/4).\n",
            "As described in Al-Hafez et al. [1], we can add a regularization term by computing ψρ(r) =\n",
            "βcEρdata\u0002\n",
            "r(s, a)2\u0003\n",
            "+ (1−β)cEρθ\u0002\n",
            "r(s, a)2\u0003\n",
            ". In other words, instead of computing the r2/4term on\n",
            "the expert trajectories only, we also compute this for the policy trajectories as well. We set c= 0.5\n",
            "andβ= 0.5. This results in an even mixture of regularization contributions from the expert and\n",
            "policy. Although this was introduced in Garg et al. [11] heuristically, it was shown in Al-Hafez et al.\n",
            "[1]that this has a well-motivated derivation as a result of the divergence between the data occupancy\n",
            "and the mixture between the data occupancy and the policy occupancy:\n",
            "2χ2(ρdata∥ρdata+ρθ\n",
            "2|{z}\n",
            "ρmix) = sup\n",
            "r2\u0012\n",
            "Eρdata[r(s, a)]−Eρmix\u0014\n",
            "r(s, a) +r(s, a)2\n",
            "4\u0015\u0013\n",
            "= sup\n",
            "rEρdata[r(s, a)]−Eρθ[r(s, a)]−cαEρdata\u0002\n",
            "r(s, a)2\u0003\n",
            "−c(1−α)Eρθ\u0002\n",
            "r(s, a)2\u0003\n",
            ".\n",
            "In practice, we find this method leads to better quality generations.\n",
            "E Additional Training Details\n",
            "E.1 Hardware\n",
            "We train each model on four A4000 GPUs with 16GB VRAM each. We keep the batch size at 128 for\n",
            "all models, although we have to reduce the batch size per device and use gradient accumulation for the\n",
            "larger models (GPT2-medium and large). As mentioned in the main text, we keep training parameters\n",
            "the same as the nanoGPT implementation. For the behavioral cloning and SequenceMatch models,\n",
            "we add a new column to the output embedding corresponding to the <backspace> output. For the\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 166}\n",
            "the same as the nanoGPT implementation. For the behavioral cloning and SequenceMatch models,\n",
            "we add a new column to the output embedding corresponding to the <backspace> output. For the\n",
            "fully trained models, we add a <BOS> token to the start of the sequences and add a new column to the\n",
            "input embedding corresponding to the <BOS> token. For the SequenceMatch models, we first train\n",
            "against the BC objective for 5,000 gradient steps, and then train against a convex combination of\n",
            "the SM loss: Ltotal=βLBC+ (1−β)LSM, where βis annealed from 1to0.2linearly from 5,000\n",
            "to 10,000 gradient steps. For the full training, we freeze all layers of the model except the output\n",
            "embedding for training steps 0 to 1000, then linearly unfreeze them for steps 1000 to 2000. We use a\n",
            "learning rate scheme consisting of a linear warmup from 0 to 2000 steps, followed by cosine decay.\n",
            "For the extra logits offset head, we use a two-layer MLP with ReLU nonlinearities and hidden size\n",
            "equal to one-quarter of the embedding size. The inputs to the layer are the position embeddings, the\n",
            "hidden values at the current position, the position id as an integer, and a Fourier embedding of the\n",
            "input position [31]. The output of the layer is added directly to the logits.\n",
            "For SequenceMatch, we keep a replay buffer of past generated sequences. The replay buffer is\n",
            "first-in-last-out, with the oldest sequences being replaced once the size of the buffer is reached. The\n",
            "size of the replay buffer is 10,000. We specify how many times each sequence in the replay buffer will\n",
            "appear in the training data before it is replaced (on average), and from that calculate how frequently\n",
            "a generation step must take place during training. We set the times each sequence will be seen in\n",
            "training to 8. The lengths of the prompts for the generated sequences used in training are randomized,\n",
            "chosen randomly from zero to half the context length. The completion is always set to continue up to\n",
            "the context length or <EOS> token.\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 167}\n",
            "chosen randomly from zero to half the context length. The completion is always set to continue up to\n",
            "the context length or <EOS> token.\n",
            "For the evaluation, we set the prompt length to be half the context length. We then generate sequences\n",
            "of length 256, for both the context length 512 and 1024. For the generation, we set the temperature\n",
            "to 1 and the top-p sampling to 1. However, for the increased length of the 1024 context length, we\n",
            "set the top-p parameter to 0.95. We use the default settings to evaluate the mauve score with 512\n",
            "samples.\n",
            "18Small Medium Large\n",
            "GPT2 Model Size100\n",
            "7×101\n",
            "8×101\n",
            "9×101\n",
            "Diversity Measure\n",
            "Diversity Score\n",
            "SequenceMatch (Full Model)\n",
            "Behavioral Cloning (Full Model)\n",
            "MLE (Full Model)\n",
            "SequenceMatch (LM Head)\n",
            "Behavioral Cloning (LM Head)\n",
            "MLE (LM Head)Figure 4: Diversity score [ 17] when fine-tuning GPT2 of various sizes on the openwebtext dataset\n",
            "with context length 512. Higher is better. Full and LM Head refer to which parts of the pretrained\n",
            "model are trained. We see that SequenceMatch outperforms behavioral cloning and MLE for a fixed\n",
            "model size. However, the fully trained models obtain lower scores than the variant only training the\n",
            "head.\n",
            "Fetch, Process Batch Grad. Step Sample Raw Grad. steps per Sample Total\n",
            "SM-(Small, Full) 0.3 ±0.05 2.2 ±0.02 10 ±2 16 3.1 ±0.5\n",
            "MLE-(Small, Full) 0.25 ±0.05 1.2 ±0.1 N/A N/A 1.45 ±0.15\n",
            "SM-(Large, LM Head) 0.3 ±0.05 4.9 ±0.1 33 ±5 12 7.95 ±0.7\n",
            "MLE-(Large, LM Head) 0.25 ±0.05 2.2 ±0.1 N/A N/A 2.45 ±0.15\n",
            "Table 2: Execution time of various parts of the training loop for the different models for the 512\n",
            "context length, χ2objective with model rollouts regularization. We show the raw time to sample a\n",
            "batch of trajectories, as well as the time for sampling once amortized due to the fact that we do not\n",
            "sample every training step.\n",
            "F Additional Plots\n",
            "In figure 4 we show the results for the diversity metric from [ 17]. We see that for a fixed model\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 168}\n",
            "sample every training step.\n",
            "F Additional Plots\n",
            "In figure 4 we show the results for the diversity metric from [ 17]. We see that for a fixed model\n",
            "size and type, the SequenceMatch models outperform the behavioral cloning and MLE baselines.\n",
            "A somewhat anomalous result is that the diversity is higher for the LM Head models, and fairly\n",
            "uniformly decreases as we increase the size of the models. This might indicate undertraining of the\n",
            "full models.\n",
            "G Overhead\n",
            "In this section we discuss the relative overhead of using the SequenceMatch training objective. We\n",
            "have additional overhead due to the necessity of sampling completions from the model. In addition,\n",
            "the actual loss has some additional complexity compared to the typical MLE loss, as we must extract\n",
            "the terminal values and compute a telescoping sum. Additionally, due to the fact that we use a batch\n",
            "of masks with a complex masking patterns, some GPU kernels that are specialized for causal masking\n",
            "cannot be utilized. However, we note that it’s not necessary to sample at every gradient step, since we\n",
            "can accumulate old trajectories in a replay buffer and re-use them. Furthermore, we can typically\n",
            "sample using a higher batch size than the gradient step batch size, so requiring fewer sampling steps\n",
            "per gradient step. In table 2 we show the breakdown of times necessary for a gradient step for each\n",
            "method. We see that the loss computation and gradient step typically takes around twice as long for\n",
            "the SequenceMatch objective than the MLE objective, due to the additional computation specified\n",
            "above. The sampling adds additional overhead. We note that the bottleneck from sampling could in\n",
            "principle be completely removed by adding a separate set of GPUs which independently generate\n",
            "sequences given the latest parameters and write to a buffer which is polled by the main training thread.\n",
            "Due to time constraints and the additional complexity of synchronizing separate processes, we did\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 169}\n",
            "Due to time constraints and the additional complexity of synchronizing separate processes, we did\n",
            "not implement this additional distributed training approach.\n",
            "H Additional Related Work\n",
            "H.1 Regularization Terms\n",
            "Due to the disadvantages of the KL-divergence discussed above, several additional training objectives\n",
            "have been proposed which take into account the model’s generated sequences. Particularly popular is\n",
            "19the (sequence-level) unlikelihood loss [37]. At step t, this is given by\n",
            "Lt\n",
            "ULS(Pθ(·|x1:t)) =Est+N∼Pθ(·|st)\n",
            "−t+NX\n",
            "k=t+1X\n",
            "c∈CkPθ(c|sk)\n",
            ",\n",
            "where Ckis a set of problematic outputs, such as repeated n-grams. The loss considers an N-step\n",
            "sequence generated from the model from stand penalizes repetitions. Although this regularization\n",
            "term performs well in practice, it relies on a heuristic that repetitive text is unnatural, which does not\n",
            "necessarily hold universally.\n",
            "However, the idea of sampling a sequence from the model and including that sequence in the loss is\n",
            "incorporated in SequenceMatch.\n",
            "I Examples\n",
            "In this section we give the first few samples from the evaluation set, with no cherry-picking, for the\n",
            "1024-context dataset and the SequenceMatch and MLE objectives. Note that we strip newlines in\n",
            "the examples and generations in order to present them on one page. At one point an unprintable\n",
            "character (btyes U+FFFD) was produced, which we write as (U+FFFD). We have two settings for the\n",
            "examples.\n",
            "In the first setting, we take the prompt sequence and change the last token to a random token\n",
            "drawn from the dataset. We see that the SequenceMatch model is able to utilize the <backspace>\n",
            "token immediately to remove the spurious token. The only case where this does not happen is for\n",
            "the prompt ... Clements disagreed particula , where the random token does make some\n",
            "semantic sense. In comparison, the MLE model cannot use the backspace and has a tendency to\n",
            "degenerate after a random token (particularly evident in the Toyota example).\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 170}\n",
            "semantic sense. In comparison, the MLE model cannot use the backspace and has a tendency to\n",
            "degenerate after a random token (particularly evident in the Toyota example).\n",
            "In the second setting, we give continuations on the evaluation dataset. We see that the SequenceMatch\n",
            "model uses the backspace token in the Toyota example with the generation skeptical of any\n",
            "measure<BKSPC> possible future partnerships . Therefore, we can see that the Sequence-\n",
            "Match model is able to learn to correct itself if it makes a generation that leads out of the distribution\n",
            "of typical sequences.\n",
            "20Table 3: Model Prompts and Completions for the 1024 SequenceMatch trained model. In this setting we add a\n",
            "random token at the end of the prompt and observe the behavior of the generated sequences.\n",
            "Prompt Completion\n",
            "<BOS>WHAT?!??! I know. That’s what you’re saying right now. \"WHAT?! DISNEY HAS A\n",
            "DONUT SUNDAE AND I DIDN’T KNOW ABOUT IT?!\" How do I know you’re saying that? Because\n",
            "that’s exactly what I was saying when Tina (<- amazing DFB photographer who’s frigging\n",
            "awesome) spotted this at the Plaza Ice Cream Parlor in the Magic Kingdom this week! But\n",
            "it’s OK. It’s brand new | not even on the menu yet | so we didn’t miss out on too much\n",
            "of the Donut Sundae lifespan. And we’re hoping that lifespan is a nice, long one! The\n",
            "Main Street Plaza Ice Cream Parlor can be found at the intersection of Main Street USA\n",
            "and Tomorrowland, just before you get to Cinderella Castle. And the sundae joins a few\n",
            "other must-have treats on the Ice Cream Parlor’s menu, including the house-made ice cream\n",
            "sandwich (preferably ordered with a drizzled sauce!), the \"kids’ cone\" (it’s totally OK to\n",
            "order this as a dysph<BKSPC> souvenir or fruit-eating piece of cake), and the Chesapeake Bay Memorial Park\n",
            "curmudgeonly ice cream truck (though don’t take our word for it). Just as some of the\n",
            "other ice cream parlor attractions on the internet can help (hopefully), you can visit\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 171}\n",
            "curmudgeonly ice cream truck (though don’t take our word for it). Just as some of the\n",
            "other ice cream parlor attractions on the internet can help (hopefully), you can visit\n",
            "the Ice Cream Parlor to find even more delicious fried Oreo sandwiches, the \"glass ice\n",
            "cream truck\" teddy bear (can’t have it either), the skipjack pooping \"free\" soda fountain,\n",
            "and the pyrotechnically gourmet ice cream fountain! Here’s the video of the Wrecking\n",
            "Ball, the Oreo pod dunk, and me having a little short squeeze last night. This also\n",
            "happened: the Lake Clark Brewery’s Donut Ice Cream Factory opened the new location back\n",
            "in 2015 and they’re rumored to be in the final phases of redevelopment. We hope you enjoy\n",
            "getting ahead of the news of their transition to the big comeback we expect to see in\n",
            "April.<|endoftext|>\n",
            "<BOS>A notorious protester convicted of wilfully promoting hatred against Muslims and\n",
            "criminally harassing a Muslim man and his family was sentenced Tuesday to nine months\n",
            "in jail. Eric Brazau handed out a flyer that \"vilified Muslims and disparages their\n",
            "religion,\" Ontario court Judge S. Ford Clements said in February, when he found Brazau\n",
            "guilty. Eric Brazau was convicted of willful promotion of hatred against Muslims and\n",
            "criminally harassing a Muslim family. ( CARLOS OSORIO / TORONTO STAR FILE PHOTO ) The case\n",
            "was far from being on the borderline between \"rough and tumble debate\" and hate speech, as\n",
            "Brazau had argued, Clements said in a College Park courtroom. Brazau handed out the flyer,\n",
            "which contained many offensive references to Islam and Muslims, in August and September\n",
            "2012. While distributing it, Brazau sometimes yelled obscenities about Islam \"in a tone of\n",
            "voice that suggested he was very angry and had little interest in debate,\" Clements said.\n",
            "Brazau had argued that he did not intend to promote hate speech; instead he wanted to\n",
            "stimulate debate about censorship, \"blasphemy laws\" and Sharia law, Clements said. Article\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 172}\n",
            "Brazau had argued that he did not intend to promote hate speech; instead he wanted to\n",
            "stimulate debate about censorship, \"blasphemy laws\" and Sharia law, Clements said. Article\n",
            "Continued Below Clements disagreed particulawith Brazau’s argument that people should not be allowed to speak in a respectful manner\n",
            "about public information and even the workings of mosques under the guise of religion. He\n",
            "said that parties to these principles must have their act out with civility training | in\n",
            "other words, someone is responsible to their location. Clements also agreed with Brazau’s\n",
            "counsel that these are disagreements within the boundaries of the courtroom. \"Based on the\n",
            "details in his core prosecution, the Attorney General has established that a search warrant\n",
            "was issued to his home located in the Brampton area of Greater Toronto prior to this arrest\n",
            "which led to a search warrant being executed,\" Clements wrote in his ruling. Clements\n",
            "added that prosecutors obtained emergency caution letters sent to him last week noting\n",
            "the violence in the community. Quebec Court Justice C. David Jones wrote for Clements\n",
            "that \"treat of hatred on members of a religion is a systematic part of a larger system of\n",
            "religious policing and a serious mistake.\" Jones said after Brazau’s conviction that his\n",
            "conviction would only worsen if a further trial was to be opened. A judge in the Woodin\n",
            "Regional Court recently recommended Brazau be held without bail to avoid a conviction or to\n",
            "avoid detention pending a potential appeal. Read more about:<|endoftext|>\n",
            "<BOS> ×Some Seattle businesses closed for ’A Day Without Immigrants’, but others decided\n",
            "against it SEATTLE | While there is no official list of local businesses participating\n",
            "in this movement, \"A Day Without Immigrants\", we did find some businesses that had closed\n",
            "their doors and posted signs up saying they were participating. In fact, one business\n",
            "owner says he’s doing things a little bit differently. Instead of shutting down, he says\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 173}\n",
            "their doors and posted signs up saying they were participating. In fact, one business\n",
            "owner says he’s doing things a little bit differently. Instead of shutting down, he says\n",
            "he is choosing to pay it forward. Please enable Javascript to watch this video Edward\n",
            "Moran moved from Mexico to the United States in 1984. He opened El Norte Lounge in Lake\n",
            "City about seven years ago. And instead of closing up shop along with many other immigrant\n",
            "small business owners across the nation, Eduardo was open for business Thursday. \"I don’t\n",
            "believe in closing a place just to protest,\" says Eduardo. Instead, he says he’s paying it\n",
            "forward. Eduardo is asking customers to pay in cash to avoid bank fees from credit card\n",
            "machines. He says he plans to donate a portion of his proceeds to a nonprofit that focuses\n",
            "on helping immigrants here in the Pacific Northwest. And he’s callingtotal<BKSPC> for a dialogue at least inside the city to try to bring pressure on the city to\n",
            "have equal access to immigrants. Steve Williams and Benjamin Romero noticed his business\n",
            "doors briefly ran out on Christmas and weekdays between Christmas and New Year’s. They\n",
            "say an important message is for businesses to consider the recent wave of protests that\n",
            "have erupted in Portland. From Mexico to the world with both women and children, signs are\n",
            "popping up throughout North America. The woman who shares an apartment with a family in\n",
            "Tacoma says all they want is a neighborly home and looks forward to a great life. Vincent\n",
            "French said when he first moved to Portland for work, people said he had been called in\n",
            "by the government and his mother. \"When I moved here in Tacoma, they said to me, ’You\n",
            "can help people who are a threat and you can help your neighbors help other people out,’\n",
            "\" French said. However, after receiving death threats online, he says his business was\n",
            "able to avoid those calls. \"I think they’ve finally decided, that’s great,\" he said.\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 174}\n",
            "\" French said. However, after receiving death threats online, he says his business was\n",
            "able to avoid those calls. \"I think they’ve finally decided, that’s great,\" he said.\n",
            "\"I want people to be there to keep order.\" However, Ani Hernandez said she hopes only\n",
            "business owners can address negative feelings in the community. \"When they start like\n",
            "this and start shutting down, people have to feel uncomfortable,\" she said. And just as\n",
            "there is no good news, some businesses are taking steps to control the negative emotions\n",
            "in the community. State of Washington state is going to hold a memorial service for\n",
            "anti-immigrant protesters at the Tenth Avenue and Aroznik Bridge. Click here to book a\n",
            "reservation.<|endoftext|>\n",
            "<BOS>Today, Toyota announced changes in executives’ areas of responsibility, as well as\n",
            "personnel changes at the sub-executive managerial level. The most important change by\n",
            "far is the appointment of Akio Toyoda, the company’s CEO and grandson of founder Kiichiro\n",
            "Toyoda, as President of a new ’EV Business Planning’ department. Earlier this month,\n",
            "we reported-admittedly a little tongue-in-cheek-about Toyota announcing the creation of\n",
            "an electric vehicle division and putting only 4 engineers on the project with the goal\n",
            "to bring EVs to market by 2020. The move seems a lot more serious now that Akio Toyoda\n",
            "is leading the effort, and several other executives, managers, and engineers have been\n",
            "assigned new responsibilities in the electric vehicle planning department, including the\n",
            "chief engineer of the Prius. At the executive level, the changes will be effective today,\n",
            "while the managers were apparently put on the program throughout the month. You can see\n",
            "the full list of changes below. It appears to be a clear sign that Toyota is more serious\n",
            "than ever about electric vehicles and it is not simply investing in fuel cell hydrogen to\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 175}\n",
            "the full list of changes below. It appears to be a clear sign that Toyota is more serious\n",
            "than ever about electric vehicles and it is not simply investing in fuel cell hydrogen to\n",
            "comply to new fuel consumption standards. Changes to executives’ areas of 113<BKSPC> responsibility require that Toyota - and its competitors - disclose further\n",
            "changes to their policies and practices as the larger EV business evolves. Toyota’s\n",
            "own EV business environment will be different from other companies which have recently\n",
            "been embracing a hybrid fleet. The company has not had a hybrid supply chain for years,\n",
            "and hybrid research research is not allowed inside its laboratories. That change will\n",
            "only come with the launch of this new integrated EV product division as the deal has not\n",
            "been finalized. The \"Reuturistic and Singular Communication Systems\" at Toyota are the\n",
            "industry’s most ambitious \"big guy\" partnerships. With that title and that vision first\n",
            "put to me, Toyota’s goal is to use a higher volume of vehicles and systems to market what’s\n",
            "best for consumers and the environment, rather than market what’s worst for them. For that\n",
            "reason, the company is opening the first car dealerships in the United States, and is doing\n",
            "so as the name suggests. There is no plan to extend out its RAV4 EV program beyond the\n",
            "University of Arizona system in Reno. While some groups like the Australian Automobile\n",
            "Club consider this as an important move, any EV owners who will be conclining to a hybrid\n",
            "infrastructure ought to be aware the company recognizes the average demand and needs\n",
            "of those vehicles and systems: With more than 300,000 vehicles on the road now, a few\n",
            "hundred more - including \"Prix Super Super 500 Hybrid\" - are being phased out. Consumers\n",
            "are more willing to hop on the electric car bandwagon than at any time in history. The\n",
            "shift to EV mandates has become what almost everyone has been dying to do for decades;\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 176}\n",
            "are more willing to hop on the electric car bandwagon than at any time in history. The\n",
            "shift to EV mandates has become what almost everyone has been dying to do for decades;\n",
            "shake off the bad habits of all regulatory agencies, deal with this as a new and necessary\n",
            "trend and transform the EV industry. Japan is still a new market after over 100 years\n",
            "of hard-charging and combustion engines. Now, we’ve seen the biggest transformation in\n",
            "our world - a future where EVs are sold by taxis, where vehicles are tracked and traded\n",
            "for cash. Existing customers, policy makers, and private sector know the EV space is big\n",
            "and we want to help. The new EV environment provides great opportunities and more rules,\n",
            "policies, and practices. Top 10 Reuturistic Energy Hybrid Designs Current LEAF motor\n",
            "vehicles are sold by Volkswagen, Toyota and Jaguar Land Rover, but very few cars contain or\n",
            "come close to this level of use. Over <|endoftext|>\n",
            "<BOS>North Korean leader Kim Jong Un. AP Images / Business Insider North Korea attempted\n",
            "to fire a missile Sunday, but it blew up within seconds. It happened one day after the\n",
            "anniversary of the country’s founding. While North Korea’s missile program may be the\n",
            "shadowiest on earth, it’s possible that US cyber warriors were the reason for the failed\n",
            "launch. A recent New York Times report uncovered a Kham secret operation to derail\n",
            "North Korea’s nuclear-missile program that has been raging for at least three years.\n",
            "Essentially, the report attributes North Korea’s high rate of failure with Russian-designed\n",
            "missiles to the US meddling in the country’s missile software and networks. Although North\n",
            "Korea’s missile infrastructure lacks the competence of Russia’s, the Soviet-era missile on\n",
            "which North Korea based its missile had a 13 rotten% failure rate, and the North Korean\n",
            "version failed a whopping 88% of the time, according to the report. While the missile\n",
            "failure on Sunday could have just been due to poor workmanship, US Deputy National Security\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 177}\n",
            "version failed a whopping 88% of the time, according to the report. While the missile\n",
            "failure on Sunday could have just been due to poor workmanship, US Deputy National Security\n",
            "Adviser K.T. McFarland seemed to leave room for speculation about espionage, telling Fox\n",
            "News, \"\"We can’t talk about secret intelligence and things that might have been done ${}<BKSPC> to try to secure a foreign adversary or cause a serious injury or insurrection\n",
            "in North Korea. We could just slip the word \"\"secrecy\"\" into the Cold War.\"\" Kham has\n",
            "published an outline of the failed missile call, with an attack message from an unknown\n",
            "country (the United States) that says, \"\"Kim Jong Un was able to detonate over as many as\n",
            "30 interceptor missiles in just a single second.\"\" The most realistic hypothesis seems to\n",
            "be that the North was firing missile-defence missiles somewhere north of the DMZ, and that\n",
            "North Korea detonated one and possibly two \"\"missiles.\"\" Also buried on the NPA resume is\n",
            "part of the chart above, which suggests that North Korea was supposed to have advanced its\n",
            "satellite, but that it wasn’t actually meant to attack, and that Kim Jong Un deployed his\n",
            "forces through the DMZ in preparation for its full-scale invasion of the United States.\n",
            "The NPA suggestion that Kim Jong Un was behind the North Korean missile launch has been a\n",
            "theme in numerous US policy meetings over the past year. President Donald Trump hosted\n",
            "President Kim Jong Un, who has since spoken with South Korea’s President Moon Jae-in on\n",
            "the importance of dialogue. The face-to-face talks between Trump and Trump have so far\n",
            "resulted in multiple calls for \"\"constructive, constructive joint action.\"\"On the campaign\n",
            "trail, Trump promised to ramp up efforts to tackle Pyongyang’s nuclear program, and claimed\n",
            "that North Korean leader Kim Jong Un was behind the misdeeds of his country. In response,\n",
            "Trump’s new administration has shown itself to be actively talking about North Korean\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 178}\n",
            "that North Korean leader Kim Jong Un was behind the misdeeds of his country. In response,\n",
            "Trump’s new administration has shown itself to be actively talking about North Korean\n",
            "diplomacy, telling the American community last month that \"\"North Korea is a wonderful,\n",
            "great nation. It has turned several dangerous institutions into havens of radicalism and\n",
            "terrorism, and I am confident that North Korea is also prepared to launch a strike and\n",
            "destroy any attempts to stop and shut down its programme. We must put forward options and\n",
            "seek a comprehensive, comprehensive, and comprehensive response.\"\" North Korea had clearly\n",
            "misreported the strategic consequences of its ballistic missile launch Sunday. A missile\n",
            "does not threaten the US or the South; it appears to have fallen from the sky. In fact,\n",
            "North Korea has already launched about 95,000 missiles in less than a month.<|endoftext|>Table 4: Model Prompts and Completions for the 1024 MLE trained model. In this setting we add a random\n",
            "token at the end of the prompt and observe the behavior of the generated sequences. Continued on next page\n",
            "Prompt Completion\n",
            "WHAT?!??! I know. That’s what you’re saying right now. \"WHAT?! DISNEY HAS A DONUT\n",
            "SUNDAE AND I DIDN’T KNOW ABOUT IT?!\" How do I know you’re saying that? Because that’s\n",
            "exactly what I was saying when Tina (<- amazing DFB photographer who’s frigging awesome)\n",
            "spotted this at the Plaza Ice Cream Parlor in the Magic Kingdom this week! But it’s\n",
            "OK. It’s brand new | not even on the menu yet | so we didn’t miss out on too much of\n",
            "the Donut Sundae lifespan. And we’re hoping that lifespan is a nice, long one! The\n",
            "Main Street Plaza Ice Cream Parlor can be found at the intersection of Main Street USA\n",
            "and Tomorrowland, just before you get to Cinderella Castle. And the sundae joins a few\n",
            "other must-have treats on the Ice Cream Parlor’s menu, including the house-made ice cream\n",
            "sandwich (preferably ordered with a drizzled sauce!), the \"kids’ cone\" (it’s totally OK to\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 179}\n",
            "other must-have treats on the Ice Cream Parlor’s menu, including the house-made ice cream\n",
            "sandwich (preferably ordered with a drizzled sauce!), the \"kids’ cone\" (it’s totally OK to\n",
            "order this as a grown ColdOnion Roll, since kids eat cake too!), and good ol’ Oldman’s Sweet Ice Cream based Mini\n",
            "Six Peanut Butter Cups, as well as brand new Oreos Made from Oakwood. Anyway, we digress.\n",
            "So basically, we did a little code-changing this afternoon, and it turns out that we’ll\n",
            "be doing the full-on \"YIMBA\" package once again. Nope, nobody added New Master Cake\n",
            "(we’ll have to share your images), and you’re walking back your welcome to Polynesian\n",
            "macen as much as possible! Also, just to reiterate, all 10 of our Frozen Shorts! You\n",
            "see, we received one gift from Disney with a date of March 20. No luck deciding to use\n",
            "another date. No silly excuse, just a sense of nostalgia. See ya all next time! (This\n",
            "pattern has been gifted to us by Original Couture Forestes, which are mostly adorable\n",
            "because they totally turned out their own thing!) Please feel free to tell us what you\n",
            "think in the comments below...we’ll see you all on. But if you have any other ideas, let\n",
            "us know and we’ll try to make them a reality in the future, too! (C) Original Couture\n",
            "Forestes & Haircutters, YAMAHA Haircutters, and Innovative Mesh, all of whom deserve the\n",
            "award for this awesome Wild Forest creativity. For more casual fan-inspired pixel-art,\n",
            "check out these indie-design throwback images from the Splendour Of Sound booth! Look\n",
            "around at this awesome hidden gem. You’re never going to be alone with these work done\n",
            "by YOU! Other [And This Has When You’re on a Mission?!] samples from above or click the\n",
            "INVERSELEads button below to explore [Colourful Imagery]. [Colourful Imagery samples] by\n",
            "Chloe Yee. Quite a handful of amazing Red Solo was shown at the Smithsonian in June 2009.\n",
            "Have some of your favorite characters and locations be inspired by their design and art!\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 180}\n",
            "Chloe Yee. Quite a handful of amazing Red Solo was shown at the Smithsonian in June 2009.\n",
            "Have some of your favorite characters and locations be inspired by their design and art!\n",
            "[Colourful Imagery samples] by Karen Moore. This series uses heart modeled Granny Smith\n",
            "cruelly, deformed words, and her favorite turquoise character Digby Pie. It’s set in\n",
            "these specially designed Styrofoam turquoise silks, assembled by the construction company\n",
            "at\n",
            "A notorious protester convicted of wilfully promoting hatred against Muslims and\n",
            "criminally harassing a Muslim man and his family was sentenced Tuesday to nine months\n",
            "in jail. Eric Brazau handed out a flyer that \"vilified Muslims and disparages their\n",
            "religion,\" Ontario court Judge S. Ford Clements said in February, when he found Brazau\n",
            "guilty. Eric Brazau was convicted of willful promotion of hatred against Muslims and\n",
            "criminally harassing a Muslim family. ( CARLOS OSORIO / TORONTO STAR FILE PHOTO ) The\n",
            "case was far from being on the borderline between \"rough and tumble debate\" and hate\n",
            "speech, as Brazau had argued, Clements said in a College Park courtroom. Brazau handed\n",
            "out the flyer, which contained many offensive references to Islam and Muslims, in August\n",
            "and September 2012. While distributing it, Brazau sometimes yelled obscenities about\n",
            "Islam \"in a tone of voice that suggested he was very angry and had little interest in\n",
            "debate,\" Clements said. Brazau had argued that he did not intend to promote hate speech;\n",
            "instead he wanted to stimulate debate about censorship, \"blasphemy laws\" and Sharia law,\n",
            "Clements said. Article Continued Below Clements disagreed.agos: It wasn’t only when an anti-Muslim protester was convicted. Clements said 2017 was\n",
            "unique because his term gives Brazau the opportunity to answer: \"Qaads ben Yehudi: Where\n",
            "are the ’deplorables’ and who are the ’parallel struggles?’\" after the jihad. Clements\n",
            "also said he found Bansaris and Jolyon Friesen guilty of helping incite hatred and hate.\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 181}\n",
            "are the ’deplorables’ and who are the ’parallel struggles?’\" after the jihad. Clements\n",
            "also said he found Bansaris and Jolyon Friesen guilty of helping incite hatred and hate.\n",
            "Zean Korah, a Supreme Court of Canada judge and former face of hate speech at the Toronto\n",
            "detention centre, will hold a preliminary hearing at Wednesday’s district court, Clements\n",
            "said. Article Continued Below Zean Joeberg will be heading up an \"anti-fascist group\";\n",
            "as Ziya Janette Sharma will be in charge of the prosecution, Clements agreed. He said\n",
            "he didn’t anticipate Clements making such a verdict in the light of events in Ferguson,\n",
            "Mo., and that findings \"are not to be taken lightly,\" Clements said. \"It’s the judge’s\n",
            "job to look like we’ve done well in addressing this issue,\" Clements said. The judge\n",
            "sentenced Brazau to nine months in prison, awarding him $500,000 for fighting the charges.\n",
            "The sentence came after a two-day trial in Toronto where he stood with fellow protesters\n",
            "in front of Chief of Staff at the Toronto Police Service, then endorsed the suspect\n",
            "in the flagrant breach of secrecy. He was scheduled to appear in St. John’s Superior\n",
            "Court on Monday. During the trial, meanwhile, some of his accusers sat in on the most\n",
            "contentious and contested event of his life, the lawyer arguing that the judge should\n",
            "have subjected Brazau to such an intense chilling procedure that could have been triggered\n",
            "by his admission of hate speech. He also called the reality of being falsely accused by\n",
            "the St. John’s Residents Association of mistreating his client a \"victory\" instead of\n",
            "\"a blow to his reputation.\" Clements said the judge was aware of so many anti-jihadary\n",
            "developments as he had been studying the case on a trip to the New York area last month.\n",
            "But he rejected allegations of political bias, saying that Brazau’s remarks were \"symbolic\n",
            "and ironic.\" Clements took issue with allegations of the judge condoning violence, saying:\n",
            "\"There’s some type!\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 182}\n",
            "and ironic.\" Clements took issue with allegations of the judge condoning violence, saying:\n",
            "\"There’s some type!\n",
            "×Some Seattle businesses closed for ’A Day Without Immigrants’, but others decided\n",
            "against it SEATTLE | While there is no official list of local businesses participating in\n",
            "this movement, \"A Day Without Immigrants\", we did find some businesses that had closed\n",
            "their doors and posted signs up saying they were participating. In fact, one business\n",
            "owner says he’s doing things a little bit differently. Instead of shutting down, he\n",
            "says he is choosing to pay it forward. Please enable Javascript to watch this video\n",
            "Edward Moran moved from Mexico to the United States in 1984. He opened El Norte Lounge\n",
            "in Lake City about seven years ago. And instead of closing up shop along with many other\n",
            "immigrant small business owners across the nation, Eduardo was open for business Thursday.\n",
            "\"I don’t believe in closing a place just to protest,\" says Eduardo. Instead, he says he’s\n",
            "paying it forward. Eduardo is asking customers to pay in cash to avoid bank fees from\n",
            "credit card machines. He says he plans to donate a portion of his proceeds to a nonprofit\n",
            "that focuses on helping immigrants here in the Pacific Northwest. And he’s calling on\n",
            "finalistsin the project to pay back what he’s paid on every purchase. Do you believe Eduardo’s\n",
            "message to businesses | donate, pay their employees back and help other immigrants live\n",
            "in dignity? How about more \"A Day Without Immigrants\"? Is it possible you could help\n",
            "encourage the nation to do something like Eduardo’s? Helping immigrants live in dignity\n",
            "isn’t the goal of the group: To give free tuition to illegal aliens. But Eduardo\n",
            "believes immigrants can be welcome here as long as they pay taxes. \"Do we need any\n",
            "more illegals in our country? No. We don’t,\" says Eduardo. District Judge Lucy Kohls\n",
            "says school district officials should remove the \"A Day Without Immigrants\" sign from\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 183}\n",
            "more illegals in our country? No. We don’t,\" says Eduardo. District Judge Lucy Kohls\n",
            "says school district officials should remove the \"A Day Without Immigrants\" sign from\n",
            "their annual accreditation program, which requires every high school student to teach\n",
            "English. But according to Korea-based Senator Hillary Schakowsky, this group has started\n",
            "competing. \"They seem to be suffering from stigma,\" says Schakowsky. \"I’m hoping the\n",
            "courts will find them to be ashamed of themselves for what they’ve done and to cast\n",
            "them in the negative light, if not as a group who would be well served to get on the\n",
            "national stage.\" Lynn Horcher says she hopes that Eduardo’s message can inspire parents\n",
            "and businesses to break down the walls around immigrants, without fear for their safety\n",
            "and to break out of fear that those still working for immigrants will be jailed, beaten\n",
            "and deported. \"I’m so sorry,\" says Lynch. \"Those who truly want to do this were always\n",
            "called ’Daddy Minjies.’ I say that as a parent who thinks that when your parents leave\n",
            "you need to do something that they really, really feel guilty about because you were\n",
            "born here.\" Witness the violence, burning and pickings, and watch the protesters packing\n",
            "tents for three days. Could immigrants be shut down for protest or collect your money?\n",
            "Photo Credit: SimplePhotographer.com Follow @ChrisBreaux on Twitter. This article was\n",
            "originally published on The Conversation. Read the original story. [h/t: Trevor Wright]\n",
            "Download our NEW Occupy Democrats app for your iPhone by clicking here or for your Android\n",
            "by clicking here. Add your name to millions demanding that!\n",
            "Today, Toyota announced changes in executives’ areas of responsibility, as well as\n",
            "personnel changes at the sub-executive managerial level. The most important change by\n",
            "far is the appointment of Akio Toyoda, the company’s CEO and grandson of founder Kiichiro\n",
            "Toyoda, as President of a new ’EV Business Planning’ department. Earlier this month,\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 184}\n",
            "far is the appointment of Akio Toyoda, the company’s CEO and grandson of founder Kiichiro\n",
            "Toyoda, as President of a new ’EV Business Planning’ department. Earlier this month,\n",
            "we reported-admittedly a little tongue-in-cheek-about Toyota announcing the creation of\n",
            "an electric vehicle division and putting only 4 engineers on the project with the goal\n",
            "to bring EVs to market by 2020. The move seems a lot more serious now that Akio Toyoda\n",
            "is leading the effort, and several other executives, managers, and engineers have been\n",
            "assigned new responsibilities in the electric vehicle planning department, including\n",
            "the chief engineer of the Prius. At the executive level, the changes will be effective\n",
            "today, while the managers were apparently put on the program throughout the month. You\n",
            "can see the full list of changes below. It appears to be a clear sign that Toyota is\n",
            "more serious than ever about electric vehicles and it is not simply investing in fuel cell\n",
            "hydrogen to comply to new fuel consumption standards. Changes to executives’ areas of\n",
            "responsibilityJohnSchmitt, Managing Director of Toyota Chris Koscarino, EV Business Planner Michelle Swahira,\n",
            "EV Executive Manager, New York Mark Page, Assistant Managing Director, Toyota Mathew\n",
            "Kennedy, EV Business Planner David Gustavsson, EV Manager, New York Leigh Munro, EV\n",
            "Manager, San Jose Ruthen Friedrich, EV Executive President, Califasto Matt Keysbury, EV\n",
            "Managing Director, Toyota Rahla Baldus, EV Management, Nippon Professional Grooming Club,\n",
            "Tokyo Let’s keep it human for a minute. Just kidding, Toyota wasn’t kidding about the new\n",
            "gig, when they announced that the first 100,000 electric vehicles will be hitting stores\n",
            "across the country in two years, and that the car maker wants to serve as \"the world’s\n",
            "largest car retailer.\" See also. Ah yes, the EV is finally relevant and the rest is\n",
            "history. Toyota President Michio Kaku and CEO Akio Toyoda are turning heads and actively\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 185}\n",
            "largest car retailer.\" See also. Ah yes, the EV is finally relevant and the rest is\n",
            "history. Toyota President Michio Kaku and CEO Akio Toyoda are turning heads and actively\n",
            "promoting the sport-class EV during the various global celebrations, and even this week,\n",
            "unveiling the new 2018 Plug-In Hybrid Transit (EV.SE), which will have a new 4.0-liter,EV\n",
            "powertrain, a plug-in hybrid. It will arrive on the market in September, and will be\n",
            "the first-ever plug-in hybrid with a highway-compliant full-size sedan in Japan. The\n",
            "new sedan will offer \"bump-in\" emissions-calming technology, making the EV more efficient\n",
            "than its rivals. The fleet should start rolling out in 2017, and even here at EV Trade,\n",
            "we encourage most prospective car buyers to take the time to check our website for what\n",
            "exactly is coming to market. You don’t want your Sonata on the road for only a short week,\n",
            "in the European market? Source: iSupermarket [Source: iSupermarket ]] Source: Quora\n",
            "Source: Quora Source: Google+ Source: Google+ Source: Quora Source: Google+ Source:\n",
            "Quora Source: Quora Source: Google+ Source: Google+ Source: Quora Source: Google+\n",
            "Source: Quora Source: Quora Source: Quora Source:Table 5: Last Model Prompt and Completion for the 1024 MLE trained model. In this setting we add a random\n",
            "token at the end of the prompt and observe the behavior of the generated sequences. Continued from previous\n",
            "page\n",
            "Prompt Completion\n",
            "North Korean leader Kim Jong Un. AP Images / Business Insider North Korea attempted\n",
            "to fire a missile Sunday, but it blew up within seconds. It happened one day after\n",
            "the anniversary of the country’s founding. While North Korea’s missile program may\n",
            "be the shadowiest on earth, it’s possible that US cyber warriors were the reason for\n",
            "the failed launch. A recent New York Times report uncovered a secret operation to\n",
            "derail North Korea’s nuclear-missile program that has been raging for at least three\n",
            "years. Essentially, the report attributes North Korea’s high rate of failure with\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 186}\n",
            "derail North Korea’s nuclear-missile program that has been raging for at least three\n",
            "years. Essentially, the report attributes North Korea’s high rate of failure with\n",
            "Russian-designed missiles to the US meddling in the country’s missile software and\n",
            "networks. Although North Korea’s missile infrastructure lacks the competence of Russia’s,\n",
            "the Soviet-era missile on which North Korea based its missile had a 13% failure rate,\n",
            "and the North Korean version failed a whopping 88% of the time, according to the report.\n",
            "While the missile failure on Sunday could have just been due to poor workmanship, US\n",
            "Deputy National Security Adviser K.T. McFarland seemed to leave room for speculation about\n",
            "espionage, telling Fox News, \"We can’t talk about secret intelligence and things that\n",
            "might have been done, covert operations scenically.\" The revelation of the secrets follow months of effort to downplay hacking\n",
            "and sabotage that has divided the country, in which North Korean cyber warriors fought\n",
            "militarily against regime change. By overthrowing the leadership of Kim Jong Un, North\n",
            "Korea has breached the territory of the United States and other big nations, as well as\n",
            "a vital diplomatic facility in Shanghai, a large regional hub, Reuters reported. North\n",
            "Korea’s attempts to sabotage Pyongyang first came to light on April 1, when it carried out\n",
            "a missile test. But even North Korea’s leaders were quick to pinpoint the launch as the\n",
            "likely result of cyber-attacks. An overwhelming majority of Pyongyang’s elite are U.S.\n",
            "intelligence, with Kim deriding Washington’s cyber security arrangements and blaming the\n",
            "country’s Soviet enemies. On Sunday’s anniversary, a cyberwarfare specialist named Kim\n",
            "Hyon Hyon conducted a test firing of the Terminal High Altitude Area Defense interceptor\n",
            "that was his main target. US intelligence officials have told Fox News that Kim Kim\n",
            "planned to enter South Korea last Friday to try and reset relations, Yonhap reported.\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 187}\n",
            "that was his main target. US intelligence officials have told Fox News that Kim Kim\n",
            "planned to enter South Korea last Friday to try and reset relations, Yonhap reported.\n",
            "[And Trump is the North’s top diplomat now] The cyber attack also likely broke out at a\n",
            "door for Kim Jong Un’s brother, Ri Yong Ho, who seems to be a devoted leader. The South\n",
            "Korean leader responded by comparing the attack on the research institute to the attacks\n",
            "on the USS Cole airbase in Yemen that killed 9 sailors during the attack. Kim is the\n",
            "country’s previous leader from 1990- 1994. Kim Jong Un’s brother, Ri Jong Il, passed away\n",
            "in 1994, but North Korea remains very close to Ri’s family and maintains a high ranking.\n",
            "By Sunday, Jong Il had tweeted, \"A war hell war!\" North Korea test fired at Ichiwa port.\n",
            "Pesatime (New York/New York) | Ri Jong Il (@ RiJi) December 10, 2016 Later, some of North\n",
            "Korea’s scientists released a book about Ri, promising to \"break his nuclear arms\" and\n",
            "help their country prevent a nuclear war. In March, Sony Hackings Interview Kim Jong\n",
            "Un and Other Executives Are Separating Their Values From Their Intelligence Perceptions.\n",
            "They are just now getting their heads around the Kim Jong Un decision. | Sputnik News\n",
            "(@SputnikNews) December 13, 2016 Before that, North Korea had been largely on alert,\n",
            "determined to achieve a nuclear war. Kim launched!Table 6: Model Prompts and Completions for the 1024 SequenceMatch trained model. In this setting we simply\n",
            "generate sequences based on the prompt\n",
            "Prompt Completion\n",
            "<BOS>WHAT?!??! I know. That’s what you’re saying right now. \"WHAT?! DISNEY HAS A\n",
            "DONUT SUNDAE AND I DIDN’T KNOW ABOUT IT?!\" How do I know you’re saying that? Because\n",
            "that’s exactly what I was saying when Tina (<- amazing DFB photographer who’s frigging\n",
            "awesome) spotted this at the Plaza Ice Cream Parlor in the Magic Kingdom this week!But\n",
            "it’s OK. It’s brand new | not even on the menu yet | so we didn’t miss out on too much\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 188}\n",
            "awesome) spotted this at the Plaza Ice Cream Parlor in the Magic Kingdom this week!But\n",
            "it’s OK. It’s brand new | not even on the menu yet | so we didn’t miss out on too much\n",
            "of the Donut Sundae lifespan. And we’re hoping that lifespan is a nice, long one! The\n",
            "Main Street Plaza Ice Cream Parlor can be found at the intersection of Main Street USA\n",
            "and Tomorrowland, just before you get to Cinderella Castle. And the sundae joins a few\n",
            "other must-have treats on the Ice Cream Parlor’s menu, including the house-made ice cream\n",
            "sandwich (preferably ordered with a drizzled sauce!), the \"kids’ cone\" (it’s totally OK to\n",
            "order this as a grown\" adult adult! But NOT on a Snickers bar, too!), the kikos and curries (unless you use\n",
            "all the glaze from other frozen items in the store), and absolutely everything | as well\n",
            "as the desserts and ice cream itself. Guests can share their undie sundae or a gummy\n",
            "mummy figurine with the kiosk. Happy Butterfingers, and thanks for checking out the best\n",
            "all summer online.<|endoftext|>\n",
            "<BOS>A notorious protester convicted of wilfully promoting hatred against Muslims and\n",
            "criminally harassing a Muslim man and his family was sentenced Tuesday to nine months\n",
            "in jail. Eric Brazau handed out a flyer that \"vilified Muslims and disparages their\n",
            "religion,\" Ontario court Judge S. Ford Clements said in February, when he found Brazau\n",
            "guilty. Eric Brazau was convicted of willful promotion of hatred against Muslims and\n",
            "criminally harassing a Muslim family. ( CARLOS OSORIO / TORONTO STAR FILE PHOTO ) The\n",
            "case was far from being on the borderline between \"rough and tumble debate\" and hate\n",
            "speech, as Brazau had argued, Clements said in a College Park courtroom. Brazau handed\n",
            "out the flyer, which contained many offensive references to Islam and Muslims, in August\n",
            "and September 2012. While distributing it, Brazau sometimes yelled obscenities about\n",
            "Islam \"in a tone of voice that suggested he was very angry and had little interest in\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 189}\n",
            "and September 2012. While distributing it, Brazau sometimes yelled obscenities about\n",
            "Islam \"in a tone of voice that suggested he was very angry and had little interest in\n",
            "debate,\" Clements said. Brazau had argued that he did not intend to promote hate speech;\n",
            "instead he wanted to stimulate debate about censorship, \"blasphemy laws\" and Sharia law,\n",
            "Clements said. Article Continued Below Clements disagreed.\"\"He presented that he could not create the views or arguments required in terms of hatred\n",
            "which I thought would promote even greater debate,\" Clements said of Brazau. Brazau’s\n",
            "lawyers have accused him of lying to get away with distributing offensive flyers to\n",
            "delegitimize a mosque location. Clements said Brazau wasn’t guilty of participating in\n",
            "the defacement of mosque equipment. He said because of his alleged victimization in an\n",
            "anti-Muslim message for months, the flyer has become an \"internal crusade against the\n",
            "fundamental rights of our Muslim community.\" He said, although Brazau received laments\n",
            "for his actions, police did not find any unlawful activity during the June 2012 incident.\n",
            "\"They could not seem to distinguish that the flyer has done anything illegal,\" Clements\n",
            "said. \"We did find up-close images of the flyer at a nearby parking lot on June 12,\n",
            "which is a stunning part of what happened.\" The flyer said he had been planning to post\n",
            "an editorial about the case on his website in January. He wrote a memo outlining how\n",
            "his comments to his prior remarks had affected his views on Islam. He told a friend at\n",
            "one point that \"I was trying to add as much drama as possible\" to Muslim accusations he\n",
            "had received about his recent conduct, Clements said, but Brazau’s actions attracted too\n",
            "much attention. Article Continued Below He denied defacing the apartment they shared\n",
            "with his brother. He was sentenced to 15 months’ probation and ordered to pay $10,000\n",
            "in restitution to his victims. Mesher Lalaini, the lawyer who represented Brazau on\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 190}\n",
            "with his brother. He was sentenced to 15 months’ probation and ordered to pay $10,000\n",
            "in restitution to his victims. Mesher Lalaini, the lawyer who represented Brazau on\n",
            "Tuesday, said he was appalled that police had failed to bring him to trial. However,\n",
            "he said his client told the judge he couldn’t believe that police were complicit in the\n",
            "attack.<|endoftext|>\n",
            "<BOS> ×Some Seattle businesses closed for ’A Day Without Immigrants’, but others decided\n",
            "against it SEATTLE | While there is no official list of local businesses participating in\n",
            "this movement, \"A Day Without Immigrants\", we did find some businesses that had closed\n",
            "their doors and posted signs up saying they were participating. In fact, one business\n",
            "owner says he’s doing things a little bit differently. Instead of shutting down, he\n",
            "says he is choosing to pay it forward. Please enable Javascript to watch this video\n",
            "Edward Moran moved from Mexico to the United States in 1984. He opened El Norte Lounge\n",
            "in Lake City about seven years ago. And instead of closing up shop along with many\n",
            "other immigrant small business owners across the nation, Eduardo was open for business\n",
            "Thursday.\"I don’t believe in closing a place just to protest,\" says Eduardo. Instead, he\n",
            "says he’s paying it forward. Eduardo is asking customers to pay in cash to avoid bank\n",
            "fees from credit card machines. He says he plans to donate a portion of his proceeds to\n",
            "a nonprofit that focuses on helping immigrants here in the Pacific Northwest. And he’s\n",
            "calling onSeattle to take steps to address this injustice. \"As some of these things have happened\n",
            "... I would like to do something a little different,\" says Eduardo. Eduardo took the day\n",
            "off from work Thursday. \"I believe an important moment is for us to really make this what\n",
            "we are,\" says Ed. From Mexico to the United States, there are dozens of signs supporting\n",
            "immigrants who live here. Eduardo doesn’t fear that his business is destroyed because\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 191}\n",
            "we are,\" says Ed. From Mexico to the United States, there are dozens of signs supporting\n",
            "immigrants who live here. Eduardo doesn’t fear that his business is destroyed because\n",
            "his business is shutting down due to currency restrictions. \"I didn’t just start out as\n",
            "a business - I just made it money,\" says Eduardo. Eduardo says a lot of people here in\n",
            "the U.S. are immigrants to this country. He says there is people who have a much larger\n",
            "family standing by as they help other immigrants out.<|endoftext|>\n",
            "<BOS>Today, Toyota announced changes in executives’ areas of responsibility, as well as\n",
            "personnel changes at the sub-executive managerial level. The most important change by\n",
            "far is the appointment of Akio Toyoda, the company’s CEO and grandson of founder Kiichiro\n",
            "Toyoda, as President of a new ’EV Business Planning’ department. Earlier this month,\n",
            "we reported-admittedly a little tongue-in-cheek-about Toyota announcing the creation of\n",
            "an electric vehicle division and putting only 4 engineers on the project with the goal\n",
            "to bring EVs to market by 2020. The move seems a lot more serious now that Akio Toyoda\n",
            "is leading the effort, and several other executives, managers, and engineers have been\n",
            "assigned new responsibilities in the electric vehicle planning department, including the\n",
            "chief engineer of the Prius. At the executive level, the changes will be effective today,\n",
            "while the managers were apparently put on the program throughout the month. You can see\n",
            "the full list of changes below. It appears to be a clear sign that Toyota is more serious\n",
            "than ever about electric vehicles and it is not simply investing in fuel cell hydrogen to\n",
            "comply to new fuel consumption standards. Changes to executives’ areas of responsibility\"(top image): Geneshiro Toyoda as President Akio Toyoda as CEO The company is stepping\n",
            "into a new era within the industry. Toyota will be taking a significant step forward\n",
            "moving forward with the program, along with all of the veteran EV experts who will be\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 192}\n",
            "into a new era within the industry. Toyota will be taking a significant step forward\n",
            "moving forward with the program, along with all of the veteran EV experts who will be\n",
            "on board. Toyota will invest in another budget program with Nissan, and will also\n",
            "spend close to 2 percent more in this effort. Late last week, Toyota said the automaker\n",
            "is seeking federal money to launch and develop a Phase I funding program to support\n",
            "the Toyota SSP. While many people are skeptical of any measure<BKSPC> possible future\n",
            "partnerships, Toyota was confident that the group will be determined to get the program\n",
            "off the ground by 2020, and that a program would be here before too long. Toyota’s\n",
            "current projections indicate it will acquire Mitsubishi A-brand engines to support its\n",
            "No. 2 seat car, which will supply approximately 220,000 electric vehicles in 2020. The\n",
            "company is expected to have expanded the name in 2017, by launching a new hydrogen fuel\n",
            "cell vehicle between 2021 and 2023. Ralph Hisefner as CEO New initiatives on Toyota’s\n",
            "Level 3 Vision Vision Future Systems Competition have been revealed today, but they are\n",
            "being designed to launch new development capabilities in this rapidly approaching market.\n",
            "By doing so, Toyota will be taking a step closer to the promise of wide-mileage electric\n",
            "vehicles, a goal already put to the ’Pim Super Super 500 Hybrid’ and the Toyota Vision\n",
            "Future technologies for the Toyota platform. Hyundai Tribute Pack Member (subsequently\n",
            "name changed): BMW i3 BMW i3 Gran Coupe SSI Kortis Accordingly, Toyota is assembling\n",
            "the brand S6, and with that said, even the really big crossover may be made by this new\n",
            "entrant. As stated in a Japanese blog: The KC-BY is forecast to surpass the 2012 ’Konami\n",
            "HD’ as the global hybrid vehicle market continues to grow. Toyota aims for the 2020\n",
            "model to be able to extend global road segment recognition and hybrid vehicle capabilities\n",
            "to the top tier of the vehicle. The new vehicle that utilizes self-driving technology\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 193}\n",
            "model to be able to extend global road segment recognition and hybrid vehicle capabilities\n",
            "to the top tier of the vehicle. The new vehicle that utilizes self-driving technology\n",
            "developed jointly in Toyota and BMW technology is expected to be available at the Tokyo\n",
            "Motor Show in March. The vehicle’s high-volume powertrain and low-land temperature drive\n",
            "technology combined with industry leading regenerative braking technology will enable use\n",
            "in some\"\n",
            "<BOS>North Korean leader Kim Jong Un. AP Images / Business Insider North Korea attempted\n",
            "to fire a missile Sunday, but it blew up within seconds. It happened one day after\n",
            "the anniversary of the country’s founding. While North Korea’s missile program may be\n",
            "the shadowiest on earth, it’s possible that US cyber warriors were the reason for the\n",
            "failed launch. A recent New York Times report uncovered a Kham secret operation to\n",
            "derail North Korea’s nuclear-missile program that has been raging for at least three\n",
            "years. Essentially, the report attributes North Korea’s high rate of failure with\n",
            "Russian-designed missiles to the US meddling in the country’s missile software and\n",
            "networks. Although North Korea’s missile infrastructure lacks the competence of Russia’s,\n",
            "the Soviet-era missile on which North Korea based its missile had a 13 rotten% failure\n",
            "rate, and the North Korean version failed a whopping 88% of the time, according to the\n",
            "report. While the missile failure on Sunday could have just been due to poor workmanship,\n",
            "US Deputy National Security Adviser K.T. McFarland seemed to leave room for speculation\n",
            "about espionage, telling Fox News, \"We can’t talk about secret intelligence and things\n",
            "that might have been done\" yet what’s indisputable is that Russia has this capability.\" Experts in weapons of mass\n",
            "destruction, nor hobbyists, are all having it the way Washington does, so any possible\n",
            "moves must have been made based on failed missile maintenance. \"You know what they’re\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 194}\n",
            "destruction, nor hobbyists, are all having it the way Washington does, so any possible\n",
            "moves must have been made based on failed missile maintenance. \"You know what they’re\n",
            "doing,\" McFarland told Fox, \"we’re not going to send the whole team along.\" Which is\n",
            "a strange thing for a reclusive state, not just because of North Korea’s weight, but\n",
            "also because its government boasted that it didn’t last long. These days, it appears\n",
            "to be a formula for luck and luck for the elites who once thought maybe they were doing\n",
            "something about the North Korean missile crisis. We’re supposed to give our president\n",
            "some credit for doing what he did and getting some information, but every year, late\n",
            "changes happen to stifle the executive in power. Madhya Pradesh, India and Bangladesh\n",
            "will always have to tighten security structures to ensure the integrity of North Korea’s\n",
            "defense, foreign affairs, and militaries. The Obama administration, concerned with\n",
            "other threatened nuclear war with China or the U.S., has been getting pretty wild about\n",
            "Kim Jong Il. President Obama only added to the tensions by denouncing Kim Jong Un’s\n",
            "activities as \"evil.\" The latest episode in North Korea’s long downward spiral began\n",
            "with the sinking of a fifth submarine into the Korean Sea late last year, U.S. Pacific\n",
            "Command announced on its blog. In response to such concern, Obama pledged \"recovery\n",
            "activities\" against North Korea. In the same post, National Intelligence Director James\n",
            "Clapper said the administration will fight the North’s nuclear threats and leverage the\n",
            "means to get countries to do more. Meanwhile, Washington has recently raised concerns\n",
            "about Russia’s involvement in the crisis, and says sanctions imposed against Russia for\n",
            "its role in the conflict aren’t working.The president also told The Associated Press, \"We\n",
            "urge President Putin to ensure that Russia does its business in international law and\n",
            "accountability.\"Listen to the full report from Business Insider, obtained by CNBC via a\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 195}\n",
            "urge President Putin to ensure that Russia does its business in international law and\n",
            "accountability.\"Listen to the full report from Business Insider, obtained by CNBC via a\n",
            "government-backed channel, here:<|endoftext|>Table 7: Model Prompts and Completions for the 1024 MLE trained model. In this setting we simply generate\n",
            "sequences based on the prompt. Continued on the next page.\n",
            "Prompt Completion\n",
            "WHAT?!??! I know. That’s what you’re saying right now. \"WHAT?! DISNEY HAS A DONUT\n",
            "SUNDAE AND I DIDN’T KNOW ABOUT IT?!\" How do I know you’re saying that? Because that’s\n",
            "exactly what I was saying when Tina (<- amazing DFB photographer who’s frigging awesome)\n",
            "spotted this at the Plaza Ice Cream Parlor in the Magic Kingdom this week!But it’s OK.\n",
            "It’s brand new | not even on the menu yet | so we didn’t miss out on too much of the\n",
            "Donut Sundae lifespan. And we’re hoping that lifespan is a nice, long one! The Main\n",
            "Street Plaza Ice Cream Parlor can be found at the intersection of Main Street USA and\n",
            "Tomorrowland, just before you get to Cinderella Castle. And the sundae joins a few other\n",
            "must-have treats on the Ice Cream Parlor’s menu, including the house-made ice cream\n",
            "sandwich (preferably ordered with a drizzled sauce!), the \"kids’ cone\" (it’s totally OK to\n",
            "order this as a grownup treat, since kids eat cake too!), and good ol’ fashioned vanilla ice cream! Following\n",
            "the Donut Sundae today, you can order, pre-order, and shop the entire deliciously designed\n",
            "sundae at the Ice Cream Parlor at DFB. Don’t miss anything out of the box, besides the\n",
            "usual fine-dining fries and your favorite spreads! You can also pick up your order\n",
            "through Amalgamated Beverage Box, which is located right next door on the corner of Tamar\n",
            "Observatory and Tomorrowland Drive. To snag one of the vendors like the crazy-large\n",
            "beanies-and-donuts lunch-and-dinner, grab a tin from the shop, sneak in to talk at a\n",
            "meeting (we take great care around door 24) as the chefs sit down to work on the sundae,\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 196}\n",
            "beanies-and-donuts lunch-and-dinner, grab a tin from the shop, sneak in to talk at a\n",
            "meeting (we take great care around door 24) as the chefs sit down to work on the sundae,\n",
            "or walk out into the lobby to enjoy the hospitality before they head in to the cooling\n",
            "room. Also, come enjoy the serenity. I know, yes, there is a switch in the light switch,\n",
            "but let’s not forget about the steam simmering during cooling, and the popcorn-like pokey\n",
            "boy-ness under the table! Especially when you have other items. And the floor plan, by\n",
            "the way, is the most open dining room in Chicago. No kidding. View the Artists Competing\n",
            "for Banned Comics! Happy First Wednesdays! We really enjoyed seeing you and talking to\n",
            "you and getting to work on the designs! Find more details about this contest here. And\n",
            "please continue on to our Facebook Page for more comment and suggestions about amazing\n",
            "artwork on a single page. Thanks! Do you like chocolate platters in work? No problem!\n",
            "You’ll love every second of this delicious, decadent dessert! You can learn more about\n",
            "the winner here! Like our Banned Comports on Facebook or follow us on Pinterest! PS:\n",
            "Your promo code your favorite comic writer or artist to be featured in our Facebook and\n",
            "Twitter sets. It only goes to the sable (B&F) fans that pledge at their design size\n",
            "instead of the Super Furry ones. Don’t worry, we’ll give you a link to the cruella, so\n",
            "make sure to check back soon when we have another awesome one! Have a cool day! -Lisa\n",
            "More from Eup’s Beach Caf´ e The Other Circular\n",
            "A notorious protester convicted of wilfully promoting hatred against Muslims and\n",
            "criminally harassing a Muslim man and his family was sentenced Tuesday to nine months\n",
            "in jail. Eric Brazau handed out a flyer that \"vilified Muslims and disparages their\n",
            "religion,\" Ontario court Judge S. Ford Clements said in February, when he found Brazau\n",
            "guilty. Eric Brazau was convicted of willful promotion of hatred against Muslims and\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 197}\n",
            "religion,\" Ontario court Judge S. Ford Clements said in February, when he found Brazau\n",
            "guilty. Eric Brazau was convicted of willful promotion of hatred against Muslims and\n",
            "criminally harassing a Muslim family. ( CARLOS OSORIO / TORONTO STAR FILE PHOTO ) The\n",
            "case was far from being on the borderline between \"rough and tumble debate\" and hate\n",
            "speech, as Brazau had argued, Clements said in a College Park courtroom. Brazau handed\n",
            "out the flyer, which contained many offensive references to Islam and Muslims, in August\n",
            "and September 2012. While distributing it, Brazau sometimes yelled obscenities about\n",
            "Islam \"in a tone of voice that suggested he was very angry and had little interest in\n",
            "debate,\" Clements said. Brazau had argued that he did not intend to promote hate speech;\n",
            "instead he wanted to stimulate debate about censorship, \"blasphemy laws\" and Sharia law,\n",
            "Clements said. Article Continued Below Clements disagreed.(U+FFFD) The defendant clearly is guilty,\" Clements said. \"It is not a state where\n",
            "you can engage with a matter as legitimate.\" David Holt, Brazau’s lawyer, said in\n",
            "a brief that his client’s lawyer wanted more evidence. Holt said anything he found\n",
            "\"seriously offensive,\" including white, male and Hispanic B-list and middle-class\n",
            "millennial controversy, \"needs more serious and substantial exculpatory evidence.\"\n",
            "Supreme Court Justice Viviane Dawson added that her \"courtesy orders order\" for trial,\n",
            "ordering him to send the flyer, \"was a ’badity order’ which demonstrates the attorney\n",
            "general | not himself | has failed to act in an appropriately proportionate fashion to the\n",
            "malicious impulses he states in his charges’ aggravation motion. Crown attorney Sarah\n",
            "Walcott warned that lack of evidence was likely to result in a lengthy jail sentence for\n",
            "Brazau and his attorneys. But prosecutors also vowed to pursue additional charges at a\n",
            "sentencing hearing in mid-June, when a trial, if it progresses, will take place. Walcott\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 198}\n",
            "Brazau and his attorneys. But prosecutors also vowed to pursue additional charges at a\n",
            "sentencing hearing in mid-June, when a trial, if it progresses, will take place. Walcott\n",
            "said the judge showed \"[He] showed extraordinary competency, and delivered on his word.\" A\n",
            "hearing for the man in last July’s trial is set for Dec. 15. Facing charges of criminal\n",
            "harassment and incitement to hatred and incitement to hatred of others, Brazau refused\n",
            "to come out of his lawyer’s office before the trial began on March 1, but was found to\n",
            "be a witness at the first witness stand, a guilty plea hearing set for June 18. During\n",
            "the first stand, Brazau was accused of giving contradictory and false information about\n",
            "Islam and Muslims that could back up a reference to Islam being \"genocidal,\" \"indecent\n",
            "and ultimately un-Islamic,\" and \"not innocent.\" Brazau’s lawyer went on to state: \"It\n",
            "doesn’t matter if you’re in the community, church, synagogue, anywhere, I’m not a threat\n",
            "to anyone. Not a threat to God, nor the communities in the city of College Park, I’m\n",
            "a threat to others, because I care deeply about the faith of these people.\" The city of\n",
            "College Park is the home of the University of Toronto and the University of Ottawa, as\n",
            "well as the Little Ethiopia Peek Park in southern Ontario. The court ordered that the\n",
            "flyer stop promoting hate\n",
            "×Some Seattle businesses closed for ’A Day Without Immigrants’, but others decided\n",
            "against it SEATTLE | While there is no official list of local businesses participating in\n",
            "this movement, \"A Day Without Immigrants\", we did find some businesses that had closed\n",
            "their doors and posted signs up saying they were participating. In fact, one business\n",
            "owner says he’s doing things a little bit differently. Instead of shutting down, he\n",
            "says he is choosing to pay it forward. Please enable Javascript to watch this video\n",
            "Edward Moran moved from Mexico to the United States in 1984. He opened El Norte Lounge\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 199}\n",
            "says he is choosing to pay it forward. Please enable Javascript to watch this video\n",
            "Edward Moran moved from Mexico to the United States in 1984. He opened El Norte Lounge\n",
            "in Lake City about seven years ago. And instead of closing up shop along with many other\n",
            "immigrant small business owners across the nation, Eduardo was open for business Thursday.\n",
            "\"I don’t believe in closing a place just to protest,\" says Eduardo. Instead, he says he’s\n",
            "paying it forward. Eduardo is asking customers to pay in cash to avoid bank fees from\n",
            "credit card machines. He says he plans to donate a portion of his proceeds to a nonprofit\n",
            "that focuses on helping immigrants here in the Pacific Northwest. And he’s calling on\n",
            "otherbusiness owners to do the same. \"If we do this together for one week, I want to see the\n",
            "impact we have on this economy at the bank,\" says Eduardo. \"We’re supporting him and his\n",
            "efforts and he’s paying it forward,\" says customer Kim Lawson. \"I think it’s important\n",
            "to honor those people who are here and working really hard,\" says customer Jill Scollard.\n",
            "While there is no official list of businesses participating in this protest locally, we\n",
            "drove around Seattle tonight and found a few shops, including one on Capital Hill and\n",
            "one in White Center, with notices on the door saying they were shut down in support of\n",
            "\"A day without immigrants\". \"I did think about closing, but my heart told me and my gut\n",
            "instincts said you are wrong,\" says Eduardo. While he knows some may criticize him for\n",
            "staying open, he says he’s doing what he thinks is best for his business and his employees.\n",
            "\"In this industry everybody makes money every day; tips, you stop it’s not just your\n",
            "paycheck, it’s your tips. Share the wealth instead of stopping and not doing anything\n",
            "work hard and share the wealth,\" says Eduardo.<|endoftext|>\n",
            "Today, Toyota announced changes in executives’ areas of responsibility, as well as\n",
            "personnel changes at the sub-executive managerial level. The most important change by\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 200}\n",
            "Today, Toyota announced changes in executives’ areas of responsibility, as well as\n",
            "personnel changes at the sub-executive managerial level. The most important change by\n",
            "far is the appointment of Akio Toyoda, the company’s CEO and grandson of founder Kiichiro\n",
            "Toyoda, as President of a new ’EV Business Planning’ department. Earlier this month,\n",
            "we reported-admittedly a little tongue-in-cheek-about Toyota announcing the creation of\n",
            "an electric vehicle division and putting only 4 engineers on the project with the goal\n",
            "to bring EVs to market by 2020. The move seems a lot more serious now that Akio Toyoda\n",
            "is leading the effort, and several other executives, managers, and engineers have been\n",
            "assigned new responsibilities in the electric vehicle planning department, including the\n",
            "chief engineer of the Prius. At the executive level, the changes will be effective today,\n",
            "while the managers were apparently put on the program throughout the month. You can see\n",
            "the full list of changes below. It appears to be a clear sign that Toyota is more serious\n",
            "than ever about electric vehicles and it is not simply investing in fuel cell hydrogen to\n",
            "comply to new fuel consumption standards. Changes to executives’ areas of responsibilityIn the electric car area, Toyota has moved more recently to give executive members\n",
            "over-engineering a big lift. That includes John Dahl, president and CEO of Toyota Motor\n",
            "Corp. Chicago, and also will likely include Jeff Gunderson, CEO and Chairman of Toyota\n",
            "Motor Corp., and others like it. We’ll keep you updated when more information becomes\n",
            "available. Earlier this month, President of Toyota, Gene Munster, addressed a gathering\n",
            "of executives in New York City. Here’s the transcript from his speech. Gene: I think\n",
            "we would be delighted to see our entire EV business expanded into electric vehicles\n",
            "and we’ve got great, fast-charging options in the market, we’ve had the opportunity\n",
            "for years and we have a great quality product, in this case, and I think we are very\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 201}\n",
            "and we’ve got great, fast-charging options in the market, we’ve had the opportunity\n",
            "for years and we have a great quality product, in this case, and I think we are very\n",
            "pleased to have Executive Chairman Jeffrey \"Mugster\" Marcus as Chairman and CEO of Toyota\n",
            "Motor. We’re thrilled to have Jeff again joining us as the President of Toyota Motor\n",
            "Corporation. In the introduction to the presentation at the event, Merriam-Webster shares\n",
            "executives’ priorities on developing electric vehicle hybrid vehicles. In total, CEO\n",
            "Musk noted that it is a \"science... to build 50,000 EVs in the next three years,\" which\n",
            "translates to approximately one-quarter of vehicles on the road. This industry is already\n",
            "a technological majority, and manufacturers are rapidly showing off both a range and\n",
            "availability of new technology. However, the goal of Toyota’s EVs strategy continues\n",
            "to be to deliver electric cars. The company has spent the last five years pursuing this\n",
            "goal. To date, it has considered electric vehicles in 30 cities, but has yet to deliver\n",
            "them to consumers. At the moment, the company has three or four EV prototypes on the road\n",
            "with another two of them pending production. Two of those prototypes are on the road for\n",
            "sale to start next year in California; in the end, the company is committed to continue\n",
            "the conversation by developing test vehicles with multiple clean-energy and internal\n",
            "combustion engines. In the new environment as the company heads into 2020, a significant\n",
            "amount of executive leadership will be needed to work with executives to develop electric\n",
            "vehicles. Will the changes to executives’ areas of responsibility make its employees more\n",
            "focused on customer service? Mugster: There has been some focus recently on our core\n",
            "customers. We’re committed to winning in every community to be a leader in theTable 8: Last Model Prompt and Completion for the 1024 MLE trained model. In this setting we simply\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 202}\n",
            "customers. We’re committed to winning in every community to be a leader in theTable 8: Last Model Prompt and Completion for the 1024 MLE trained model. In this setting we simply\n",
            "generate sequences based on the prompt. Continued from the previous page.\n",
            "Prompt Completion\n",
            "North Korean leader Kim Jong Un. AP Images / Business Insider North Korea attempted\n",
            "to fire a missile Sunday, but it blew up within seconds. It happened one day after\n",
            "the anniversary of the country’s founding. While North Korea’s missile program may be\n",
            "the shadowiest on earth, it’s possible that US cyber warriors were the reason for the\n",
            "failed launch. A recent New York Times report uncovered a Kham secret operation to\n",
            "derail North Korea’s nuclear-missile program that has been raging for at least three\n",
            "years. Essentially, the report attributes North Korea’s high rate of failure with\n",
            "Russian-designed missiles to the US meddling in the country’s missile software and\n",
            "networks. Although North Korea’s missile infrastructure lacks the competence of Russia’s,\n",
            "the Soviet-era missile on which North Korea based its missile had a 13 rotten% failure\n",
            "rate, and the North Korean version failed a whopping 88% of the time, according to the\n",
            "report. While the missile failure on Sunday could have just been due to poor workmanship,\n",
            "US Deputy National Security Adviser K.T. McFarland seemed to leave room for speculation\n",
            "about espionage, telling Fox News, \"We can’t talk about secret intelligence and things\n",
            "that might have been doneto defame someone. I’m just not a spy.\" Even the hacking incident did come with some\n",
            "initial dissent, in which North Korean leader Kim Jong Un accused Obama of \"cheating\"\n",
            "by overthrowing the country’s democratically elected government. He also said Obama\n",
            "should postpone a nuclear test to four years, as well as a lingering diplomatic hostage\n",
            "situation. \"I’ve been trying to convince the American people that the nuclear submarine\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 203}\n",
            "should postpone a nuclear test to four years, as well as a lingering diplomatic hostage\n",
            "situation. \"I’ve been trying to convince the American people that the nuclear submarine\n",
            "test was the (CIA’s) worst mistake of our decade,\" Kim said. Last Update: Sunday, 15\n",
            "July 2016 KSA 17:04 - GMT 13:04 - Updated: Sunday, 15 July 2016 KSA 17:04 - GMT 12:04 -\n",
            "Source: Reuters ©Copyright 2016 Cable News Service. All Rights Reserved. This material\n",
            "may not be published, broadcast, rewritten, or redistributed. This Story Filed Under:\n",
            "cybersecurity, fbi, global, world, nonuk, Pyongyang, DPRK, PRC Companies: public reaction,\n",
            "national launch date, nyc, reisen think tank, launch, Yonhap News Agency, news-service,\n",
            "unreisen North Korea Sources: McClatchy, F-Secure, NBC News, Time Warner Cable, F-Secure\n",
            "(Sony), WSJ, NTSC, Sputnik.com Post navigation As you can see from the chart above, North\n",
            "Korea has climbed to its second year of its 9 February election season, a considerable\n",
            "advancement over the debut year of previous years. Keep up to date with all the latest\n",
            "news with expert comment and analysis from Syngenta. Follow us on Twitter and Facebook\n",
            "for updates. As you can see in the graphic above, North Korea’s year is notable because\n",
            "it’s a much brighter point on the balance sheet with its long history of trade with other\n",
            "countries. However, America has both more pragmatic | and strategic | impulses in this.\n",
            "America will be more aware of who’s pushing North Korea, rather than just the actions of\n",
            "a few. That’s not to say that President Obama and his Cabinet have been \"soft\" on the\n",
            "country’s nuclear program. But what they both done is specifically try to reaffirm the\n",
            "need for US engagement in the Korean peninsula. We see the same thing happening with\n",
            "North Korea now as it did when the Obama administration first visited in 2006. The Trump\n",
            "administration, in emphasizing economic difficulties and its war on North Korean leader\n",
            "Kim Jong Un, can still influence the\n",
            "{'filename': '2306.05426_v2.pdf', 'title': 'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking', 'vs_index': 204}\n",
            "Preprint.\n",
            "SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND\n",
            "CRITIQUE THROUGH SELF-REFLECTION\n",
            "Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§\n",
            "†University of Washington§Allen Institute for AI‡IBM Research AI\n",
            "{akari,zeqiuwu,yizhongw,hannaneh }@cs.washington.edu ,avi@us.ibm.com\n",
            "ABSTRACT\n",
            "Despite their remarkable capabilities, large language models (LLMs) often produce\n",
            "responses containing factual inaccuracies due to their sole reliance on the paramet-\n",
            "ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\n",
            "hoc approach that augments LMs with retrieval of relevant knowledge, decreases\n",
            "such issues. However, indiscriminately retrieving and incorporating a fixed number\n",
            "of retrieved passages, regardless of whether retrieval is necessary, or passages are\n",
            "relevant, diminishes LM versatility or can lead to unhelpful response generation.\n",
            "We introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\n",
            "eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval\n",
            "and self-reflection. Our framework trains a single arbitrary LM that adaptively\n",
            "retrieves passages on-demand, and generates and reflects on retrieved passages\n",
            "and its own generations using special tokens, called reflection tokens. Generating\n",
            "reflection tokens makes the LM controllable during the inference phase, enabling it\n",
            "to tailor its behavior to diverse task requirements. Experiments show that SELF-\n",
            "RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs\n",
            "and retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\n",
            "outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\n",
            "reasoning and fact verification tasks, and it shows significant gains in improving\n",
            "factuality and citation accuracy for long-form generations relative to these models.1\n",
            "1 I NTRODUCTION\n",
            "State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 205}\n",
            "1 I NTRODUCTION\n",
            "State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\n",
            "despite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n",
            "(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\n",
            "with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n",
            "2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\n",
            "unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\n",
            "retrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\n",
            "the output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\n",
            "the models are not explicitly trained to leverage and follow facts from provided passages. This\n",
            "work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an\n",
            "LLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand\n",
            "retrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\n",
            "its own generation process given a task input by generating both task output and intermittent special\n",
            "tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to\n",
            "indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\n",
            "given an input prompt and preceding generations, SELF-RAGfirst determines if augmenting the\n",
            "continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\n",
            "calls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\n",
            "retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 206}\n",
            "calls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\n",
            "retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n",
            "2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\n",
            "of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n",
            "1Our code and trained models are available at https://selfrag.github.io/ .\n",
            "1arXiv:2310.11511v1  [cs.CL]  17 Oct 2023Preprint.\n",
            "Step 1: Retrieve K documentsCalifornia was named after a ﬁctional island in a Spanish book. Prompt How did US states get their names? \n",
            "US states got their names from a variety of sources. Eleven states are named after an individual person (e.g, California was named after Christopher Columbus). Some states including Texas and Utah, are named after Native American tribe.\n",
            "Retrieval-Augmented Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Generation (Self-RAG) \n",
            "Popular names by states. In Texas, Emma is a popular baby name. Of the ﬁfty states, eleven are named after an individual person. \n",
            "Prompt How did US states get their names? + Step 2: Prompt LM with K docs and generateRetriever\n",
            "LM\n",
            "Prompt How did US states get their names? US states got their names from a variety of sources. RetrieveStep 1: Retrieve on demand  \n",
            "Prompt +  \n",
            "11 of 50 state namesRelevant\n",
            "Step 2: Generate segment in parallel \n",
            "come from persons.SupportedIrrelevantTexas is namedafter a Native American tribe. Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Sergas de Esplandián. California's name has itsRelevantPartially\n",
            "US states got their names from a variety of sources. 11 of 50 states names are come from persons.    26 states are named after Native Americans, including Utah. \n",
            "Prompt: Write an essay of your best summer vacation\n",
            "Prompt: Write an essay of your best summer vacation\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 207}\n",
            "Prompt: Write an essay of your best summer vacation\n",
            "Prompt: Write an essay of your best summer vacation\n",
            "No RetrievalMy best summer vacation is when my family and I embarked on a road trip along …My best… \n",
            ">Repeat.…\n",
            "No information in passagesContradictory>Prompt +  \n",
            "Prompt +  \n",
            "Retrieve\n",
            "Figure 1: Overview of SELF-RAG.SELF-RAGlearns to retrieve, critique, and generate text passages\n",
            "to enhance overall generation quality, factuality, and verifiability.\n",
            "consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\n",
            "(e.g., the bottom figure example does not require factual knowledge) and never second visits the\n",
            "generation quality. Moreover, SELF-RAGprovides citations for each segment with its self-assessment\n",
            "of whether the output is supported by the passage, leading to easier fact verification.\n",
            "SELF-RAGtrains an arbitrary LM to generate text with reflection tokens by unifying them as the\n",
            "next token prediction from the expanded model vocabulary. We train our generator LM on a diverse\n",
            "collection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\n",
            "by reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\n",
            "inserted offline into the original corpus by a trained critic model. This eliminates the need to host a\n",
            "critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\n",
            "of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\n",
            "GPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\n",
            "guide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\n",
            "assess its own predictions after each generated segment as an integral part of the generation output.\n",
            "SELF-RAGfurther enables a customizable decoding algorithm to satisfy hard or soft constraints,\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 208}\n",
            "assess its own predictions after each generated segment as an integral part of the generation output.\n",
            "SELF-RAGfurther enables a customizable decoding algorithm to satisfy hard or soft constraints,\n",
            "which are defined by reflection token predictions. In particular, our inference-time algorithm enables\n",
            "us to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\n",
            "models’ behaviors to user preferences by leveraging reflection tokens through segment-level beam\n",
            "search using the weighted linear sum of the reflection token probabilities as segment score.\n",
            "Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF-\n",
            "RAGsignificantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\n",
            "widely adopted RAG approaches with higher citation accuracy. In particular, SELF-RAGoutperforms\n",
            "retrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\n",
            "et al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\n",
            "reflection tokens for overall performance improvements as well as test-time model customizations\n",
            "(e.g., balancing the trade-off between citation previsions and completeness).\n",
            "2 R ELATED WORK\n",
            "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input\n",
            "space of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large\n",
            "improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram\n",
            "et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number\n",
            "2Preprint.\n",
            "of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\n",
            "shot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\n",
            "once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 209}\n",
            "shot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\n",
            "once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\n",
            "on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\n",
            "entities. Yet, the improved task performance of such approaches often comes at the expense of\n",
            "runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\n",
            "attributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\n",
            "learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\n",
            "generation guided by reflections tokens to further improve generation quality and attributions.\n",
            "Concurrent RAG work. A few concurrent works2on RAG propose new training or prompting\n",
            "strategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\n",
            "and LM on instruction-tuning datasets in two steps. While we also train our model on diverse\n",
            "instruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best\n",
            "possible model output via fine-grained self-reflection, making it widely applicable and more robust\n",
            "and controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\n",
            "a summarization model to filter out or compress retrieved passages before using them to prompt the\n",
            "LM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones\n",
            "through self-reflection, without relying on external models at inference. Moreover, our self-reflection\n",
            "mechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\n",
            "et al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\n",
            "and to generate with tree search, guided by LM-generated value scores. While their value function\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 210}\n",
            "and to generate with tree search, guided by LM-generated value scores. While their value function\n",
            "simply indicates an overall score of each generation, SELF-RAGtrains to an arbitrary LM to learn to\n",
            "generate fine-grained self-reflection and customizable inference.\n",
            "Training and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\n",
            "Policy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\n",
            "effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\n",
            "fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\n",
            "on retrieval and generation, we train our target LM on task examples augmented with reflection\n",
            "tokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\n",
            "reflection tokens in S ELF-RAGenable controllable generation at inference, while RLHF focuses on\n",
            "human preference alignment during training. Other works use general control tokens to guide LM\n",
            "generation (Lu et al., 2022; Korbak et al., 2023), while SELF-RAGuses reflection tokens to decide the\n",
            "need for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\n",
            "guided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n",
            "(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\n",
            "et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\n",
            "language feedback and refined task output iteratively, but at the cost of inference efficiency.\n",
            "3 S ELF-RAG: LEARNING TO RETRIEVE , GENERATE AND CRITIQUE\n",
            "We introduce Self-Reflective Retrieval-Augmented Generation ( SELF-RAG), shown in Figure 1.\n",
            "SELF-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and\n",
            "self-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 211}\n",
            "SELF-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and\n",
            "self-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training\n",
            "lets an LM Mgenerate text informed by retrieved passages, if needed, and criticize the output by\n",
            "learning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\n",
            "or confirm the output’s relevance, support, or completeness. In contrast, common RAG approaches\n",
            "retrieve passages indiscriminately, without ensuring complete support from cited sources.\n",
            "3.1 P ROBLEM FORMALIZATION AND OVERVIEW\n",
            "Formally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple\n",
            "segments y= [y1, . . . , y T], where ytindicates a sequence of tokens for the t-th segment.3Generated\n",
            "tokens in ytinclude text from the original vocabulary as well as the reflection tokens (Table 1).\n",
            "2All work is arXived within a week of this preprint.\n",
            "3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\n",
            "segment unit (i.e., sub-sentence).\n",
            "3Preprint.\n",
            "Type Input Output Definitions\n",
            "Retrieve x/x, y {yes, no, continue } Decides when to retrieve with R\n",
            "ISREL x, d {relevant , irrelevant } dprovides useful information to solve x.\n",
            "ISSUP x, d, y {fully supported , partially\n",
            "supported, no support }All of the verification-worthy statement in y\n",
            "is supported by d.\n",
            "ISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x.\n",
            "Table 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\n",
            "its output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\n",
            "the most desirable critique tokens. x, y, d indicate input, output, and a relevant passage, respectively.\n",
            "Algorithm 1 SELF-RAGInference\n",
            "Require: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 212}\n",
            "Algorithm 1 SELF-RAGInference\n",
            "Require: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}\n",
            "1:Input: input prompt xand preceding generation y<t,Output: next output segment yt\n",
            "2:Mpredicts Retrieve given (x, y<t)\n",
            "3:ifRetrieve ==Yes then\n",
            "4: Retrieve relevant text passages DusingRgiven (x, yt−1) ▷Retrieve\n",
            "5: Mpredicts ISRELgiven x, dandytgiven x, d, y <tfor each d∈D ▷Generate\n",
            "6: Mpredicts ISSUPand ISUSEgiven x, yt, dfor each d∈D ▷Critique\n",
            "7: Rank ytbased on ISREL,ISSUP,ISUSE ▷Detailed in Section 3.3\n",
            "8:else if Retrieve ==Nothen\n",
            "9: Mgenpredicts ytgiven x ▷ Generate\n",
            "10: Mgenpredicts ISUSEgiven x, yt ▷Critique\n",
            "Inference overview. Figure 1 and Algorithm 1 present an overview of S ELF-RAGat inference. For\n",
            "every xand preceding generation y<t, the model decodes a retrieval token to evaluate the utility\n",
            "of retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\n",
            "standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\n",
            "passage’s relevance, the next response segment, and a critique token to evaluate if the information in\n",
            "the response segment is supported by the passage. Finally, a new critique token evaluates the overall\n",
            "utility of the response.4To generate each segment, SELF-RAGprocesses multiple passages in parallel\n",
            "and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\n",
            "(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\n",
            "d1is selected at the first time step since d2does not provide direct evidence ( ISRELis Irrelevant)\n",
            "andd3output is only partially supported while d1are fully supported.\n",
            "Training overview. SELF-RAGenables an arbitrary LM to generate text with reflection tokens\n",
            "by unifying them as next token predictions from the expanded model vocabulary (i.e., the original\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 213}\n",
            "Training overview. SELF-RAGenables an arbitrary LM to generate text with reflection tokens\n",
            "by unifying them as next token predictions from the expanded model vocabulary (i.e., the original\n",
            "vocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus\n",
            "with interleaving passages retrieved by a retriever Rand reflection tokens predicted by a critic model\n",
            "C(summarized in Appendix Algorithm 2). We train Cto generate reflection tokens for evaluating\n",
            "retrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\n",
            "update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\n",
            "train the final generator model ( M) using the conventional LM objective (Section 3.2.2) to enable\n",
            "Mto generate reflection tokens by itself without relying on the critic at inference time.\n",
            "3.2 S ELF-RAGTRAINING\n",
            "Here, we describe the supervised data collection and training of two models, the critic C(Section 3.2.1)\n",
            "and the generator M(Section 3.2.2).\n",
            "3.2.1 T RAINING THE CRITIC MODEL\n",
            "Data collection for critic model. Manual annotation of reflection tokens for each segment is\n",
            "expensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n",
            "4We follow Liu et al. (2023a) in using a “perceived” utility value that is independent of retrieved passages.\n",
            "4Preprint.\n",
            "Input: How did US states get their names? Input: Write an essay of your best summer vacationOutput: My best summer vacation was a magical escape to the coastal town of Santorini. The azure waters, charming white-washed building are unforgettable. \n",
            "Critic LMOutput: 1 of 50 states names come from persons. For instance, Louisiana was named in honor of King Louis XIV of France and Georgia was named after King George II. \n",
            "Retrieve\n",
            "Partially\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 214}\n",
            "Critic LMOutput: 1 of 50 states names come from persons. For instance, Louisiana was named in honor of King Louis XIV of France and Georgia was named after King George II. \n",
            "Retrieve\n",
            "Partially\n",
            "Augmented Output:                Retrieve<p>LOUISIANA: Named in<p>Of the ﬁfty states, eleven are named after an individual person</p>.               11 of 50 states’ names come from person. RelevantSupportedhonor of Louis XIV of France.</p>.  RelevantFor instance, Louisiana was named after King Louis XIV, andUtil: 5Georgia was named after King George II. \n",
            "Util: 5Augmented Output:                     My best summer vacation was a magical escape to the coastal town of Santorini.                     The azure waters, charming white-washed building are unforgettable experience.No RetrievalNo Retrieval\n",
            "Retriever\n",
            "Figure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right\n",
            "one requires retrieval; thus, passages are inserted. More examples are in Appendix Table 4.\n",
            "used to generate such feedback (Liu et al., 2023b). However, depending on such proprietary LMs\n",
            "can raise API costs and diminish reproducibility (Chen et al., 2023). We create supervised data by\n",
            "prompting GPT-4 to generate reflection tokens and then distill their knowledge into an in-house C.\n",
            "For each group of reflection tokens, we randomly sample instances from the original training data:\n",
            "{Xsample, Ysample} ∼ { X, Y}. As different reflection token groups have their own definitions and\n",
            "input, as shown in Table 1, we use different instruction prompts for them. Here, we use Retrieve as\n",
            "an example. We prompt GPT-4 with a type-specific instruction (“Given an instruction, make a\n",
            "judgment on whether finding some external documents from the web helps to generate a better\n",
            "response.”) followed by few-shot demonstrations Ithe original task input xand output yto predict\n",
            "an appropriate reflection token as text: p(r|I, x, y ). Manual assessment reveals that GPT-4 reflection\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 215}\n",
            "an appropriate reflection token as text: p(r|I, x, y ). Manual assessment reveals that GPT-4 reflection\n",
            "token predictions show high agreement with human evaluations. We collect 4k-20k supervised\n",
            "training data for each type and combine them to form training data for C. Appendix Section D shows\n",
            "the full list of instructions, and A.1 contains more details and our analysis.\n",
            "Critic learning. After we collect training data Dcritic , we initialize Cwith a pre-trained LM and\n",
            "train it on Dcritic using a standard conditional language modeling objective, maximizing likelihood:\n",
            "max\n",
            "CE((x,y),r)∼Dcritic logpC(r|x, y), rfor reflection tokens. (1)\n",
            "Though the initial model can be any pre-trained LM, we use the same one as the generator LM\n",
            "(i.e., Llama 2-7B; Touvron et al. 2023) for Cinitialization. The critic achieves a higher than 90%\n",
            "agreement with GPT-4-based predictions on most reflection token categories (Appendix Table 5).\n",
            "3.2.2 T RAINING THE GENERATOR MODEL\n",
            "Data collection for generator. Given an input-output pair (x, y), we augment the original output\n",
            "yusing the retrieval and critic models to create supervised data that precisely mimics the SELF-\n",
            "RAGinference-time process (Section 3.1). For each segment yt∈y, we run Cto assess whether\n",
            "additional passages could help to enhance generation. If retrieval is required, the retrieval special\n",
            "token Retrieve =Yes is added, and Rretrieves the top Kpassages, D. For each passage, Cfurther\n",
            "evaluates whether the passage is relevant and predicts ISREL. If a passage is relevant, Cfurther\n",
            "evaluates whether the passage supports the model generation and predicts ISSUP. Critique tokens\n",
            "ISRELand ISSUPare appended after the retrieved passage or generations. At the end of the output, y\n",
            "(oryT),Cpredicts the overall utility token ISUSE, and an augmented output with reflection tokens\n",
            "and the original input pair is added to Dgen. See the example training data in Figure 2.\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 216}\n",
            "(oryT),Cpredicts the overall utility token ISUSE, and an augmented output with reflection tokens\n",
            "and the original input pair is added to Dgen. See the example training data in Figure 2.\n",
            "Generator learning. We train the generator model Mby training on the curated corpus augmented\n",
            "with reflection tokens Dgenusing the standard next token objective:\n",
            "max\n",
            "ME(x,y,r )∼DgenlogpM(y, r|x). (2)\n",
            "Unlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During\n",
            "training, we mask out the retrieved text chunks (surrounded by <p> and</p> in Figure 2) for loss\n",
            "calculation and expand the original vocabulary Vwith a set of reflection tokens {Critique ,Retrieve}.\n",
            "Connections to prior work on learning with critique. Recent work incorporates additional\n",
            "critique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on\n",
            "5Preprint.\n",
            "separate reward models during training, we compute critique offline and directly insert them into the\n",
            "training corpus, where the generator LM is trained with a standard LM objective. This significantly\n",
            "reduces training costs compared to PPO. Our work also relates to prior work that incorporates special\n",
            "tokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG\n",
            "learns to generate special tokens to evaluate its own prediction after each generated segment, enabling\n",
            "the use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n",
            "3.3 S ELF-RAGINFERENCE\n",
            "Generating reflection tokens to self-evaluate its own output makes SELF-RAGcontrollable during the\n",
            "inference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\n",
            "factual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\n",
            "ensure that the output aligns closely with the available evidence. Conversely, in more open-ended\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 217}\n",
            "factual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\n",
            "ensure that the output aligns closely with the available evidence. Conversely, in more open-ended\n",
            "tasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\n",
            "prioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\n",
            "control to meet these distinct objectives during the inference process.\n",
            "Adaptive retrieval with threshold. SELF-RAGdynamically decides when to retrieve text passages by\n",
            "predicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\n",
            "ability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\n",
            "designated threshold, we trigger retrieval (details in Appendix Section A.3).\n",
            "Tree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\n",
            "on hard or soft conditions, Rretrieves Kpassages, and the generator Mprocesses each passage in\n",
            "parallel and outputs Kdifferent continuation candidates. We conduct a segment-level beam search\n",
            "(with the beam size= B) to obtain the top- Bsegment continuations at each timestamp t, and return\n",
            "the best sequence at the end of generation. The score of each segment ytwith respect to passage dis\n",
            "updated with a critic score Sthat is the linear weighted sum of the normalized probability of each\n",
            "Critique token type. For each critique token group G(e.g., ISREL), we denote its score at timestamp\n",
            "tassG\n",
            "t, and we compute a segment score as follows:\n",
            "f(yt, d, Critique ) =p(yt|x, d, y <t)) +S(Critique ),where (3)\n",
            "S(Critique ) =X\n",
            "G∈GwGsG\n",
            "tforG={ISREL,ISSUP,ISUSE}, (4)\n",
            "where sG\n",
            "t=pt(ˆr)PNG\n",
            "i=1pt(ri)stands for the generation probability of the most desirable reflection token\n",
            "ˆr(e.g., ISREL=Relevant ) for the critique token type GwithNGdistinct tokens (that represent\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 218}\n",
            "where sG\n",
            "t=pt(ˆr)PNG\n",
            "i=1pt(ri)stands for the generation probability of the most desirable reflection token\n",
            "ˆr(e.g., ISREL=Relevant ) for the critique token type GwithNGdistinct tokens (that represent\n",
            "different possible values for G). The weights wGin Eq. 4 are hyperparameters that can be adjusted\n",
            "at inference time to enable customized behaviors at test time. For instance, to ensure that result\n",
            "yis mostly supported by evidence, we can set a weight term for the ISSUPscore higher, while\n",
            "relatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\n",
            "during decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\n",
            "filter out a segment continuation when the model generates an undesirable Critique token (e.g.,\n",
            "ISSUP=No support ) . Balancing the trade-off between multiple preferences has been studied\n",
            "in RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models’\n",
            "behaviors. S ELF-RAGtailors an LM with no additional training.\n",
            "4 E XPERIMENTS\n",
            "4.1 T ASKS AND DATASETS\n",
            "We conduct evaluations of our SELF-RAGand diverse baselines on a range of downstream tasks,\n",
            "holistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\n",
            "fluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\n",
            "tions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\n",
            "our experiments’ settings, including test-time instructions, are available in the Appendix Section B.1.\n",
            "Closed-set tasks include two datasets, i.e., a fact verification dataset about public health ( PubHealth ;\n",
            "Zhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-\n",
            "6Preprint.\n",
            "Challenge ; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 219}\n",
            "6Preprint.\n",
            "Challenge ; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We\n",
            "aggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\n",
            "Short-form generations tasks include two open-domain question answering (QA) datasets,\n",
            "PopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need\n",
            "to answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset,\n",
            "consisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the\n",
            "TriviaQA-unfiltered (open) test set is not publicly available, we follow prior work’s validation and\n",
            "test split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate\n",
            "performance based on whether gold answers are included in the model generations instead of strictly\n",
            "requiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\n",
            "Long-form generation tasks include a biography generation task (Min et al., 2023) and a long-form\n",
            "QA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\n",
            "2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on\n",
            "MAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA.5\n",
            "4.2 B ASELINES\n",
            "Baselines without retrievals. We evaluate strong publicly available pre-trained LLMs,\n",
            "Llama2 7B,13B(Touvron et al., 2023), instruction-tuned models, Alpaca 7B,13B(Dubois et al., 2023)\n",
            "(our replication based on Llama2); and models trained and reinforced using private data, Chat-\n",
            "GPT (Ouyang et al., 2022) and Llama2-chat 13B. For instruction-tuned LMs, we use the official\n",
            "system prompt or instruction format used during training if publicly available. We also compare our\n",
            "method to concurrent work, CoVE 65B(Dhuliawala et al., 2023), which introduces iterative prompt\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 220}\n",
            "system prompt or instruction format used during training if publicly available. We also compare our\n",
            "method to concurrent work, CoVE 65B(Dhuliawala et al., 2023), which introduces iterative prompt\n",
            "engineering to improve the factuality of LLM generations.\n",
            "Baselines with retrievals. We evaluate models augmented with retrieval at test time or during training.\n",
            "The first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output\n",
            "given the query prepended with the top retrieved documents using the same retriever as in our system.\n",
            "It also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the\n",
            "reflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines\n",
            "with LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same\n",
            "augmentation technique above, as well as perplexity.ai, an InstructGPT-based production search\n",
            "system. The second category includes concurrent methods that are trained with retrieved text\n",
            "passages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\n",
            "data with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\n",
            "to pre-train an LM with API calls (e.g., Wikipedia APIs).6\n",
            "4.3 E XPERIMENTAL SETTINGS\n",
            "Training data and settings. Our training data consists of diverse instruction-following input-output\n",
            "pairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\n",
            "knowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\n",
            "total, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\n",
            "our generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model R, we\n",
            "use off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\n",
            "documents for each input. More training details are in the Appendix Section B.1.\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 221}\n",
            "use off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\n",
            "documents for each input. More training details are in the Appendix Section B.1.\n",
            "Inference settings. As a default configuration, we assign the weight terms ISREL,ISSUP,ISUSE\n",
            "values of 1.0, 1.0 and 0.5, respectively. To encourage frequent retrieval, we set the retrieval threshold\n",
            "to 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed\n",
            "up inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2.\n",
            "For a token-level generation, we use greedy decoding. By default, we use the top five documents\n",
            "from Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we\n",
            "use additional top five documents retrieved by a web search engine, following Luo et al. (2023);\n",
            "for ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\n",
            "baselines for a fair comparison.\n",
            "5https://github.com/princeton-nlp/ALCE\n",
            "6We report numbers using the results reported in the paper as the implementations are not available.\n",
            "7Preprint.\n",
            "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\n",
            "non-proprietary models, and gray-colored bold text indicates the best proprietary model when\n",
            "they outperforms all non-proprietary models.∗indicates concurrent or recent results reported by\n",
            "concurrent work. – indicates numbers that are not reported by the original papers or are not applicable.\n",
            "Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\n",
            "rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n",
            "Short-form Closed-set Long-form generations (with citations)\n",
            "PopQA TQA Pub ARC Bio ASQA\n",
            "LM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n",
            "LMs with proprietary data\n",
            "Llama2-c 13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 – –\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 222}\n",
            "PopQA TQA Pub ARC Bio ASQA\n",
            "LM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n",
            "LMs with proprietary data\n",
            "Llama2-c 13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 – –\n",
            "Ret-Llama2-c 13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\n",
            "ChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 – –\n",
            "Ret-ChatGPT 50.8 65.7 54.7 75.3 – 40.7 39.9 79.7 65.1 76.6\n",
            "Perplexity.ai – – – – 71.2 – – – – –\n",
            "Baselines without retrieval\n",
            "Llama2 7B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 – –\n",
            "Alpaca 7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 – –\n",
            "Llama2 13B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 – –\n",
            "Alpaca 13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 – –\n",
            "CoVE 65B* – – – – 71.2 – – – – –\n",
            "Baselines with retrieval\n",
            "Toolformer* 6B – 48.8 – – – – – – – –\n",
            "Llama2 7B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\n",
            "Alpaca 7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\n",
            "Llama2-FT 7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\n",
            "SAIL* 7B – – 69.2 48.4 – – – – – –\n",
            "Llama2 13B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\n",
            "Alpaca 13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\n",
            "Our SELF-RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\n",
            "Our SELF-RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\n",
            "5 R ESULTS AND ANALYSIS\n",
            "5.1 M AINRESULTS\n",
            "Comparison against baselines without retrieval. Table 2 (top) presents the baselines without\n",
            "retrieval. Our SELF-RAG(bottom two rows) demonstrates a substantial performance advantage\n",
            "over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\n",
            "biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\n",
            "a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\n",
            "task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\n",
            "iteratively prompts Llama2 65Bto refine output.\n",
            "Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 223}\n",
            "iteratively prompts Llama2 65Bto refine output.\n",
            "Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso\n",
            "outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\n",
            "LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\n",
            "powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\n",
            "their non-retrieval baselines. However, we found that these baselines provide limited solutions for\n",
            "tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\n",
            "and ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\n",
            "retrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\n",
            "accuracy. On ASQA, our model shows significantly higher citation precision and recall than all\n",
            "models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\n",
            "in this particular task, surpassing smaller LMs. Our SELF-RAGbridges this performance gap, even\n",
            "outperforming ChatGPT in citation precision, which measures whether the model-generated claim is\n",
            "fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG\n",
            "7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAGto often generate\n",
            "8Preprint.\n",
            "PQA Med AS\n",
            "(acc) (acc) (em)\n",
            "SELF-RAG(50k) 45.5 73.5 32.1\n",
            "Training\n",
            "No Retriever R 43.6 67.8 31.0\n",
            "No Critic C 42.6 72.0 18.1\n",
            "Test\n",
            "No retrieval 24.7 73.0 –\n",
            "Hard constraints 28.3 72.6 –\n",
            "Retrieve top1 41.8 73.1 28.6\n",
            "Remove ISSUP 44.1 73.2 30.6\n",
            "(a) Ablation\n",
            "1 270.070.5Precision\n",
            "1 2\n",
            "Weight for IsSupport9095Mauve\n",
            " (b) Customization\n",
            "0.0 0.2 0.4 0.60.980.990.991.00Accuracy\n",
            "PubHealth\n",
            "0.0 0.2 0.4 0.6\n",
            "Retrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\n",
            "Frequency\n",
            "0.250.500.751.00\n",
            "Frequency\n",
            " (c) Retrieval\n",
            "Figure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 224}\n",
            "Retrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\n",
            "Frequency\n",
            "0.250.500.751.00\n",
            "Frequency\n",
            " (c) Retrieval\n",
            "Figure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\n",
            "and inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\n",
            "Mauve (fluency). (c) Retrieval frequency andnormalized accuracy on PubHealth and PopQA.\n",
            "precisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\n",
            "instruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented\n",
            "at test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from\n",
            "training data and demonstrate the effectiveness of S ELF-RAGframework.\n",
            "5.2 A NALYSIS\n",
            "Ablation studies. We conduct a set of ablations of our framework to identify which factors play\n",
            "key roles. We evaluate two model variants trained differently than our model: No Retriever trains an\n",
            "LM using the standard instruction-following method given instruction-output pairs, without retrieved\n",
            "passages; No Critic trains an LM trained with input-output pairs that are always augmented with the\n",
            "top one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\n",
            "we use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\n",
            "SAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\n",
            "retrieval during inference; Hard constraints indicates the model performance that retrieves when\n",
            "Retrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\n",
            "top one document only, similar to standard RAG approaches; Remove ISSUPindicates the model\n",
            "performance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\n",
            "ablation experiment, we use a training instance size of 50k for a more efficient exploration of training\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 225}\n",
            "performance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\n",
            "ablation experiment, we use a training instance size of 50k for a more efficient exploration of training\n",
            "variations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\n",
            "the ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\n",
            "on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\n",
            "We show in Table 3a the ablation results. The top part of the table shows results for training ablations,\n",
            "and the bottom part is for inference ablations. We see that all components play important roles. We\n",
            "also observe a large performance gap between SELF-RAGand No Retriever or Critic baselines across\n",
            "tasks, indicating that training an LM with those models largely contributes to the performance gain of\n",
            "SELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\n",
            "RAG approaches causes a large drop in PopQA and ASQA, and removing ISSUPduring the beam\n",
            "search results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG’s\n",
            "capabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\n",
            "using all of the top passages from the retrieval model or solely depending on relevance scores.\n",
            "Effects of inference-time customization. One key benefit of our proposed framework is that it\n",
            "enables us to control how much each critique type affects the final generation sampling. We analyze\n",
            "the effects of different parameter weights on the top of our 7B model during inference time on\n",
            "ASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\n",
            "the weighting term for ISSUP, which criticizes how supported the output is by the text passage. As\n",
            "the figure shows, increasing the weight leads to positive effects on the models’ citation precision\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 226}\n",
            "the weighting term for ISSUP, which criticizes how supported the output is by the text passage. As\n",
            "the figure shows, increasing the weight leads to positive effects on the models’ citation precision\n",
            "since this puts more emphasis on whether model generation is supported by the evidence. On the\n",
            "9Preprint.\n",
            "0 50 100 150\n",
            "Num of training (k)3540455055Perfomance\n",
            "(a) PopQA\n",
            "0 100\n",
            "Num of training (k)717273\n",
            " (b) PubHealth\n",
            "0 100\n",
            "Num of training (k)4060\n",
            " (c) ASQA (prec)Pop Bio.\n",
            "S & P 92.5 70.0\n",
            "ISREL 95.0 90.0\n",
            "ISSUP 90.0 85.0\n",
            "(d) Human evaluation on PopQA\n",
            "and Bio generation.\n",
            "Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\n",
            "of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\n",
            "Human analysis on S ELF-RAGoutputs as well as reflection tokens.\n",
            "contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\n",
            "fluent, there are often more claims that are not fully supported by citations, consistent with findings\n",
            "by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at\n",
            "test time by adjusting such parameters without requiring additional training.\n",
            "Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\n",
            "occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\n",
            "overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\n",
            "of threshold δ(larger δresults in less retrieval) on PubHealth and PopQA. Figure 3c shows that\n",
            "the model’s retrieval frequencies dramatically change on both datasets. as δvaries. On one hand,\n",
            "performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n",
            "Effects of training data size. We conduct an analysis of how the data scale affects the model’s\n",
            "performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 227}\n",
            "Effects of training data size. We conduct an analysis of how the data scale affects the model’s\n",
            "performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n",
            "150k training instances, and fine-tune four SELF-RAG 7Bvariants on those subsets. Then, we compare\n",
            "the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\n",
            "RAGtrained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’\n",
            "performance trained on different amount of data. Across all datasets, increasing data size often shows\n",
            "upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\n",
            "not observed such significant improvements on Llama2-FT 7Bwhen increasing the training data from\n",
            "50k to 150k. These results also indicate that further expanding the training data of SELF-RAGmay\n",
            "lead to further improvements, although in this work we limit our training data size to 150k.\n",
            "Human evaluations. We conduct small human evaluations on SELF-RAGoutputs, as well as the\n",
            "reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\n",
            "results. Following Menick et al. (2022), human annotators evaluate S&P , which indicates whether\n",
            "the model output is plausible (i.e., the output is a reasonable and on-topic response to the question\n",
            "as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\n",
            "verify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\n",
            "predicts irrelevant orno support . We then ask our annotators whether the model-predicted\n",
            "reflection tokens about ISRELand ISSUPmatch their inspections (e.g., whether the fully supported\n",
            "output is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\n",
            "plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 228}\n",
            "output is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\n",
            "plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\n",
            "consistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token\n",
            "predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\n",
            "examples and explanations on assessments.\n",
            "6 C ONCLUSION\n",
            "This work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\n",
            "through retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\n",
            "and critique text passages and its own generation by predicting the next tokens from its original\n",
            "vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\n",
            "the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\n",
            "six tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\n",
            "more parameters or with conventional retrieval-augmented generation approaches.\n",
            "10Preprint.\n",
            "ETHICAL CONCERNS\n",
            "This work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\n",
            "merous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\n",
            "advice). While our method shows significant improvements in terms of performance, factuality, and\n",
            "citation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\n",
            "that explicit self-reflection and fine-grained attribution may help users verify factual errors in the\n",
            "model outputs.\n",
            "ACKNOWLEDGMENTS\n",
            "We thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\n",
            "in the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\n",
            "for valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 229}\n",
            "in the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\n",
            "for valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\n",
            "Akari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing\n",
            "to train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research\n",
            "Program for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program\n",
            "through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\n",
            "{'filename': '2310.11511_v1.pdf', 'title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through  Self-Reflection', 'vs_index': 230}\n",
            "LLM A UGMENTED LLM S:\n",
            "EXPANDING CAPABILITIES THROUGH COMPOSITION\n",
            "Rachit Bansal1Bidisha Samanta1Siddharth Dalmia2Nitish Gupta1Shikhar Vashishth1\n",
            "Sriram Ganapathy1Abhishek Bapna1Prateek Jain1Partha Talukdar1\n",
            "1Google Research2Google DeepMind\n",
            "ABSTRACT\n",
            "Foundational models with billions of parameters which have been trained on large\n",
            "corpora of data have demonstrated non-trivial skills in a variety of domains. How-\n",
            "ever, due to their monolithic structure, it is challenging and expensive to augment\n",
            "them or impart new skills. On the other hand, due to their adaptation abilities,\n",
            "several new instances of these models are being trained towards new domains and\n",
            "tasks. In this work, we study the problem of efficient and practical composition\n",
            "of existing foundation models with more specific models to enable newer capa-\n",
            "bilities. To this end, we propose CALM—Composition to Augment Language\n",
            "Models—which introduces cross-attention between models to compose their rep-\n",
            "resentations and enable new capabilities. Salient features of CALM are: (i) Scales\n",
            "up LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional\n",
            "parameters and data, (ii) Existing model weights are kept intact, and hence pre-\n",
            "serves existing capabilities, and (iii) Applies to diverse domains and settings. We\n",
            "illustrate that augmenting PaLM2-S with a smaller model trained on low-resource\n",
            "languages results in an absolute improvement of up to 13% on tasks like trans-\n",
            "lation into English and arithmetic reasoning for low-resource languages. Simi-\n",
            "larly, when PaLM2-S is augmented with a code-specific model, we see a relative\n",
            "improvement of 40% over the base model for code generation and explanation\n",
            "tasks—on-par with fully fine-tuned counterparts.\n",
            "1 I NTRODUCTION\n",
            "Large Language Models (LLMs) have shown to encompass a range of foundational capabilities\n",
            "such as commonsense and factual reasoning, world knowledge, and coherent language generation\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 231}\n",
            "1 I NTRODUCTION\n",
            "Large Language Models (LLMs) have shown to encompass a range of foundational capabilities\n",
            "such as commonsense and factual reasoning, world knowledge, and coherent language generation\n",
            "(Bubeck et al., 2023; Google et al., 2023). Leveraging these foundational capabilities, a number of\n",
            "efforts in the community have fine-tuned these models to enable domain-specific capabilities such as\n",
            "code generation, copy editing, and mathematical problem solving (Lewkowycz et al., 2022; Singhal\n",
            "et al., 2023). This has resulted in the development of several specialized large models with domain-\n",
            "specific capabilities. For example, there are models that do well on standard code generation but\n",
            "are not as proficient in general logical reasoning and vice-versa. Presence of such a large number\n",
            "of domain-specific models leads to a natural question: Can we compose an anchor model with\n",
            "a domain-specific augmenting model to enable new capabilities? For example, can we compose\n",
            "an augmenting model’s code understanding capability with an anchor LLM’s language generation\n",
            "capability to enable code-to-text generation capability?\n",
            "The typical approach for this problem is to further pre-train or (efficiently) fine-tune the anchor\n",
            "model on the data that was originally used to train the augmenting model (Hu et al., 2022; Kessler\n",
            "et al., 2021). However, many a times such solutions are not feasible since training large models is\n",
            "computationally expensive, especially since the augmenting model itself may be an LLM trained\n",
            "on a massive corpora. Further, processing data from multiple sources might not be feasible due\n",
            "to privacy concerns and organizational boundaries. Working with multiple distinct models is also\n",
            "desirable since it allows the reuse of existing models with established capabilities, providing better\n",
            "control and avoiding catastrophic forgetting that is prevalent in conventional approaches.\n",
            "Correspondence to Rachit and Bidisha: [brachit, bidishasamanta]@google.com\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 232}\n",
            "control and avoiding catastrophic forgetting that is prevalent in conventional approaches.\n",
            "Correspondence to Rachit and Bidisha: [brachit, bidishasamanta]@google.com\n",
            "1arXiv:2401.02412v1  [cs.LG]  4 Jan 2024mBmA\n",
            "Translate from XX to En:\n",
            "<Source XX Sentence>Everything but the kitchen sink\n",
            "Low -resource\n",
            "Language \n",
            "Pre-trained\n",
            "What does this Python code do?\n",
            "<Python Code Snippet>Implements the classic word \n",
            "game of Hangman\n",
            "m BKey-value \n",
            "Mapping\n",
            "x1 = 10\n",
            "     \n",
            "xn = 2x1 = 10\n",
            "     \n",
            "xn = 2\n",
            "m A\n",
            "lA,ilA,i lB,jlB,(j+1)\n",
            "WKWVWQAttention\n",
            "What is the value of x 1 + x 8 * xn?Since x 1=10, x 8=14, x n=2, x 1 + x 8 * xn = 38\n",
            "mBmAPre-trained \n",
            "on GitHubNumeric Arithmetic\n",
            "Figure 1: Overview of CALM. To augment an anchor LLM ( mB) with new capabilities through\n",
            "composition with a specialized augmenting model ( mA). Figure illustrates three mAwith differ-\n",
            "ent capabilities: key-value mapping ( left), low-resource languages ( center ), and code ( right ). Mod-\n",
            "elsmAandmBremain unchanged ( ^) during composition. A few additional parameters are learnt\n",
            "over models’ layer representations. Leftmost plot shows an mAtrained on a set of string-integer\n",
            "mappings, e.g., {x1: 10,. . .,xn: 2}.mBis a large LM with arithmetic capabilities. CALM com-\n",
            "poses these two frozen models to solve the task of arithmetic on keys which either models could not\n",
            "solve on their own (§4.1). Notably, CALM generalizes to the entire key-value set despite training\n",
            "with arithmetic examples spanning only 20% of the keys.\n",
            "To address the training and the data challenges mentioned above, we propose and study a practical\n",
            "setting for model composition : (i) we are given access to one (or more) augmenting model(s) and an\n",
            "anchor model, (ii) we are not allowed to modify the weights of either models, and (iii) we only have\n",
            "access to a small amount of data, representing the “combined skills” of the given models, e.g., code\n",
            "generation with complex logical reasoning.\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 233}\n",
            "access to a small amount of data, representing the “combined skills” of the given models, e.g., code\n",
            "generation with complex logical reasoning.\n",
            "Prior work has largely approached the question of composition from either a routing or a merging\n",
            "standpoint, neither of which provide an effective solution to capture this setting. Routing between the\n",
            "given models, i.e., choosing an output of one model over the other (Ma et al., 2019), or performing a\n",
            "soft ensemble (Muqeeth et al., 2023) is not effective when neither of the models can demonstrate the\n",
            "desired capability. Another body of work creates a combined model by an arithmetic combination\n",
            "of base model parameters (Wortsman et al., 2022; Ilharco et al., 2022; Matena & Raffel, 2022).\n",
            "However, these settings are naturally restrictive and their efficacy is unclear when combining models\n",
            "with different sizes and pre-training objectives (Yadav et al., 2023).\n",
            "In this work, we propose a novel Composition to Augment Language Models (CALM ) framework\n",
            "to address the general model composition setting mentioned above. Rather than a shallow combi-\n",
            "nation of the augmenting and anchor LMs (Wortsman et al., 2022; Ilharco et al., 2022), CALM\n",
            "introduces a small number of trainable parameters over both augmenting and anchor models’ inter-\n",
            "mediate layer representations. CALM finds an effective combination of the given models to perform\n",
            "new challenging tasks more accurately than either of the models alone, while preserving the capa-\n",
            "bilities of individual models. Figure 1 highlights few motivating scenarios for CALM.\n",
            "We study key practical applications of CALM: language inclusivity and code generation. For lan-\n",
            "guage inclusivity (§4.2), we use a model that has been trained on a set of low-resource languages.\n",
            "We observe that composing this model with the LLM allows us to borrow its generation and reason-\n",
            "ing capabilities to achieve significantly better performance on translation and arithmetic reasoning\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 234}\n",
            "We observe that composing this model with the LLM allows us to borrow its generation and reason-\n",
            "ing capabilities to achieve significantly better performance on translation and arithmetic reasoning\n",
            "tasks for low-resource languages (Tables 2 and 3). This composed model outperforms not only the\n",
            "two base models but also versions of the LLM that have been further pre-trained or LoRA (Hu et al.,\n",
            "2022) fine-tuned for the set of low-resource languages. For code generation (§4.3), we use a model\n",
            "that has been trained on open-source code across a variety of programming languages. Compos-\n",
            "ing this model with the LLM—hence borrowing its low-level logic and generation capabilities—\n",
            "outperforms the two base models (Table 4) on code explanation and code completion tasks.\n",
            "22 R ELATED WORKS\n",
            "Parameter efficient fine-tuning: A large body of work focuses on efficient ways of fine-tuning\n",
            "models for new domains by introducing a small number of trainable parameters, keeping the original\n",
            "model intact (Houlsby et al., 2019; Wang et al., 2021; Pfeiffer et al., 2021; Hu et al., 2022; Kessler\n",
            "et al., 2021). Since this paradigm allows a small set of new parameters to be trained, it is challenging\n",
            "to use this approach to adapt a model to a new domain, which is absent from the original training\n",
            "corpus. In contrast, CALM enables a model to be adapted to completely new domains using an\n",
            "augmenting model. In Section 4.4, we demonstrate that CALM is significantly more effective than\n",
            "LoRA (Hu et al., 2022), a representative parameter efficient fine-tuning method.\n",
            "Model Merging: Merging different expert models with simple techniques like task vector aver-\n",
            "aging provides a way of recombining different capabilities of these models (Ilharco et al., 2022;\n",
            "Matena & Raffel, 2022). However, these methods are only relevant when the original models are\n",
            "well aligned. Other related approaches are also applicable only when the models are derived from\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 235}\n",
            "Matena & Raffel, 2022). However, these methods are only relevant when the original models are\n",
            "well aligned. Other related approaches are also applicable only when the models are derived from\n",
            "the same model (Matena & Raffel, 2022) or they are of same size (Muqeeth et al., 2023). In contrast,\n",
            "CALM is more generic and is applicable to any set of models.\n",
            "Model and Task Compositionality: The modular encoder-decoder based method in (Dalmia\n",
            "et al., 2022) adapts components of encoder-decoder models to allow flexible re-usability of dif-\n",
            "ferent encoders, each with their own capabilities. Several past studies explore compositionality\n",
            "from a multi-modal standpoint. Alayrac et al. (2022) introduce cross-attention parameters across a\n",
            "language model in order to attend to representations coming from an image encoder. They show\n",
            "very effective transfer of capabilities between the two models. In this work, we extend the ideology\n",
            "of model re-use and modularity to extend composition of capabilities in a large language model.\n",
            "Models as Tools: Another interesting direction for using multiple language models to solve a\n",
            "downstream task has been to perform composition in the models’ input text space (Zeng et al.,\n",
            "2022; Shen et al., 2023). Schick et al. (2023) have demonstrated how a model can be taught to use\n",
            "external tools—there might be an opportunity to investigate if other models can be called as a part\n",
            "of the same framework. Since these approaches require a large amount of prompt engineering, in\n",
            "this work we focus on composition through representations that can be learnt automatically.\n",
            "3 C OMPOSITION TO AUGMENT LANGUAGE MODELS (CALM)\n",
            "Given an anchor model mBand an augmenting model mA, CALM aims to compose the two models\n",
            "(mA⊕B) to enable new capabilities as a composition of capabilities of the two individual models.\n",
            "As discussed in the introduction, we study this composition in a practical setting with the following\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 236}\n",
            "(mA⊕B) to enable new capabilities as a composition of capabilities of the two individual models.\n",
            "As discussed in the introduction, we study this composition in a practical setting with the following\n",
            "assumptions: i) we can access weights, run forward and backward pass, and access intermediate\n",
            "representations of both mBandmA, ii) we are not allowed to change weights of both the models,\n",
            "iii) we do not have access to the training data, hyperparameters, training states of both the base\n",
            "models, iv) we are provided a few examples from the target composition domain.\n",
            "The goal is to learn a composition mA⊕B=f(mA,mB,ΘC,DC) to achieve some joint task C. The\n",
            "weights of mAandmBare frozen. ΘCis the additional set of trainable parameters introduced to\n",
            "learn the composition and DCrefers to the set of examples that are used to learn this composition.\n",
            "3.1 L EARNING TO COMPOSE (ΘC)\n",
            "As outlined in Figure 1, we operate over a selected set of layers from mBandmAat all times. We\n",
            "learn two sets of additional parameters over these layers: (i) A simple set of linear transformations,\n",
            "fproj(.) that maps an ithlayer representation from mAto the dimensionality of representations from\n",
            "mB, and (ii) A set of cross-attention layers, fcross(.,.) that cross-attend between this transformed\n",
            "layer representation and a jthlayer representation from mB.\n",
            "Compositional Layers: Let the augmenting model mAand the anchor model mBhaveNAand\n",
            "NBlayers, respectively. Also, let DAandDBbe the token dimensionality of the two models. We\n",
            "first choose a set of compositional layers— LAandLB—for both models, over which the set of new\n",
            "3learnable parameters are introduced during composition. nA=|LA|andnB=|LB|. For simplicity,\n",
            "we set nA=nB=nand the gap between two contiguous selected layers is kept uniform based\n",
            "on the number of selected layers—that is, ( l2−l1) =···= (ln−l(n−1)) =N/n . Further, HA\n",
            "∈ {HA1,HA2, . . . ,HAnA}denote the layer representation of a given input after each layer in LA.\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 237}\n",
            "on the number of selected layers—that is, ( l2−l1) =···= (ln−l(n−1)) =N/n . Further, HA\n",
            "∈ {HA1,HA2, . . . ,HAnA}denote the layer representation of a given input after each layer in LA.\n",
            "Learned Projections: Next we map representations from mAto that of mBvia a projection layer.\n",
            "In particular, for each layer in LA, we learn a projection function fproj:RDA→RDB, that projects\n",
            "representations from these layers to the desired representation size of mB. Let,\n",
            "fproj(HA)← − {fproj(HA1), fproj(HA2), . . . , f proj(HAnA)}\n",
            "This transformation enables cross-attention across models, and also performs an alignment of rep-\n",
            "resentations from mAandmBdespite frozen weights of the base models.\n",
            "Cross-attention Layers: Similar to the multi-headed cross-attention in encoder-decoder models\n",
            "(for example Vaswani et al. (2017) and Raffel et al. (2020))—we introduce cross-attention between\n",
            "representations of the anchor and the augmenting model. In particular, we use fproj(HAi)from the\n",
            "augmenting model as the keyandvalue vectors for each head in cross-attention. We use the vector\n",
            "HBjfrom the anchor model as the query vector, which leads to the following cross-attention setup:\n",
            "fcross(fproj(HAi),HBj) =Concat. k(head k)WO∀k∈NH\n",
            "where, head k=Attn.(QB,KA,VA),\n",
            "and,QB=HBjWQ\n",
            "k,\n",
            "KA,VA=fproj(HAi)WK\n",
            "k, fproj(HAi)WV\n",
            "k\n",
            "Here, NHrepresents the number of attention heads used for cross-attention which, in our case, is\n",
            "typically the same as the number of heads used for self-attention in mB. Each of WO∈RDB×DB,\n",
            "WQ\n",
            "k,WK\n",
            "k, andWV\n",
            "k∈RDB×DB//NHare learnable weight matrices, where k∈ {1..NH}.\n",
            "Finally, the cross-attention output is added as a residual connection to the layer representations of\n",
            "mB. The resultant output vector, in-turn, is the input to the succeeding layer in mB:\n",
            "HA⊕Bj=HBj+fcross(fproj(HAi),HBj)\n",
            "Here,HA⊕Bjdenotes the input to the (j+ 1)thlayer of the composed model. All layers in LAand\n",
            "LBare utilized in a similar manner. Propagating over the remaining layers in mBgives us a final\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 238}\n",
            "Here,HA⊕Bjdenotes the input to the (j+ 1)thlayer of the composed model. All layers in LAand\n",
            "LBare utilized in a similar manner. Propagating over the remaining layers in mBgives us a final\n",
            "output token ytdecoded for the tthtimestep. Akin to usual auto-regressive decoding, the output\n",
            "token for each time-step is appended to the input: xt+1=xt⊕yt, Since the updated input at each\n",
            "time step is passed to both models, all representations for the two models are refreshed.\n",
            "3.2 C OMPOSITION TRAINING DATA (DC)\n",
            "Since the target model mA⊕Binvolves a composition over the two models mAandmB, we construct\n",
            "the set of training examples DCto depict a “combined skill” that enables ΘCto attend over the two\n",
            "models appropriately for the target task.\n",
            "Ideally, if the set of tasks involved in composition task are distinguished as t1andt2respectively,\n",
            "then we design DCto depict the a joint task C. For example, with respect to our synthetic key-value\n",
            "setup: our final task ( C) is to perform arithmetic over a set of keys. The augmenting model mAis\n",
            "trained to learn the given key-value pairs (notated as task, t1) and the anchor model mBis generic\n",
            "model that can perform numeric arithmetic well (task t2). For learning the set of parameters ΘCfor\n",
            "composition, we consider DCto be arithmetic over a held-in set of keys (task C), encompassing\n",
            "combined skills from the two models. In contrast to fine-tuning approaches like LoRA (Hu et al.,\n",
            "2022) that would require the entire knowledge source (here, key-values) during training time, we\n",
            "find that training composition on only a fraction of the keys can generalize to the full set.\n",
            "In other real world settings, a clear distinction in specializing tasks for each model might be difficult\n",
            "to formulate and hence defining a task that captures the combined skills can be challenging. We find\n",
            "that using a set of examples that capture certain capabilities of the two models suffices, i.e., some\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 239}\n",
            "that using a set of examples that capture certain capabilities of the two models suffices, i.e., some\n",
            "rough notion of tA∪B. For our language inclusivity task, we use a mixture of examples containing\n",
            "a small amount of low-resource language and high-resource language data.\n",
            "4Composing multiple models: Finally, we note that while the method has been presented for a\n",
            "setting with one anchor model and only one augmenting model, CALM is applicable to multiple\n",
            "augmenting models as well. In particular, CALM would require learning similar projection and\n",
            "cross-attention components between the anchor and each of the augmenting model. We leave a\n",
            "thorough investigation of this as a topic of future work.\n",
            "4 E XPERIMENTS\n",
            "We demonstrate the following in three domains: (a)an anchor LLM ( mB) can be composed with an\n",
            "augmenting model ( mA) trained on mappings between string keys and number values to solve arith-\n",
            "metic expressions over those keys requiring both, knowledge of the KV mappings and arithmetic\n",
            "capabilities (§4.1); (b)how CALM can be used to expand the language coverage of an anchor LLM\n",
            "(mB) to low-resource languages it has not seen during pre-training. We show that an augmenting\n",
            "model ( mA) pre-trained on low-resource languages can be composed with such an anchor model to\n",
            "significantly improve translation and math-word problem solving capabilities in low-resource lan-\n",
            "guages (§4.2); (c)how code completion and explanation can be improved by composing an anchor\n",
            "LLM with an augmenting model ( mA) specializing in the code domain (§4.3).\n",
            "In all experiments, we start with a PaLM2-XXS model and further train it on domain-specific data to\n",
            "arrive at an augmenting model ( mA) that is then kept frozen during composition. Note that no task\n",
            "specific training data was used to train CALM. We use PaLM2-XS or PaLM2-S models as the anchor\n",
            "LLM ( mB) that is also kept frozen during composition training. For all our experiments, we set\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 240}\n",
            "specific training data was used to train CALM. We use PaLM2-XS or PaLM2-S models as the anchor\n",
            "LLM ( mB) that is also kept frozen during composition training. For all our experiments, we set\n",
            "NA/n= 4, i.e., we perform composition using every 4th layer output from mA. Correspondingly,\n",
            "layers from mA(LB) are chosen such that nB=nA=n, hence nB=NA/4.\n",
            "4.1 K EY-VALUE ARITHMETIC\n",
            "We first study the setting where we have a small augmenting LM that has been trained to memorize\n",
            "string-to-integer key-value (KV) mappings, and a large anchor LM that is capable of performing\n",
            "arithmetic over integers. We wish to use CALM to compose them and enable a new capability of\n",
            "solving arithmetic expressions containing those keys.\n",
            "Key-Value Domain Knowledge We first generate a repository of KV pairs containing N KV= 25 K\n",
            "pairs by sampling English strings of length 2−6characters from the vocabulary of the PaLM2-XXS\n",
            "model and randomly assigning them unique integer values in the range [1,NKV]. This constitutes\n",
            "the knowledge artifact, DKV. We further generate a collection of arithmetic expressions ( DKV-EXP )\n",
            "containing addition ( +), subtraction ( −), and multiplication ( ×) operations between 3−6keys by\n",
            "randomly sampling keys from DKVand operations to perform between them.\n",
            "Using these arithmetic expressions, we generate three datasets:\n",
            "(i)KV-Substitution ( DKV-SUBS ): This dataset maps each expression in DKV-EXP , to an expression\n",
            "where the keys are replaced by their corresponding values. For example, this dataset contains exam-\n",
            "ples of the form ( <K1> +<K2>−<K3> ,10 + 22 −24).\n",
            "(ii)KV-Arithmetic ( DKV-MATH ): This dataset maps each expression in DKV-EXP to the numeric value\n",
            "arrived at by solving the arithmetic expression when the keys would be replaced by the correspond-\n",
            "ing values. For example, examples in this dataset look like ( <K1> +<K2>−<K3> ,8).\n",
            "(iii) Numeric-Arithmetic ( DNUM-MATH ): This dataset maps the value substituted version of each\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 241}\n",
            "ing values. For example, examples in this dataset look like ( <K1> +<K2>−<K3> ,8).\n",
            "(iii) Numeric-Arithmetic ( DNUM-MATH ): This dataset maps the value substituted version of each\n",
            "expression in DKV-EXP to the numeric value arrived at by solving the arithmetic expression. For\n",
            "example, examples in this dataset look like ( 10 + 22 −24,8).\n",
            "Models We obtain augmenting model mAby further training a pre-trained PaLM2-XXS model on\n",
            "DKV-SUBS to make it memorize the KV pairs in DKV. Note that, training on DKV-SUBS does not teach\n",
            "this augmenting model how to solve arithmetic expressions. Next, we use a pre-trained PaLM2-XS\n",
            "model as the anchor model mB. This model is capable of solving numeric expressions with decent\n",
            "performance (see Table 1). Note that, this model has no knowledge of the KV pairs in DKV.\n",
            "We now take examples from the KV-Substitution dataset DKV-SUBS that only span 20% of the keys in\n",
            "DKVto form the training data for composition ( DC). We use DCto compose the augmenting model\n",
            "5(mA) having knowledge of DKVand the pre-trained anchor model mBby training the composition\n",
            "parameters ( ΘC) using CALM as explained in §3. Both mAandmBare kept unchanged.\n",
            "Evaluation Task We evaluate the composed model mA⊕Bfor its ability to solve arithmetic ex-\n",
            "pressions containing keys from DKV. Specifically, we evaluate on the subset of DKV-MATH dataset\n",
            "that does not contain expressions used in DCduring training. This way, we are able to measure the\n",
            "composed model’s ability to generalize to keys beyond what was observed during training.\n",
            "mAmBCALM\n",
            "(mA⊕B)\n",
            "DKV-SUBS 98.1 0.0 92.9\n",
            "DNUM-MATH 4.2 73.7 72.0\n",
            "DKV-MATH 0.7 0.0 84.3\n",
            "Table 1: Evaluation (accuracy (%)) for\n",
            "a synthetic key-value (KV) task. mA\n",
            "is trained to memorize the KV mappings\n",
            "while mBexcels at arithmetic We see that\n",
            "a composition mA⊕Bis able to perform\n",
            "arithmetic over held-out keys.Results Table 1 shows the performance of the three\n",
            "models: mA,mB, and mA⊕Bacross the aforemen-\n",
            "tioned datasets. First, we observe that the augmenting\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 242}\n",
            "arithmetic over held-out keys.Results Table 1 shows the performance of the three\n",
            "models: mA,mB, and mA⊕Bacross the aforemen-\n",
            "tioned datasets. First, we observe that the augmenting\n",
            "model mAachieves 98.1%at the KV-Substitution task\n",
            "showing that memorizes DKVwell. Next, we see that\n",
            "it performs poorly ( 4.2%) at the Numeric-Arithmetic\n",
            "task showing that it does not have arithmetic capabili-\n",
            "ties. As a result, this model is not able to solve arith-\n",
            "metic expressions containing keys from DKV.\n",
            "As expected, the anchor model mBgets0%accuracy\n",
            "on the KV-Substitution and KV-Arithmetic tasks as it\n",
            "has not seen any data from DKV. However, it performs\n",
            "well ( 73.7%) on the Numeric-Arithmetic task demon-\n",
            "strating capability of arithmetic over numerals.\n",
            "Lastly, we see that the composed model mA⊕Bis able\n",
            "to solve all tasks with high accuracy, especially the KV-Arithmetic task ( 84.3%) which both the\n",
            "underlying models fail at. This shows that the composed model is able to leverage the relevant\n",
            "capabilities from both the augmenting and anchor model to solve a complex task.\n",
            "4.2 L OW-RESOURCE LANGUAGE INCLUSIVITY\n",
            "FLORES-200 (XX to En; chrF1)\n",
            "Modellij mr taq nn su ban pl th min acm avg.\n",
            "PaLM2-XXS 24.0 16.5 21.6 33.3 20.6 2.1 5.3 63.2 44.0 59.8 29.0\n",
            "+NTL ( mA) 32.0 21.6 46.9 50.0 40.6 4.1 4.0 63.8 47.8 61.1 37.2\n",
            "PaLM2-S ( mB) 32.6 24.2 44.6 50.8 50.9 5.4 9.5 69.0 61.0 68.6 41.7\n",
            "CALM ( mA⊕B) 44.1 30.4 55.1 54.6 54.4 11.8 11.3 69.4 61.1 68.9 46.1\n",
            "mB+NTL ( mNTL\n",
            "B)48.1 39.1 59.2 57.5 57.3 11.4 9.9 69.4 61.4 69.0 48.2\n",
            "Table 2: Translation performance for XX to English direction on the FLORES-200 dataset (Costa-\n",
            "juss`a et al., 2022): We show results for a subset of 10 low-resource languages. Note that the com-\n",
            "posed model mA⊕Bsignificantly outperforms both mAandmB. On the complete language list,\n",
            "mA⊕Boutperforms both the underlying models for 175 of 192 languages (Appendix A; Figure 2).\n",
            "mNTL\n",
            "Brepresents a skyline where mBhas been further pre-trained on DNTL. The composed model\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 243}\n",
            "mA⊕Boutperforms both the underlying models for 175 of 192 languages (Appendix A; Figure 2).\n",
            "mNTL\n",
            "Brepresents a skyline where mBhas been further pre-trained on DNTL. The composed model\n",
            "achieves similar performance for a tiny fraction of the training cost.\n",
            "In this section, we study if we can compose such a large anchor LM mBwith a smaller augmenting\n",
            "LMmAthat has been pre-trained on low-resource languages, to perform translation and math-word\n",
            "problem solving tasks presented in these low-resource languages.\n",
            "Low-resource Language Corpora We use the long-tail language set and the associated corpora\n",
            "from the Next Thousand Languages (NTL) effort (Caswell et al., 2020; Bapna et al., 2022) as the\n",
            "domain data DNTL. This large-scale corpora contains web-crawled monolingual sentences and trans-\n",
            "lation pairs for ∼1000 languages. The dataset has been used for language expansion in translation\n",
            "systems and language models (Garcia et al., 2021; Siddhant et al., 2022).\n",
            "6GSM8K (Low-resource Languages; Accuracy)\n",
            "Modelmeo mfa pcm efi min ilo ady mai nso mzn avg.\n",
            "PaLM2-XXS 5.2 6.8 6.8 4.0 5.6 7.2 6.0 3.6 7.2 6.8 5.9\n",
            "+NTL ( mA) 7.6 4.0 4.4 3.2 6.0 4.8 6.4 3.2 6.0 4.8 5.0\n",
            "PaLM2-S ( mB)28.8 14.0 34.4 14.8 25.2 14.8 30.0 22.8 8.4 31.6 22.5\n",
            "CALM ( mA⊕B)34.0 17.6 33.6 18.0 23.6 16.8 36.4 24.8 8.4 36.4 25.0\n",
            "mNTL\n",
            "B 33.2 20.4 31.6 14.0 24.8 14.0 29.2 21.2 9.6 27.6 22.6\n",
            "(High-resource Languages)\n",
            "Modelen te bn sw ja zh th fr es de avg.\n",
            "PaLM2-XXS 5.6 4.0 2.0 7.6 2.0 4.4 6.0 6.8 5.6 9.2 5.3\n",
            "+NTL ( mA) 4.8 3.6 3.2 4.8 3.2 7.6 6.4 9.2 5.6 7.2 5.6\n",
            "PaLM2-S ( mB)36.8 19.2 23.2 16.0 2.0 39.2 29.6 38.0 32.4 43.2 28.0\n",
            "CALM ( mA⊕B)37.2 28.0 27.2 18.0 2.4 43.6 33.2 42.8 36.0 49.2 31.8\n",
            "mNTL\n",
            "B 36.0 17.6 18.4 14.4 0.8 33.6 27.2 34.8 31.2 42.0 25.6\n",
            "Table 3: Evaluations for grade-school mathematics (GSM) problems on low-resource (LRL) and\n",
            "high-resource (HRL) languages. We observe that CALM yields significant gains for both evaluation\n",
            "sets. Gains on the HRL set suggests that CALM avoids catastrophic forgetting.\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 244}\n",
            "high-resource (HRL) languages. We observe that CALM yields significant gains for both evaluation\n",
            "sets. Gains on the HRL set suggests that CALM avoids catastrophic forgetting.\n",
            "Models Akin to §4.1, we obtain augmenting model mAby training the PaLM2-XXS model on\n",
            "DNTLto impart knowledge about these low-resource languages to the model. For mB, we use the\n",
            "pre-trained PaLM2-S model. We use ∼5%of the same low-resource language corpora DNTLas\n",
            "the training data DCto compose mAandmBvia CALM. Since both models are untrained during\n",
            "composition, the anchor model mBisnottrained on any of the low-resource language data.\n",
            "Evaluation Tasks We evaluate the composed model mA⊕Bon two tasks:\n",
            "(i)Translating text from a non-English language to English: We carry out these evaluations in a\n",
            "5-shot in-context learning paradigm on the FLORES-200 (Costa-juss `a et al., 2022) dataset. This\n",
            "dataset contains examples for 200 high- and low-resource languages.\n",
            "(ii)Performing grade school math word problems expressed in a non-English language: We evaluate\n",
            "on the multilingual version of the GSM-8K dataset (Shi et al., 2023) containing math word problems\n",
            "for English and 9 other high-resource languages. We further generated a silver-standard GSM-8K\n",
            "dataset for low-resource languages by automatically translating the English examples in GSM-8K\n",
            "to 25 low-resource languages supported by Google Translate.1\n",
            "Results Table 2 shows results on the FLORES-200 dataset (Costa-juss `a et al., 2022), where the\n",
            "input is a low-resource (XX) language sentence and the output should be the corresponding English\n",
            "translation. For 10 low-resource languages shown in the Table, we see that both the underlying\n",
            "models mAandmBare outperformed by our composed model mA⊕B. We find that the composed\n",
            "model mA⊕Boutperforms mBon 175 of the complete set of 192 languages (Appendix A).\n",
            "Table 3 shows the performance of these models on the grade-school math word problems from the\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 245}\n",
            "model mA⊕Boutperforms mBon 175 of the complete set of 192 languages (Appendix A).\n",
            "Table 3 shows the performance of these models on the grade-school math word problems from the\n",
            "GSM8K task (Cobbe et al., 2021) on low-resource languages ( top) and high-resource languages (Shi\n",
            "et al. (2023); bottom ). Firstly, we observe that the augmenting model mAdoes not perform well on\n",
            "this task due to its limited mathematical reasoning capabilities. On the other hand, the anchor model\n",
            "mBdoes much better given its mathematical reasoning capabilities and transfer-learning from high-\n",
            "resource languages. Finally, we observe that mA⊕Boutperforms both mAandmBon18 of 25\n",
            "low-resource and 9 of 10 high-resource languages, demonstrating effective composition of models.\n",
            "See Table 6 (Appendix A.2) for a complete set of evaluations. Note that the last row in Table 3 shows\n",
            "thatmBwhen fine-tuned on DNTLleads to worse performance than the pre-trained mBindicating\n",
            "forgetting. Composing domain-specific model mAwithmBusing CALM avoids this.\n",
            "1We perform quality evaluations in Appendix 7.\n",
            "7Model CC (P@1) T2C (P@1) C2T (chrF1)\n",
            "HumanEval MBPP Python PHP Go Java JS Ruby\n",
            "PaLM2-XXS\n",
            "+ Code ( mA)19.5 28.0 28.0 34.7 32.6 29.6 26.5 26.0\n",
            "PaLM2-S ( mB) 16.4 28.6 30.4 35.5 40.4 31.0 28.8 27.9\n",
            "CALM ( mA⊕B) 22.5 32.2 30.5 35.8 40.6 31.4 29.3 29.0\n",
            "mCode\n",
            "B 24.3 43.0 18.9 35.0 41.1 31.1 20.2 27.6\n",
            "Table 4: Evaluations for code generation and understanding across three tasks: Code Completion\n",
            "(CC), Text-to-Code (T2C), and Code-to-Text (C2T). Augmenting code understanding to mBusing\n",
            "mAsignificantly improves performances across all datasets. mCode\n",
            "Brepresents a skyline where mB\n",
            "further pretrained on the DCode, which shows catastrophic forgetting of text generation task.\n",
            "4.3 C ODE UNDERSTANDING AND GENERATION\n",
            "Code understanding and generation require two distinct types of capabilities: ( a) knowledge of the\n",
            "syntax and semantics of code, and ( b) knowledge of the world that the code is manipulating. While\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 246}\n",
            "Code understanding and generation require two distinct types of capabilities: ( a) knowledge of the\n",
            "syntax and semantics of code, and ( b) knowledge of the world that the code is manipulating. While\n",
            "LLMs have a wealth of world knowledge, they could often lack the specific knowledge of code\n",
            "syntax due to a skewed representation of code data in their pretraining corpora. Conversely, small\n",
            "models trained specifically on code data could exhibit a good understanding of code syntax, but they\n",
            "may lack broad world knowledge and reasoning. CALM can enable best of both worlds.\n",
            "Code Domain Data Here, we use the code-specific corpus, DCode, consisting of open-source code\n",
            "extracted from GitHub heads for a variety of programming languages to train mA.\n",
            "Models Similar to §4.1, a version of the PaLM2-XXS model has been further pre-trained on DCode\n",
            "is used as mA, while the base pre-trained PaLM2-S model acts as mB. We build mA⊕Bby training\n",
            "CALM with only 7% of the same code data (data used for mA) to have a data parity.\n",
            "Evaluation Tasks We evaluate the efficacy of CALM on three different tasks:\n",
            "(i)Code-Completion (CC): Given an initial set of lines of a code, the model is prompted to complete\n",
            "the code snippet. Here the aim is to evaluate the model for code syntax. We perform zero-shot eval-\n",
            "uations on HumanEval benchmark dataset (Chen et al., 2021) and report the Pass@1 (P@1) metric.\n",
            "(ii)Text-to-Code (T2C): Given a textual context, the model is prompted to generate the correspond-\n",
            "ing code snippet. Here, the evaluation indicates language understanding and code generation capa-\n",
            "bilities. We perform 3-shot inference on the MBPP dataset (Austin et al., 2021) and report P@1.\n",
            "(iii)Code-to-Text (C2T): Given a code snippet, the goal is to generate a natural language explanation\n",
            "of the code. This task evaluates code understanding and text generation. We perform 3-shot evalua-\n",
            "tions on the CodeXGlue benchmark (Lu et al., 2021) and report chrF1 scores across languages.\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 247}\n",
            "of the code. This task evaluates code understanding and text generation. We perform 3-shot evalua-\n",
            "tions on the CodeXGlue benchmark (Lu et al., 2021) and report chrF1 scores across languages.\n",
            "Results Table 4 reports comparative performance for the individual models mAandmB, the com-\n",
            "posed version mA⊕B, and a fine-tuned anchor baseline mCode\n",
            "B. Firstly, evaluations on the HumanEval\n",
            "dataset suggest that mAhas a superior understanding of code syntax as a result of its additional train-\n",
            "ing on DCode. While, due to the larger scale and general purpose pre-training of mB, it excels at\n",
            "general language understanding and hence performs better on the T2C and C2T tasks.\n",
            "When employing CALM to compose the two models, we observe a clear transfer and composition\n",
            "of capabilities through significant performance improvements: 6.1%and3.6%absolute gains over\n",
            "mBon the CC and T2C tasks, respectively. We observe that fine-tuning mBonDCodeleads to a\n",
            "significant decline in the C2T performance due to catastrophic forgetting. CALM retains the perfor-\n",
            "mance and is marginally better than mBacross all languages. We also study qualitative examples\n",
            "on the C2T task and observe interesting common patterns that are discussed in Appendix B.\n",
            "8mNTL/Code\n",
            "BCALM\n",
            "mA⊕BVanilla\n",
            "mARandom\n",
            "mAmAas an\n",
            "encoderLoRA\n",
            "chrF1 62.1 60.5 59.2 58.8 59.3 59.2 FLORES-200\n",
            "(XX-En) #( >mB) 171 175 115 43 102 82\n",
            "Accuracy 19.8 21.4 19.0 17.8 19.1 20.9 GSM-8K\n",
            "(LRL) #( >mB) 15 20 15 9 12 15\n",
            "Accuracy 27.1 33.1 29.7 28.5 29.1 31.2 GSM-8K\n",
            "(HRL) #( >mB) 1 11 8 4 6 9\n",
            "HumanEval Pass@1 24.3 22.5 20.0 20.1 16.0 18.3\n",
            "MBPP Pass@1 43.0 32.2 28.0 27.0 27.0 28.7\n",
            "CodeXGLUE chrF1 29.0 32.6 32.2 32.1 32.0 32.6\n",
            "Table 5: Comparative performance of CALM ( mA⊕B) across various possible ablations. The met-\n",
            "ric “#( >mB)” depicts the number of languages for which the corresponding model is better than\n",
            "the base for NTL, mB—out of 192, 25, and 11 languages for the three tasks respectively. For all\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 248}\n",
            "ric “#( >mB)” depicts the number of languages for which the corresponding model is better than\n",
            "the base for NTL, mB—out of 192, 25, and 11 languages for the three tasks respectively. For all\n",
            "compared settings, the number of added parameters are kept the same.\n",
            "4.4 A BLATIONS\n",
            "Influence of mAWe first study the influence of mAby replacing it with vanilla and random\n",
            "variants during composition. Table 5 shows the variation of performance across NTL and Code tasks\n",
            "when the specialized mAis replaced with a vanilla PaLM2-XXS checkpoint or an untrained version\n",
            "of the model, i.e., a random model. We see that there is a considerable drop of performance with\n",
            "these variants across all tasks. On FLORES-200 XX-En task, languages improved with composition\n",
            "drop to 115 and 43 with vanilla and random, respectively. A slight improvement of the vanilla model\n",
            "overmBindicates that an un-specialized model (with a different training regime than mB) might\n",
            "have orthogonal capabilities leading to an enhanced model. This finding validates that performance\n",
            "gains seen with CALM is a result of utilizing mAand not the added ΘCparameters.\n",
            "Influence of iterative decoding We also investigate a variation where we use mAas an encoder,\n",
            "i.e., an output token decoded at a given timestep is not amended to mA’s input. In this case, only the\n",
            "prefix representations of mAare used. This setting eludes to past work for image and text models\n",
            "(Alayrac et al., 2022) where encoder and decoder models are composed. We observe a significant\n",
            "decline in performance across our various tasks when employing this setting.\n",
            "Comparision with LoRA Finally, we evaluate a parameter efficient fine-tuning approach by train-\n",
            "ing LoRA (Hu et al., 2022) layers to adapt mB. For all experiments, we set the LoRA rank such\n",
            "that the number of added parameters is equal to the number of parameters introduced with CALM.\n",
            "We also train LoRA on the same data as CALM, i.e., DC. We see a considerable difference in\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 249}\n",
            "that the number of added parameters is equal to the number of parameters introduced with CALM.\n",
            "We also train LoRA on the same data as CALM, i.e., DC. We see a considerable difference in\n",
            "performance between the two approaches across all tasks and metrics.\n",
            "5 C ONCLUSION\n",
            "The proposed CALM framework composes an anchor LLM with specialized augmenting models to\n",
            "enable new tasks not achievable by either models individually. CALM does not require updating the\n",
            "individual models and learns a dense interaction between the models through a few trainable cross-\n",
            "attention parameters. Our experiments present consistent evidence that CALM learns to utilize the\n",
            "expertise from the two models. That is, when composed with relevant augmenting models, we\n",
            "observe a significant uptick in the anchor model’s performance across multiple challenging tasks,\n",
            "such as low-resource translation, reasoning, and code explanation/generation.\n",
            "That is, CALM is especially useful in scenarios where proprietary data and knowledge is stored in\n",
            "parametric models. With CALM, a foundational LLM could be augmented with such proprietary\n",
            "models to extend a variety of foundational capabilities such as reasoning, world knowledge, and\n",
            "coherent generation over the target proprietary domains. Finally, extensions of CALM could be\n",
            "used to acquire distinct knowledge from multiple augmenting models.\n",
            "9ACKNOWLEDGMENTS\n",
            "This work was done during RB’s pre-doctoral tenure at Google Research, India (GRI) with PT and\n",
            "PJ. RB is indebted to Manish Gupta, Divvy Thakkar, and all others who enabled this oppurtunity.\n",
            "RB would also like to thank the members of the Languages team and other researchers at GRI\n",
            "(and beyond), including the incredible pre-doctoral cohort. This work wouldn’t have been possible\n",
            "without their constant support. Namely: Aishwarya P.S., Laurent El Shafey, and Qiao Zhang for\n",
            "their massive help in coding and debugging; Palak Jain and Sagar Gubbi for their feedback and\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 250}\n",
            "without their constant support. Namely: Aishwarya P.S., Laurent El Shafey, and Qiao Zhang for\n",
            "their massive help in coding and debugging; Palak Jain and Sagar Gubbi for their feedback and\n",
            "support throughout the project; Kartikeya Badola, Shreyas Havaldar, Amandeep Kaur, and Rishabh\n",
            "Tiwari for being the first ears to all ideas; Cyrus Rashtchian and Richa Dixit for their mentorship.\n",
            "{'filename': '2401.02412_v1.pdf', 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'vs_index': 251}\n",
            "TinyLlama: An Open-Source Small Language Model\n",
            "Peiyuan Zhang∗Guangtao Zeng∗Tianduo Wang Wei Lu\n",
            "StatNLP Research Group\n",
            "Singapore University of Technology and Design\n",
            "{peiyuan_zhang, tianduo_wang, luwei}@sutd.edu.sg\n",
            "guangtao_zeng@mymail.sutd.edu.sg\n",
            "Abstract\n",
            "We present TinyLlama, a compact 1.1B language model pretrained on around 1\n",
            "trillion tokens for approximately 3 epochs. Building on the architecture and tok-\n",
            "enizer of Llama 2 (Touvron et al., 2023b), TinyLlama leverages various advances\n",
            "contributed by the open-source community (e.g., FlashAttention (Dao, 2023)),\n",
            "achieving better computational efficiency. Despite its relatively small size, TinyL-\n",
            "lama demonstrates remarkable performance in a series of downstream tasks. It\n",
            "significantly outperforms existing open-source language models with compara-\n",
            "ble sizes. Our model checkpoints and code are publicly available on GitHub at\n",
            "https://github.com/jzhang38/TinyLlama .\n",
            "1 Introduction\n",
            "Recent progress in natural language processing (NLP) has been largely propelled by scaling up\n",
            "language model sizes (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a,b). Large\n",
            "Language Models (LLMs) pre-trained on extensive text corpora have demonstrated their effectiveness\n",
            "on a wide range of tasks (OpenAI, 2023; Touvron et al., 2023b). Some empirical studies demonstrated\n",
            "emergent abilities in LLMs, abilities that may only manifest in models with a sufficiently large number\n",
            "of parameters, such as few-shot prompting (Brown et al., 2020) and chain-of-thought reasoning (Wei\n",
            "et al., 2022). Other studies focus on modeling the scaling behavior of LLMs (Kaplan et al., 2020;\n",
            "Hoffmann et al., 2022). Hoffmann et al. (2022) suggest that, to train a compute-optimal model, the\n",
            "size of the model and the amount of training data should be increased at the same rate. This provides\n",
            "a guideline on how to optimally select the model size and allocate the amount of training data when\n",
            "the compute budget is fixed.\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 252}\n",
            "a guideline on how to optimally select the model size and allocate the amount of training data when\n",
            "the compute budget is fixed.\n",
            "*The first two authors contributed equally.\n",
            "Technical Report, work in progress.arXiv:2401.02385v1  [cs.CL]  4 Jan 2024Although these works show a clear preference on large models, the potential of training smaller\n",
            "models with larger dataset remains under-explored. Instead of training compute-optimal language\n",
            "models, Touvron et al. (2023a) highlight the importance of the inference budget, instead of focusing\n",
            "solely on training compute-optimal language models. Inference-optimal language models aim for\n",
            "optimal performance within specific inference constraints This is achieved by training models with\n",
            "more tokens than what is recommended by the scaling law (Hoffmann et al., 2022). Touvron et al.\n",
            "(2023a) demonstrates that smaller models, when trained with more data, can match or even outperform\n",
            "their larger counterparts. Also, Thaddée (2023) suggest that existing scaling laws (Hoffmann et al.,\n",
            "2022) may not predict accurately in situations where smaller models are trained for longer periods.\n",
            "Motivated by these new findings, this work focuses on exploring the behavior of smaller models\n",
            "when trained with a significantly larger number of tokens than what is suggested by the scaling\n",
            "law (Hoffmann et al., 2022). Specifically, we train a Transformer decoder-only model (Vaswani et al.,\n",
            "2017) with 1.1B parameters using approximately 3 trillion tokens. To our knowledge, this is the\n",
            "first attempt to train a model with 1B parameters using such a large amount of data. Following the\n",
            "same architecture and tokenizer as Llama 2 (Touvron et al., 2023b), we name our model TinyLlama.\n",
            "TinyLlama shows competitive performance compared to existing open-source language models of\n",
            "similar sizes. Specifically, TinyLlama surpasses both OPT-1.3B (Zhang et al., 2022) and Pythia-\n",
            "1.4B (Biderman et al., 2023) in various downstream tasks.\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 253}\n",
            "similar sizes. Specifically, TinyLlama surpasses both OPT-1.3B (Zhang et al., 2022) and Pythia-\n",
            "1.4B (Biderman et al., 2023) in various downstream tasks.\n",
            "Our TinyLlama is open-source, aimed at improving accessibility for researchers in language model\n",
            "research. We believe its excellent performance and compact size make it an attractive platform for\n",
            "researchers and practitioners in language model research.\n",
            "2 Pretraining\n",
            "This section describes how we pre-trained TinyLlama. First, we introduce the details of the pre-\n",
            "training corpus and the data sampling method. Next, we elaborate on the model architecture and the\n",
            "hyperparameters used during pretraining.\n",
            "2.1 Pre-training data\n",
            "Our main objective is to make the pre-training process effective and reproducible. We adopt a mixture\n",
            "of natural language data and code data to pre-train TinyLlama, sourcing natural language data from\n",
            "SlimPajama (Soboleva et al., 2023) and code data from Starcoderdata (Li et al., 2023). We adopt\n",
            "Llama’s tokenizer (Touvron et al., 2023a) to process the data.\n",
            "SlimPajama This is a large open-source corpus created for training language models based on\n",
            "RedPajama (Together Computer, 2023). The original RedPajama corpus is an open-source research\n",
            "effort aimed at reproducing Llama’s pretraining data (Touvron et al., 2023a) containing over 1.2\n",
            "trillion tokens. The SlimPajama was derived by cleaning and deduplicating the original RedPajama.\n",
            "Starcoderdata This dataset was collected to train StarCoder (Li et al., 2023), a powerful open-\n",
            "source large code language model. It comprises approximately 250 billion tokens across 86 program-\n",
            "ming languages. In addition to code, it also includes GitHub issues and text-code pairs that involve\n",
            "natural languages. To avoid data duplication, we remove the GitHub subset of the SlimPajama and\n",
            "only sample code data from the Starcoderdata.\n",
            "After combining these two corpora, we have approximately 950 billion tokens for pre-training in total.\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 254}\n",
            "only sample code data from the Starcoderdata.\n",
            "After combining these two corpora, we have approximately 950 billion tokens for pre-training in total.\n",
            "TinyLlama is trained on these tokens for approximately three epochs, as observed by Muennighoff\n",
            "et al. (2023), where training on data repeated for up to four epochs results in minimal performance\n",
            "degradation compared to using unique data. During training, we sample the natural language data to\n",
            "achieve a ratio of around 7:3 between natural language data and code data.\n",
            "2.2 Architecture\n",
            "We adopt a similar model architecture to Llama 2 (Touvron et al., 2023b). We use a Transformer\n",
            "architecture based on Vaswani et al. (2017) with the following details:\n",
            "2Table 1: The details of model architecture\n",
            "Hidden size Intermediate Hidden Size Context Len Heads Layers Vocab size\n",
            "2,048 5,632 2,048 16 22 32,000\n",
            "Positional embedding We use RoPE (Rotary Positional Embedding) (Su et al., 2021) to inject\n",
            "positional information into our model. RoPE is a widely adopted method recently used by many\n",
            "mainstream large language models, such as PaLM (Anil et al., 2023), Llama (Touvron et al., 2023a),\n",
            "and Qwen (Bai et al., 2023).\n",
            "RMSNorm In pre-normalization, to attain a more stable training, we normalize the input before\n",
            "each transformer sub-layer. In addition, we apply RMSNorm (Zhang and Sennrich, 2019) as our\n",
            "normalization technique, which can improve training efficiency.\n",
            "SwiGLU Instead of using the traditional ReLU non-linearity, we follow Llama 2 and combine\n",
            "Swish and Gated Linear Unit together, which is referred to as SwiGLU (Shazeer, 2020), as our\n",
            "activation function in TinyLlama.\n",
            "Grouped-query Attention To reduce memory bandwidth overhead and speed up inference, we use\n",
            "grouped-query attention (Ainslie et al., 2023) in our model. We have 32 heads for query attention\n",
            "and use 4 groups of key-value heads. With this technique, the model can share key and value\n",
            "representations across multiple heads without sacrificing much performance.\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 255}\n",
            "and use 4 groups of key-value heads. With this technique, the model can share key and value\n",
            "representations across multiple heads without sacrificing much performance.\n",
            "2.3 Speed Optimizations\n",
            "Fully Sharded Data Parallel (FSDP) During training, our codebase has integrated FSDP1to\n",
            "leverage multi-GPU and multi-node setups efficiently. This integration is crucial in scaling the\n",
            "training process across multiple computing nodes, which significantly improves the training speed\n",
            "and efficiency.\n",
            "Flash Attention Another critical improvement is the integration of Flash Attention 2 (Dao, 2023),\n",
            "an optimized attention mechanism. The repository also provides fused layernorm, fused cross\n",
            "entropy loss, and fused rotary positional embedding, which together play a pivotal role in boosting\n",
            "computational throughput.\n",
            "xFormers We have replaced the fused SwiGLU module from the xFormers (Lefaudeux et al., 2022)\n",
            "repository with the original SwiGLU module, further enhancing the efficiency of our codebase. With\n",
            "these features, we can reduce the memory footprint, enabling the 1.1B model to fit within 40GB of\n",
            "GPU RAM.\n",
            "Performance Analysis and Comparison with Other Models The incorporation of these elements\n",
            "has propelled our training throughput to 24,000 tokens per second per A100-40G GPU. When\n",
            "compared with other models like Pythia-1.0B (Biderman et al., 2023) and MPT-1.3B2, our codebase\n",
            "demonstrates superior training speed. For instance, the TinyLlama-1.1B model requires only 3,456\n",
            "A100 GPU hours for 300B tokens, in contrast to Pythia’s 4,830 and MPT’s 7,920 hours. This shows\n",
            "the effectiveness of our optimizations and the potential for substantial time and resource savings in\n",
            "large-scale model training.\n",
            "2.4 Training\n",
            "We build our framework based on lit-gpt.3In adhering to Llama 2 (Touvron et al., 2023b), we employ\n",
            "an autoregressive language modeling objective during the pretraining phase. Consistent with Llama\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 256}\n",
            "We build our framework based on lit-gpt.3In adhering to Llama 2 (Touvron et al., 2023b), we employ\n",
            "an autoregressive language modeling objective during the pretraining phase. Consistent with Llama\n",
            "2’s settings, we utilize the AdamW optimizer (Loshchilov and Hutter, 2019), setting β1at 0.9 and\n",
            "1https://huggingface.co/docs/accelerate/usage_guides/fsdp\n",
            "2https://huggingface.co/mosaicml/mpt-1b-redpajama-200b\n",
            "3https://github.com/Lightning-AI/lit-gpt\n",
            "3Figure 1: Comparison of the training speed of our codebase with Pythia and MPT.\n",
            "β2at 0.95. Additionally, we use a cosine learning rate schedule with maximum learning rate as\n",
            "4.0×10−4and minimum learning rate as 4.0×10−5. We use 2,000 warmup steps to facilitate\n",
            "optimized learning.4We set the batch size as 2M tokens. We assign weight decay as 0.1, and use\n",
            "a gradient clipping threshold of 1.0 to regulate the gradient value. We pretrain TinyLlama with 16\n",
            "A100-40G GPUs in our project.\n",
            "3 Results\n",
            "We evaluate TinyLlama on a wide range of commonsense reasoning and problem-solving tasks and\n",
            "compare it with several existing open-source language models with similar model parameters.\n",
            "Baseline models We primarily focus on language models with a decoder-only architecture, compris-\n",
            "ing approximately 1 billion parameters. Specifically, we compare TinyLlama with OPT-1.3B (Zhang\n",
            "et al., 2022), Pythia-1.0B, and Pythia-1.4B (Biderman et al., 2023).\n",
            "Commonsense reasoning tasks To understand the commonsense reasoning ability of TinyLlama,\n",
            "we consider the following tasks: Hellaswag (Zellers et al., 2019), OpenBookQA (Mihaylov et al.,\n",
            "2018), WinoGrande (Sakaguchi et al., 2021), ARC-Easy and ARC-Challenge (Clark et al., 2018),\n",
            "BoolQ (Clark et al., 2019), and PIQA (Bisk et al., 2020). We adopt the Language Model Evaluation\n",
            "Harness framework (Gao et al., 2023) to evaluate the models. Following previous practice (Biderman\n",
            "et al., 2023), the models are evaluated in a zero-shot setting on these tasks. The results are presented\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 257}\n",
            "et al., 2023), the models are evaluated in a zero-shot setting on these tasks. The results are presented\n",
            "in Table 2. We notice that TinyLlama outperforms baselines on many of the tasks and obtains the\n",
            "highest averaged scores.\n",
            "Table 2: Zero-shot performance on commonsense reasoning tasks.\n",
            "HellaSwag Obqa WinoGrande ARC-c ARC-e boolq piqa Avg\n",
            "OPT-1.3B 53.65 33.40 59.59 29.44 50.80 60.83 72.36 51.44\n",
            "Pythia-1.0B 47.16 31.40 53.43 27.05 48.99 57.83 69.21 48.30\n",
            "Pythia-1.4B 52.01 33.20 57.38 28.50 54.00 63.27 70.95 51.33\n",
            "TinyLlama-1.1B 59.20 36.00 59.12 30.10 55.25 57.83 73.29 52.99\n",
            "Evolution of performance during training We tracked the accuracy of TinyLlama on common-\n",
            "sense reasoning benchmarks during its pre-training, as shown in Fig. 2. Generally, the performance of\n",
            "4Due to a bug in the config file, the learning rate did not decrease immediately after warmup and remained at\n",
            "the maximum value for several steps before we fixed this.\n",
            "4TinyLlama improves with increased computational resources, surpassing the accuracy of Pythia-1.4B\n",
            "in most benchmarks.5\n",
            "102103104\n",
            "GPU-Hours2426283032Accuracy (%)arc_challenge\n",
            "TinyLlama\n",
            "Pythia-1.4B\n",
            "102103104\n",
            "GPU-Hours40455055Accuracy (%)arc_easy\n",
            "TinyLlama\n",
            "Pythia-1.4B\n",
            "102103104\n",
            "GPU-Hours505560Accuracy (%)boolq\n",
            "TinyLlama\n",
            "Pythia-1.4B\n",
            "102103104\n",
            "GPU-Hours354045505560Accuracy (%)hellaswag\n",
            "TinyLlama\n",
            "Pythia-1.4B\n",
            "102103104\n",
            "GPU-Hours30323436Accuracy (%)openbookqa\n",
            "TinyLlama\n",
            "Pythia-1.4B\n",
            "102103104\n",
            "GPU-Hours62.565.067.570.072.5Accuracy (%)piqa\n",
            "TinyLlama\n",
            "Pythia-1.4B\n",
            "102103104\n",
            "GPU-Hours50.052.555.057.560.0Accuracy (%)winogrande\n",
            "TinyLlama\n",
            "Pythia-1.4B\n",
            "102103104\n",
            "GPU-Hours42.545.047.550.052.5Accuracy (%)Average\n",
            "TinyLlama\n",
            "Pythia-1.4B\n",
            "Figure 2: Evolution of performance in commonsense reasoning benchmarks during pre-training. The perfor-\n",
            "mance of Pythia-1.4B is also included in the figure for comparison.\n",
            "Problem-solving evaluation We also evaluate TinyLlama’s problem-solving capabilities using the\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 258}\n",
            "mance of Pythia-1.4B is also included in the figure for comparison.\n",
            "Problem-solving evaluation We also evaluate TinyLlama’s problem-solving capabilities using the\n",
            "InstructEval benchmark (Chia et al., 2023). This benchmark includes the following tasks:\n",
            "•Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021): This task is\n",
            "used to measure a model’s world knowledge and problem-solving capabilities across various\n",
            "subjects. We evaluate the models in a 5-shot setting.\n",
            "•BIG-Bench Hard (BBH) (Suzgun et al., 2023): This is a subset of 23 challenging tasks from\n",
            "the BIG-Bench benchmark (Srivastava et al., 2022) designed to measure a language model’s\n",
            "abilities in complex instruction following. The models are evaluated in a 3-shot setting.\n",
            "•Discrete Reasoning Over Paragraphs (DROP) (Dua et al., 2019): This reading comprehen-\n",
            "sion task measures a model’s math reasoning abilities. We evaluate the models in a 3-shot\n",
            "setting.\n",
            "•HumanEval (Zheng et al., 2023): This task is used to measure a model’s programming\n",
            "capabilities. The models are evaluated in a zero-shot setting.\n",
            "The evaluation results are presented in Table 3. We observe that TinyLlama demonstrates better\n",
            "problem-solving skills compared to existing models.\n",
            "Table 3: Performance of problem-solving tasks on the InstructEval Benchmark.\n",
            "MMLU BBH HumanEval DROP Avg.\n",
            "5-shot 3-shot 0-shot 3-shot\n",
            "Pythia-1.0B 25.70 28.19 01.83 04.25 14.99\n",
            "Pythia-1.4B 25.41 29.01 04.27 12.27 17.72\n",
            "TinyLlama-1.1B 25.34 29.65 09.15 15.34 19.87\n",
            "5In our initial dataset preprocessing, we inadvertently over-inserted end-of-sequence (EOS) tokens. This\n",
            "excess of EOS tokens may have negatively affected the model by introducing substantial less meaningful signals\n",
            "into the training data. However, after approximately 2.3T tokens, we removed these repetitive EOS tokens and\n",
            "continued pre-training TinyLlama with our refined data. This rectification likely contributed significantly to\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 259}\n",
            "continued pre-training TinyLlama with our refined data. This rectification likely contributed significantly to\n",
            "the observed sudden improvements in performance on benchmarks such as hellasag, piqa, arc_challenge, and\n",
            "arc_easy during that period.\n",
            "54 Conclusion\n",
            "In this paper, we introduce TinyLlama, an open-source, small-scale language model. To promote\n",
            "transparency in the open-source LLM pre-training community, we have released all relevant infor-\n",
            "mation, including our pre-training code, all intermediate model checkpoints, and the details of our\n",
            "data processing steps. With its compact architecture and promising performance, TinyLlama can\n",
            "enable end-user applications on mobile devices, and serve as a lightweight platform for testing a\n",
            "wide range of innovative ideas related to language models. We will leverage the rich experience\n",
            "accumulated during the open, live phase of this project and aim to develop improved versions of\n",
            "TinyLlama, equipping it with a diverse array of capabilities to enhance its performance and versatility\n",
            "across various tasks. We will document further findings and detailed results in upcoming reports.\n",
            "Acknowledgements\n",
            "We express our gratitude to the open-source community for their strong support during the open,\n",
            "live phase of our research. Special thanks go to Qian Liu, Longxu Dou, Hai Leong Chieu, and\n",
            "Larry Law for their help to our project. This research/project is supported by Ministry of Education,\n",
            "Singapore, under its Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award\n",
            "No.: MOE-T2EP20122-0011), Ministry of Education, Singapore, under its Tier 3 Programme (The\n",
            "Award No.: MOET320200004), the National Research Foundation Singapore and DSO National\n",
            "Laboratories under the AI Singapore Program (AISG Award No: AISG2-RP-2020-016), an AI\n",
            "Singapore PhD Scholarship (AISG Award No: AISG2-PhD-2021-08-007), an SUTD Kick-Starter\n",
            "Project (SKI 2021_03_11), and the grant RS-INSUR-00027-E0901-S00. Any opinions, findings and\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 260}\n",
            "Singapore PhD Scholarship (AISG Award No: AISG2-PhD-2021-08-007), an SUTD Kick-Starter\n",
            "Project (SKI 2021_03_11), and the grant RS-INSUR-00027-E0901-S00. Any opinions, findings and\n",
            "conclusions or recommendations expressed in this material are those of the authors and do not reflect\n",
            "the views of the funding agencies.\n",
            "{'filename': '2401.02385_v1.pdf', 'title': 'TinyLlama: An Open-Source Small Language Model', 'vs_index': 261}\n"
          ]
        }
      ],
      "source": [
        "vs_indices = [doc.metadata['vs_index'] for doc in docs_processed]\n",
        "for page in docs_processed:\n",
        "  print(page.page_content)\n",
        "  print(page.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO: Once have the pipeline working and have a baseline, look into https://huggingface.co/spaces/mteb/leaderboard.  SFR-Embedding-Mistral or something along those lines may work much better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.save_local('../data/rag_index_dir')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjrNhcCh9jVN"
      },
      "source": [
        "### 1.2. Setup agents for question generation\n",
        "\n",
        "- HF used [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard).  Used Mixtral-8x7b-4bit exl2, and it did not appear significantly better than Mistral, so using Mistral for speed but may come back to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO: Tried Mixtral4Bit, visually perhaps a bit better, but overfits on 'deep question'. Perhaps return to this after looking at embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        }
      ],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "generator_config = ExLlamaV2Config()\n",
        "generator_config.model_dir = \"../../MiStralInference\"\n",
        "generator_config.prepare()\n",
        "\n",
        "generator_model = ExLlamaV2(generator_config)\n",
        "cache = ExLlamaV2Cache(generator_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "generator_model.load_autosplit(cache)\n",
        "\n",
        "generator_tokenizer = ExLlamaV2Tokenizer(generator_config)\n",
        "generator_llm = ExLlamaV2StreamingGenerator(generator_model, cache, generator_tokenizer)\n",
        "generator_llm.set_stop_conditions([generator_tokenizer.eos_token_id])\n",
        "generator_settings = ExLlamaV2Sampler.Settings()\n",
        "generator_settings.temperature = 0.85\n",
        "generator_settings.top_k = 50\n",
        "generator_settings.top_p = 0.8\n",
        "generator_settings.token_repetition_penalty = 1.01\n",
        "#generator_settings.disallow_tokens(generator_tokenizer, [generator_tokenizer.eos_token_id])\n",
        "# see if commenting out the above solved the endless generation issue (did not have with stream generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<s>[INST] How can I get my cat to like me? [/INST] 1. Spend time with your cat: Cats enjoy spending time with their owners, so take some time to play with them, cuddle with them, and give them attention.\\n2. Provide food and shelter: Make sure your cat has a comfortable place to sleep, eat, and drink water.\\n3. Use positive reinforcement: Reward your cat with treats, praise, and affection when they behave well around you.\\n4. Be patient: Cats can take time to warm up to new people, so be patient and give them time to get used to you.\\n5. Show respect: Cats are independent creatures, so it's important to show respect for their boundaries and allow them to make their own choices.\\n6. Keep your environment clean: Cats are clean animals, so make sure their litter box is clean and accessible at all times.\\n7. Provide toys and scratching posts: Cats love to play and scratch, so provide them with toys and scratching posts to keep them entertained.\\n8. Be consistent: Consistency is key when it comes to building a relationship with your cat. Stick to a routine and be consistent in your actions and behavior.\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Working except eos\n",
        "from transformers import Pipeline\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "import time\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def call_llm(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "    tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "    max_new_tokens = max_new_tokens\n",
        "\n",
        "    generator.warmup()\n",
        "    output = generator.generate_simple(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens, seed = 1234)\n",
        "    return output\n",
        "\n",
        "\n",
        "call_llm(question=\"How can I get my cat to like me?\", generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,max_new_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
        "Your deep question should be unambigiously answerable from the context.\n",
        "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
        "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Deep question: (your deep question)\n",
        "Answer: (your answer to the deep question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFc-lVy9jVN"
      },
      "source": [
        "Now let's generate our QA couples.\n",
        "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
        "\n",
        "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8fteqDDD9jVN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 5 QA couples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:10<00:00,  2.06s/it]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "N_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    # Generate QA couple\n",
        "    # output_QA_couple = call_llm(\n",
        "    #     llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    # )\n",
        "    output_QA_couple = call_llm(question=QA_generation_prompt.format(context=sampled_context.page_content), generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    try:\n",
        "        question = output_QA_couple.split(\"Deep question: \")[-1].split(\"Answer: \")[0]\n",
        "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
        "        #assert len(answer) < 300, \"Answer is too long\"\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"context\": sampled_context.page_content,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"source_doc\": sampled_context.metadata[\"title\"],\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'context': 'reasoning or code. Full reasoning results are included in Tab. 11.\\nEvaluating across Safety Metrics Tab. 6 (full results in Tab. 10 in Appendix) compares dif-\\nferent reward models across different safety categories, indicating challenges on striking a bal-\\nance between refusing too much or not refusing. Models, such as zephyr-7b-beta and\\nzephyr-7b-gemma-v0.1 show how a model focused on helpfulness without a strong notion of\\nsafety will score poorly on the should-refuse subsets of the safety section, but highly on XSTest\\nShould Respond . Other models, namely those at the top of the overall leaderboard, clearly in-\\nclude safety information in the training process andmaintain strong performance on trick questions\\nthat could induce false refusals ( XSTest Should Respond ). Finally, the third option is also repre-\\nsented in models – those that score highly on prompts that they should refuse and poorly on those\\nthey should not, indicating a model that is likely to falsely refusal queries (for example, the Qwen\\nchat models). These three behavior modes being represented indicates that R EWARD BENCH can\\nbe used as a quick check of the safety behavior of a candidate model, especially when trained with\\nDPO (as it will not need further RL training like the classifier models).\\nEvaluating Length Bias Given the results showing length bias in RLHF and reward models (Sing-\\nhal et al., 2023), we designed R EWARD BENCH so that the chosen responses are either a similar\\nlength or shorter than the rejected responses. For example, the AlpacaEval Length subset is de-\\nsigned to differentiate between other Chat subsets by having notably different models capabilities\\nwith the same average length (results in Tab. 8). In this case, the results are lower than other easy\\n10chat subsets, but 90% plus accuracy is achieved by over 10 models – far above random for most\\nmodels. Though, more detailed statistical tests are needed to fully understand this, as this only tests',\n",
              "  'question': \"Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n\",\n",
              "  'answer': \"The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.\",\n",
              "  'source_doc': 'RewardBench: Evaluating Reward Models for Language Modeling'},\n",
              " {'context': 'derman et al., 2023) and Llama- {7B, 13B, 30B }(Touvron\\net al., 2023). This permits us to see how LLM alignment\\nscales within a model family (Llama-2 lacks a 30B model,\\nhence our use of Llama). Later experiments ( §4.2) are done\\non Mistral-7B and its derivatives (Jiang et al., 2023). The\\nmodels were trained on a combination of Anthropic HH\\n(Ganguli et al., 2022), OpenAssistant (K ¨opf et al., 2023),\\nand SHP (Ethayarajh et al., 2022).\\nAll models were aligned under identical settings on the\\nsame data (e.g., same effective batch size, same optimizer,\\netc.), save for hyperparameters unique to them. Similar to\\nRafailov et al. (2023), the target sequences for SFT are a\\nsubset of the generations used to subsequently align the\\nmodel; however, for a more realistic SFT setup, we do not\\nnecessarily set the most preferred generation to be the target\\n(with the exception of HH, since the dispreferred output in\\nthat dataset is often harmful). Then we used GPT-4-0613\\n4Model Alignment as Prospect Theoretic Optimization\\nFigure 3. Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC\\nand CSFT), though the gap is only significant (p < 0.05)at 13B+ model sizes. In fact, only the HALO-aligned Llama- {13B, 30B }\\nmodels are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset.\\nIt is also worth noting that up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage.\\nto judge whether the aligned model’s response was bet-\\nter than the SFT target for the given input with respect to\\nhelpfulness, harmlessness, and conciseness, a now standard\\npractice (Zheng et al., 2023; Li et al., 2023).3Note that\\nwhile the SFT target is considered a desirable output for x,\\nit is by no means the best output, meaning that it can be\\nimproved upon by an aligned model.\\nIn Figure 3, we see the results of this analysis:',\n",
              "  'question': 'Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n',\n",
              "  'answer': \"Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p < 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it can be improved upon by an aligned model.\",\n",
              "  'source_doc': 'KTO: Model Alignment as Prospect Theoretic Optimization'}]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aUlOUDv59jVN",
        "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>reasoning or code. Full reasoning results are included in Tab. 11.\\nEvaluating across Safety Metrics Tab. 6 (full results in Tab. 10 in Appendix) compares dif-\\nferent reward models across different safety categories, indicating challenges on striking a bal-\\nance between refusing too much or not refusing. Models, such as zephyr-7b-beta and\\nzephyr-7b-gemma-v0.1 show how a model focused on helpfulness without a strong notion of\\nsafety will score poorly on the should-refuse subsets of the safety section, but highly on XSTest\\nShould Respond . Other models, namely those at the top of the overall leaderboard, clearly in-\\nclude safety information in the training process andmaintain strong performance on trick questions\\nthat could induce false refusals ( XSTest Should Respond ). Finally,...</td>\n",
              "      <td>Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n</td>\n",
              "      <td>The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.</td>\n",
              "      <td>RewardBench: Evaluating Reward Models for Language Modeling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>derman et al., 2023) and Llama- {7B, 13B, 30B }(Touvron\\net al., 2023). This permits us to see how LLM alignment\\nscales within a model family (Llama-2 lacks a 30B model,\\nhence our use of Llama). Later experiments ( §4.2) are done\\non Mistral-7B and its derivatives (Jiang et al., 2023). The\\nmodels were trained on a combination of Anthropic HH\\n(Ganguli et al., 2022), OpenAssistant (K ¨opf et al., 2023),\\nand SHP (Ethayarajh et al., 2022).\\nAll models were aligned under identical settings on the\\nsame data (e.g., same effective batch size, same optimizer,\\netc.), save for hyperparameters unique to them. Similar to\\nRafailov et al. (2023), the target sequences for SFT are a\\nsubset of the generations used to subsequently align the\\nmodel; however, for a more realistic SFT setup, we do ...</td>\n",
              "      <td>Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n</td>\n",
              "      <td>Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p &lt; 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it c...</td>\n",
              "      <td>KTO: Model Alignment as Prospect Theoretic Optimization</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           context  \\\n",
              "0  reasoning or code. Full reasoning results are included in Tab. 11.\\nEvaluating across Safety Metrics Tab. 6 (full results in Tab. 10 in Appendix) compares dif-\\nferent reward models across different safety categories, indicating challenges on striking a bal-\\nance between refusing too much or not refusing. Models, such as zephyr-7b-beta and\\nzephyr-7b-gemma-v0.1 show how a model focused on helpfulness without a strong notion of\\nsafety will score poorly on the should-refuse subsets of the safety section, but highly on XSTest\\nShould Respond . Other models, namely those at the top of the overall leaderboard, clearly in-\\nclude safety information in the training process andmaintain strong performance on trick questions\\nthat could induce false refusals ( XSTest Should Respond ). Finally,...   \n",
              "1  derman et al., 2023) and Llama- {7B, 13B, 30B }(Touvron\\net al., 2023). This permits us to see how LLM alignment\\nscales within a model family (Llama-2 lacks a 30B model,\\nhence our use of Llama). Later experiments ( §4.2) are done\\non Mistral-7B and its derivatives (Jiang et al., 2023). The\\nmodels were trained on a combination of Anthropic HH\\n(Ganguli et al., 2022), OpenAssistant (K ¨opf et al., 2023),\\nand SHP (Ethayarajh et al., 2022).\\nAll models were aligned under identical settings on the\\nsame data (e.g., same effective batch size, same optimizer,\\netc.), save for hyperparameters unique to them. Similar to\\nRafailov et al. (2023), the target sequences for SFT are a\\nsubset of the generations used to subsequently align the\\nmodel; however, for a more realistic SFT setup, we do ...   \n",
              "\n",
              "                                                                                                                                                                                                                                          question  \\\n",
              "0                                                                      Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n   \n",
              "1  Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            answer  \\\n",
              "0                                                                                                                                                                 The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.   \n",
              "1  Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p < 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it c...   \n",
              "\n",
              "                                                    source_doc  \n",
              "0  RewardBench: Evaluating Reward Models for Language Modeling  \n",
              "1      KTO: Model Alignment as Prospect Theoretic Optimization  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',800)\n",
        "display(pd.DataFrame(outputs).head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KG4dNtg9jVN"
      },
      "source": [
        "### 1.3. Setup critique agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating critique for each QA couple...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:21<00:00,  4.38s/it]\n"
          ]
        }
      ],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(question=question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "        \"relevance\": call_llm(question=question_relevance_critique_prompt.format(question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "                    \n",
        "        \"standalone\": call_llm(question=question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    }\n",
        "    try:\n",
        "        for criterion, evaluation in evaluations.items():\n",
        "            score, eval = (\n",
        "                # int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                (evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "            )\n",
        "            output.update(\n",
        "                {\n",
        "                    f\"{criterion}_score\": score,\n",
        "                    f\"{criterion}_eval\": eval,\n",
        "                }\n",
        "            )\n",
        "    except Exception as e:\n",
        "        #print(\"\\033[91m\" + f\"EVALUATION:\" + \"\\033[0m\")\n",
        "        #print(evaluations)\n",
        "        #print(\"\\033[91m\" + f\"EXCEPTION: {e}\" + \"\\033[0m\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n\\n\\nContext: Mistral (7B) with single-turn conversation dataset,\\nUltraFeedback, and ORPO withλof 0.1 outperforms\\nZephyr series, which are the Mistral (7B) models\\nfine-tuned with SFT on 20K UltraChat (Ding et al.,\\n2023) and DPO on the full UltraFeedback. As\\nshown in Table 1, Mistral- ORPO -α(7B) achieves\\n87.92% and 11.33%, which exceeds Zephyr αby\\n1.98% and Zephyr βby 0.34% in AlpacaEval2.0.\\nThe sample responses and corresponding refer-\\nences from GPT-4 can be found in Appendix I.\\nMistral- ORPO -β(7B) Using the same configura-\\ntion of Mistral- ORPO -α(7B), we additionally com-\\npare fine-tuning Mistral on the cleaned version of\\nthe UltraFeedback8to demonstrate the effect of the\\ndata quality (Bartolome et al., 2023). While the\\nactual sizes of datasets are similar, ORPO gains fur-\\nther advantages from the dataset quality by scoring\\nover 91% and 12% on AlpacaEval, as shown in\\nTable 1. Further instruction-following evaluation\\non two Mistral-based models with IFEval (Zhou\\net al., 2023c) is reported in the Appendix D.\\n8https://huggingface.co/datasets/argilla/\\nultrafeedback-binarized-preferences-cleaned\\nWriting\\nRoleplay\\nReasoning\\nMath\\nCodingExtractionSTEMHumanities\\n0246810Model\\nLlama-2-Chat (70B)\\nGPT-3.5-turbo\\nClaude V1\\nGPT-4\\nMistral- ORPO -⍺ (7B): 7.23\\nMistral- ORPO -β (7B): 7.32\\nLoading [MathJax]/extensions/MathMenu.jsFigure 4: MT-Bench result of Mistral- ORPO -α(7B) and\\nMistral- ORPO -β(7B) by the category. Further compari-\\nson can be found in the Appendix G.\\n6.2 Multi-turn Instruction Following\\nWith our best model, Mistral- ORPO -α(7B) and\\nMistral- ORPO -β(7B), we also assess the multi-turn\\ninstruction-following skills with deterministic an-\\nswers (e.g., math) through MT-Bench.\\nAs shown in Figure 4, ORPO -Mistral (7B) series\\nachieve comparable results to either larger or the\\nproprietary models, including Llama-2-Chat (70B)\\nand Claude. Eventually, Mistral- ORPO -α(7B) and\\nMistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-\\nBench without being exposed to the multi-turn con-\\n\\nAnswer:::  [/INST] Evaluation: In order to evaluate the performance of the given models in a multi-turn instruction-following task, we need to compare their scores on a benchmark metric such as AlpacaEval2.0 or MT-Bench. We also need to consider the size and quality of the datasets used to train the models. Based on the provided context, we can see that Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) outperform the Zephyr series, which are fine-tuned on UltraChat and UltraFeedback datasets. Additionally, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) achieve comparable results to larger or proprietary models such as Llama-2-Chat (70B) and Claude V1 on the MT-Bench metric. Therefore, we can rate the performance of the given models as good.\\n\\nTotal rating: 4\",\n",
              " 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation:\\nThe question provides a comparison of the performance of two models, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B), in a multi-turn instruction-following task, with larger or proprietary models such as Llama-2-Chat (70B) and Claude V1. This comparison can be useful for machine learning developers building NLP applications with the Hugging Face ecosystem, as it allows them to evaluate the relative performance of different models in a specific task.\\n\\nTotal rating: 4\\n\\nThe question is useful because it provides a comparison of the performance of two models in a specific task, which can help developers make informed decisions about which model to use for their application. However, it would be more useful if it also included a comparison of the computational resources required by each model, as this can also be an important factor when choosing a model for a particular application.\",\n",
              " 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independant this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation:\\n\\nThe question is about the performance of two specific models, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B), in a multi-turn instruction-following task and how they compare to the performance of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1. It does not require any additional information or context to be understood.\\n\\nTotal rating: 5'}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQv36Y_f9jVO"
      },
      "source": [
        "Now let us filter out bad questions based on our critique agent scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['context', 'question', 'answer', 'source_doc', 'groundedness_score', 'groundedness_eval', 'relevance_score', 'relevance_eval', 'standalone_score', 'standalone_eval'])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation dataset before filtering:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n</td>\n",
              "      <td>The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.</td>\n",
              "      <td>4.5\\n\\nThe context provides a clear explanation of how R EWARD BENCH evaluates models' safety behavior and its significance in identifying potential false refusal queries. The evaluation considers multiple safety categories and provides examples of how different models perform on these categories. Additionally, the context explains how R EWARD BENCH is designed to avoid length bias, which is a common issue in RLHF and reward models. Overall, the context provides a comprehensive understanding of how R EWARD BENCH evaluates models' safety behavior and its significance in identifying potential false refusal queries.</td>\n",
              "      <td>4.5\\n\\nThe R EWARD BENCH is a very useful tool for machine learning developers building NLP applications with the Hugging Face ecosystem. It provides a comprehensive evaluation of models' safety behavior and can help identify potential false refusal queries, which are an important issue in many NLP applications. The combination of rule-based and machine learning-based approaches used by the R EWARD BENCH also makes it a very versatile tool that can be used with a wide range of NLP models. Overall, the R EWARD BENCH is an extremely useful tool for machine learning developers building NLP applications with the Hugging Face ecosystem.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n</td>\n",
              "      <td>Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p &lt; 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it can be improved upon by an aligned model.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What specific changes is Toyota implementing to its EV business environment and how does it differ from other companies that have recently embraced a hybrid fleet?\\n\\n</td>\n",
              "      <td>Toyota is implementing changes to its executives' areas of responsibility that require disclosure of further changes to its policies and practices as the larger EV business evolves. The company has not had a hybrid supply chain for years and hybrid research is not allowed inside its laboratories. These changes will only come with the launch of a new integrated EV product division, which has not been finalized yet. Additionally, Toyota's \"Reuturistic and Singular Communication Systems\" are the industry's most ambitious \"big guy\" partnerships, and the company is opening the first car dealerships in the United States as the name suggests. There is no plan to extend out its RAV4 EV program beyond the University of Arizona system in Reno. Consumers are more willing to hop on the electric car bandwagon than at any time in history, and the shift to EV mandates has become what almost everyone has been dying to do for decades.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4. This question is moderately useful for machine learning developers building NLP applications with the Hugging Face ecosystem, as it requires them to analyze and compare specific changes implemented by Toyota in its EV business environment with those of other companies that have recently embraced a hybrid fleet. However, the question could be more useful if it provided more specific details or criteria for evaluating the changes implemented by each company, or if it focused on a specific aspect of EV adoption, such as charging infrastructure or government policies.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the estimated local density of an example using the kernel sum approach and the Densi-LLM method?\\n\\n</td>\n",
              "      <td>The estimated local density of an example using the kernel sum approach and the Densi-LLM method is calculated by taking the kernel sum of the scores of all examples in the dataset. The kernel is a function that determines the similarity between two examples based on their embeddings. The smoothing parameter λ controls the scale of the points' effects and helps reduce the complexity from O(N^2) to O(NlogN). The method resembles that of Coleman et al. (2022), except that it uses a two-pass sampling algorithm with stronger theoretical guarantees and performs the density estimation in the latent space of the model.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n</td>\n",
              "      <td>In the multi-turn instruction-following task, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) achieved comparable results to larger or proprietary models such as Llama-2-Chat (70B) and Claude V1. Specifically, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-Bench without being exposed to the multi-turn conversation dataset.</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                          question  \\\n",
              "0                                                                      Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n   \n",
              "1  Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n   \n",
              "2                                                                          What specific changes is Toyota implementing to its EV business environment and how does it differ from other companies that have recently embraced a hybrid fleet?\\n\\n   \n",
              "3                                                                                                                                    What is the estimated local density of an example using the kernel sum approach and the Densi-LLM method?\\n\\n   \n",
              "4                              In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                     The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.   \n",
              "1                                                                                                 Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p < 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it can be improved upon by an aligned model.   \n",
              "2  Toyota is implementing changes to its executives' areas of responsibility that require disclosure of further changes to its policies and practices as the larger EV business evolves. The company has not had a hybrid supply chain for years and hybrid research is not allowed inside its laboratories. These changes will only come with the launch of a new integrated EV product division, which has not been finalized yet. Additionally, Toyota's \"Reuturistic and Singular Communication Systems\" are the industry's most ambitious \"big guy\" partnerships, and the company is opening the first car dealerships in the United States as the name suggests. There is no plan to extend out its RAV4 EV program beyond the University of Arizona system in Reno. Consumers are more willing to hop on the electric car bandwagon than at any time in history, and the shift to EV mandates has become what almost everyone has been dying to do for decades.   \n",
              "3                                                                                                                                                                                                                                                                                                                          The estimated local density of an example using the kernel sum approach and the Densi-LLM method is calculated by taking the kernel sum of the scores of all examples in the dataset. The kernel is a function that determines the similarity between two examples based on their embeddings. The smoothing parameter λ controls the scale of the points' effects and helps reduce the complexity from O(N^2) to O(NlogN). The method resembles that of Coleman et al. (2022), except that it uses a two-pass sampling algorithm with stronger theoretical guarantees and performs the density estimation in the latent space of the model.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In the multi-turn instruction-following task, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) achieved comparable results to larger or proprietary models such as Llama-2-Chat (70B) and Claude V1. Specifically, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-Bench without being exposed to the multi-turn conversation dataset.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             groundedness_score  \\\n",
              "0  4.5\\n\\nThe context provides a clear explanation of how R EWARD BENCH evaluates models' safety behavior and its significance in identifying potential false refusal queries. The evaluation considers multiple safety categories and provides examples of how different models perform on these categories. Additionally, the context explains how R EWARD BENCH is designed to avoid length bias, which is a common issue in RLHF and reward models. Overall, the context provides a comprehensive understanding of how R EWARD BENCH evaluates models' safety behavior and its significance in identifying potential false refusal queries.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           4.5   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   relevance_score  \\\n",
              "0  4.5\\n\\nThe R EWARD BENCH is a very useful tool for machine learning developers building NLP applications with the Hugging Face ecosystem. It provides a comprehensive evaluation of models' safety behavior and can help identify potential false refusal queries, which are an important issue in many NLP applications. The combination of rule-based and machine learning-based approaches used by the R EWARD BENCH also makes it a very versatile tool that can be used with a wide range of NLP models. Overall, the R EWARD BENCH is an extremely useful tool for machine learning developers building NLP applications with the Hugging Face ecosystem.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN   \n",
              "2                                                                    4. This question is moderately useful for machine learning developers building NLP applications with the Hugging Face ecosystem, as it requires them to analyze and compare specific changes implemented by Toyota in its EV business environment with those of other companies that have recently embraced a hybrid fleet. However, the question could be more useful if it provided more specific details or criteria for evaluating the changes implemented by each company, or if it focused on a specific aspect of EV adoption, such as charging infrastructure or government policies.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN   \n",
              "\n",
              "  standalone_score  \n",
              "0                5  \n",
              "1              NaN  \n",
              "2              NaN  \n",
              "3              NaN  \n",
              "4              NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "generated_questions['groundedness_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['relevance_score']=generated_questions['relevance_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['standalone_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>groundedness_eval</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>relevance_eval</th>\n",
              "      <th>standalone_score</th>\n",
              "      <th>standalone_eval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>reasoning or code. Full reasoning results are included in Tab. 11.\\nEvaluating across Safety Metrics Tab. 6 (full results in Tab. 10 in Appendix) compares dif-\\nferent reward models across different safety categories, indicating challenges on striking a bal-\\nance between refusing too much or not refusing. Models, such as zephyr-7b-beta and\\nzephyr-7b-gemma-v0.1 show how a model focused on helpfulness without a strong notion of\\nsafety will score poorly on the should-refuse subsets of the safety section, but highly on XSTest\\nShould Respond . Other models, namely those at the top of the overall leaderboard, clearly in-\\nclude safety information in the training process andmaintain strong performance on trick questions\\nthat could induce false refusals ( XSTest Should Respond ). Finally, the third option is also repre-\\nsented in models – those that score highly on prompts that they should refuse and poorly on those\\nthey should not, indicating a model that is likely to falsely refusal queries (for example, the Qwen\\nchat models). These three behavior modes being represented indicates that R EWARD BENCH can\\nbe used as a quick check of the safety behavior of a candidate model, especially when trained with\\nDPO (as it will not need further RL training like the classifier models).\\nEvaluating Length Bias Given the results showing length bias in RLHF and reward models (Sing-\\nhal et al., 2023), we designed R EWARD BENCH so that the chosen responses are either a similar\\nlength or shorter than the rejected responses. For example, the AlpacaEval Length subset is de-\\nsigned to differentiate between other Chat subsets by having notably different models capabilities\\nwith the same average length (results in Tab. 8). In this case, the results are lower than other easy\\n10chat subsets, but 90% plus accuracy is achieved by over 10 models – far above random for most\\nmodels. Though, more detailed statistical tests are needed to fully understand this, as this only tests</td>\n",
              "      <td>Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n</td>\n",
              "      <td>The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.</td>\n",
              "      <td>RewardBench: Evaluating Reward Models for Language Modeling</td>\n",
              "      <td>4.5</td>\n",
              "      <td>R EWARD BENCH evaluates models' safety behavior by comparing their performance on different safety categories, such as should-refuse subsets, trick questions, and prompts that they should refuse. The evaluation considers how well the models balance refusing too much or not refusing, and how well they include safety information in the training process. Additionally, R EWARD BENCH is designed to avoid length bias by choosing responses that are either similar in length or shorter than the rejected responses.\\n\\n</td>\n",
              "      <td>4.5</td>\n",
              "      <td>The R EWARD BENCH is a benchmark for evaluating the safety behavior of NLP models. It measures how well a model can correctly identify potential false refusal queries, which are queries that are intended to be accepted but are incorrectly marked as rejected. The R EWARD BENCH uses a combination of rule-based and machine learning-based approaches to evaluate models' safety behavior.\\n\\n</td>\n",
              "      <td>4.5</td>\n",
              "      <td>The REWARD BENCH evaluates models' safety behavior by measuring their performance on a set of tasks that are designed to test their ability to recognize and respond to potentially harmful or malicious inputs. The benchmark assesses a model's ability to correctly identify and reject inputs that could cause harm to users or systems. The significance of this evaluation in identifying potential false refusal queries lies in the fact that false refusal queries can occur when a model is not able to correctly identify and reject potentially harmful inputs. By evaluating models' safety behavior using the REWARD BENCH, researchers can identify models that are more likely to generate false refusal queries and take steps to improve their performance.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>derman et al., 2023) and Llama- {7B, 13B, 30B }(Touvron\\net al., 2023). This permits us to see how LLM alignment\\nscales within a model family (Llama-2 lacks a 30B model,\\nhence our use of Llama). Later experiments ( §4.2) are done\\non Mistral-7B and its derivatives (Jiang et al., 2023). The\\nmodels were trained on a combination of Anthropic HH\\n(Ganguli et al., 2022), OpenAssistant (K ¨opf et al., 2023),\\nand SHP (Ethayarajh et al., 2022).\\nAll models were aligned under identical settings on the\\nsame data (e.g., same effective batch size, same optimizer,\\netc.), save for hyperparameters unique to them. Similar to\\nRafailov et al. (2023), the target sequences for SFT are a\\nsubset of the generations used to subsequently align the\\nmodel; however, for a more realistic SFT setup, we do not\\nnecessarily set the most preferred generation to be the target\\n(with the exception of HH, since the dispreferred output in\\nthat dataset is often harmful). Then we used GPT-4-0613\\n4Model Alignment as Prospect Theoretic Optimization\\nFigure 3. Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC\\nand CSFT), though the gap is only significant (p &lt; 0.05)at 13B+ model sizes. In fact, only the HALO-aligned Llama- {13B, 30B }\\nmodels are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset.\\nIt is also worth noting that up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage.\\nto judge whether the aligned model’s response was bet-\\nter than the SFT target for the given input with respect to\\nhelpfulness, harmlessness, and conciseness, a now standard\\npractice (Zheng et al., 2023; Li et al., 2023).3Note that\\nwhile the SFT target is considered a desirable output for x,\\nit is by no means the best output, meaning that it can be\\nimproved upon by an aligned model.\\nIn Figure 3, we see the results of this analysis:</td>\n",
              "      <td>Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n</td>\n",
              "      <td>Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p &lt; 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it can be improved upon by an aligned model.</td>\n",
              "      <td>KTO: Model Alignment as Prospect Theoretic Optimization</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              context  \\\n",
              "0              reasoning or code. Full reasoning results are included in Tab. 11.\\nEvaluating across Safety Metrics Tab. 6 (full results in Tab. 10 in Appendix) compares dif-\\nferent reward models across different safety categories, indicating challenges on striking a bal-\\nance between refusing too much or not refusing. Models, such as zephyr-7b-beta and\\nzephyr-7b-gemma-v0.1 show how a model focused on helpfulness without a strong notion of\\nsafety will score poorly on the should-refuse subsets of the safety section, but highly on XSTest\\nShould Respond . Other models, namely those at the top of the overall leaderboard, clearly in-\\nclude safety information in the training process andmaintain strong performance on trick questions\\nthat could induce false refusals ( XSTest Should Respond ). Finally, the third option is also repre-\\nsented in models – those that score highly on prompts that they should refuse and poorly on those\\nthey should not, indicating a model that is likely to falsely refusal queries (for example, the Qwen\\nchat models). These three behavior modes being represented indicates that R EWARD BENCH can\\nbe used as a quick check of the safety behavior of a candidate model, especially when trained with\\nDPO (as it will not need further RL training like the classifier models).\\nEvaluating Length Bias Given the results showing length bias in RLHF and reward models (Sing-\\nhal et al., 2023), we designed R EWARD BENCH so that the chosen responses are either a similar\\nlength or shorter than the rejected responses. For example, the AlpacaEval Length subset is de-\\nsigned to differentiate between other Chat subsets by having notably different models capabilities\\nwith the same average length (results in Tab. 8). In this case, the results are lower than other easy\\n10chat subsets, but 90% plus accuracy is achieved by over 10 models – far above random for most\\nmodels. Though, more detailed statistical tests are needed to fully understand this, as this only tests   \n",
              "1  derman et al., 2023) and Llama- {7B, 13B, 30B }(Touvron\\net al., 2023). This permits us to see how LLM alignment\\nscales within a model family (Llama-2 lacks a 30B model,\\nhence our use of Llama). Later experiments ( §4.2) are done\\non Mistral-7B and its derivatives (Jiang et al., 2023). The\\nmodels were trained on a combination of Anthropic HH\\n(Ganguli et al., 2022), OpenAssistant (K ¨opf et al., 2023),\\nand SHP (Ethayarajh et al., 2022).\\nAll models were aligned under identical settings on the\\nsame data (e.g., same effective batch size, same optimizer,\\netc.), save for hyperparameters unique to them. Similar to\\nRafailov et al. (2023), the target sequences for SFT are a\\nsubset of the generations used to subsequently align the\\nmodel; however, for a more realistic SFT setup, we do not\\nnecessarily set the most preferred generation to be the target\\n(with the exception of HH, since the dispreferred output in\\nthat dataset is often harmful). Then we used GPT-4-0613\\n4Model Alignment as Prospect Theoretic Optimization\\nFigure 3. Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC\\nand CSFT), though the gap is only significant (p < 0.05)at 13B+ model sizes. In fact, only the HALO-aligned Llama- {13B, 30B }\\nmodels are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset.\\nIt is also worth noting that up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage.\\nto judge whether the aligned model’s response was bet-\\nter than the SFT target for the given input with respect to\\nhelpfulness, harmlessness, and conciseness, a now standard\\npractice (Zheng et al., 2023; Li et al., 2023).3Note that\\nwhile the SFT target is considered a desirable output for x,\\nit is by no means the best output, meaning that it can be\\nimproved upon by an aligned model.\\nIn Figure 3, we see the results of this analysis:   \n",
              "\n",
              "                                                                                                                                                                                                                                          question  \\\n",
              "0                                                                      Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n   \n",
              "1  Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 answer  \\\n",
              "0                                                                                                                                                                                                      The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.   \n",
              "1  Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p < 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it can be improved upon by an aligned model.   \n",
              "\n",
              "                                                    source_doc  \\\n",
              "0  RewardBench: Evaluating Reward Models for Language Modeling   \n",
              "1      KTO: Model Alignment as Prospect Theoretic Optimization   \n",
              "\n",
              "   groundedness_score  \\\n",
              "0                 4.5   \n",
              "1                 NaN   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    groundedness_eval  \\\n",
              "0  R EWARD BENCH evaluates models' safety behavior by comparing their performance on different safety categories, such as should-refuse subsets, trick questions, and prompts that they should refuse. The evaluation considers how well the models balance refusing too much or not refusing, and how well they include safety information in the training process. Additionally, R EWARD BENCH is designed to avoid length bias by choosing responses that are either similar in length or shorter than the rejected responses.\\n\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 NaN   \n",
              "\n",
              "   relevance_score  \\\n",
              "0              4.5   \n",
              "1              NaN   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                         relevance_eval  \\\n",
              "0  The R EWARD BENCH is a benchmark for evaluating the safety behavior of NLP models. It measures how well a model can correctly identify potential false refusal queries, which are queries that are intended to be accepted but are incorrectly marked as rejected. The R EWARD BENCH uses a combination of rule-based and machine learning-based approaches to evaluate models' safety behavior.\\n\\n   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "\n",
              "   standalone_score  \\\n",
              "0               4.5   \n",
              "1               NaN   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     standalone_eval  \n",
              "0  The REWARD BENCH evaluates models' safety behavior by measuring their performance on a set of tasks that are designed to test their ability to recognize and respond to potentially harmful or malicious inputs. The benchmark assesses a model's ability to correctly identify and reject inputs that could cause harm to users or systems. The significance of this evaluation in identifying potential false refusal queries lies in the fact that false refusal queries can occur when a model is not able to correctly identify and reject potentially harmful inputs. By evaluating models' safety behavior using the REWARD BENCH, researchers can identify models that are more likely to generate false refusal queries and take steps to improve their performance.\\n\\n  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_questions.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "generated_questions.to_csv(\"../data/pdfs_ws_mrkp_test/generated_questions_pdf_raw.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n</td>\n",
              "      <td>The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n</td>\n",
              "      <td>Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p &lt; 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it can be improved upon by an aligned model.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What specific changes is Toyota implementing to its EV business environment and how does it differ from other companies that have recently embraced a hybrid fleet?\\n\\n</td>\n",
              "      <td>Toyota is implementing changes to its executives' areas of responsibility that require disclosure of further changes to its policies and practices as the larger EV business evolves. The company has not had a hybrid supply chain for years and hybrid research is not allowed inside its laboratories. These changes will only come with the launch of a new integrated EV product division, which has not been finalized yet. Additionally, Toyota's \"Reuturistic and Singular Communication Systems\" are the industry's most ambitious \"big guy\" partnerships, and the company is opening the first car dealerships in the United States as the name suggests. There is no plan to extend out its RAV4 EV program beyond the University of Arizona system in Reno. Consumers are more willing to hop on the electric car bandwagon than at any time in history, and the shift to EV mandates has become what almost everyone has been dying to do for decades.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the estimated local density of an example using the kernel sum approach and the Densi-LLM method?\\n\\n</td>\n",
              "      <td>The estimated local density of an example using the kernel sum approach and the Densi-LLM method is calculated by taking the kernel sum of the scores of all examples in the dataset. The kernel is a function that determines the similarity between two examples based on their embeddings. The smoothing parameter λ controls the scale of the points' effects and helps reduce the complexity from O(N^2) to O(NlogN). The method resembles that of Coleman et al. (2022), except that it uses a two-pass sampling algorithm with stronger theoretical guarantees and performs the density estimation in the latent space of the model.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n</td>\n",
              "      <td>In the multi-turn instruction-following task, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) achieved comparable results to larger or proprietary models such as Llama-2-Chat (70B) and Claude V1. Specifically, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-Bench without being exposed to the multi-turn conversation dataset.</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                          question  \\\n",
              "0                                                                      Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n   \n",
              "1  Which existing alignment methods outperform non-HALOs in generating high-quality responses, and how do the HALO-aligned Llama-{13B, 30B} models compare to the SFT target sequences in terms of helpfulness, harmlessness, and conciseness?\\n\\n   \n",
              "2                                                                          What specific changes is Toyota implementing to its EV business environment and how does it differ from other companies that have recently embraced a hybrid fleet?\\n\\n   \n",
              "3                                                                                                                                    What is the estimated local density of an example using the kernel sum approach and the Densi-LLM method?\\n\\n   \n",
              "4                              In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                     The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.   \n",
              "1                                                                                                 Among existing alignment methods, the HALOs (DPO and our offline PPO variant) generally outperform non-HALOs (SLiC and CSFT). However, the gap is only significant (p < 0.05) at 13B+ model sizes. The HALO-aligned Llama-{13B, 30B} models are able to match or exceed the generation quality of SFT target sequences, which are drawn directly from the alignment dataset. Up to a scale of 7B parameters, virtually all of the gains from LLM alignment come from the SFT stage. When judging whether the aligned model's response was better than the SFT target for the given input with respect to helpfulness, harmlessness, and conciseness, a now standard practice is to use a standard analysis that considers the SFT target as a desirable output for x, but not necessarily the best output, meaning that it can be improved upon by an aligned model.   \n",
              "2  Toyota is implementing changes to its executives' areas of responsibility that require disclosure of further changes to its policies and practices as the larger EV business evolves. The company has not had a hybrid supply chain for years and hybrid research is not allowed inside its laboratories. These changes will only come with the launch of a new integrated EV product division, which has not been finalized yet. Additionally, Toyota's \"Reuturistic and Singular Communication Systems\" are the industry's most ambitious \"big guy\" partnerships, and the company is opening the first car dealerships in the United States as the name suggests. There is no plan to extend out its RAV4 EV program beyond the University of Arizona system in Reno. Consumers are more willing to hop on the electric car bandwagon than at any time in history, and the shift to EV mandates has become what almost everyone has been dying to do for decades.   \n",
              "3                                                                                                                                                                                                                                                                                                                          The estimated local density of an example using the kernel sum approach and the Densi-LLM method is calculated by taking the kernel sum of the scores of all examples in the dataset. The kernel is a function that determines the similarity between two examples based on their embeddings. The smoothing parameter λ controls the scale of the points' effects and helps reduce the complexity from O(N^2) to O(NlogN). The method resembles that of Coleman et al. (2022), except that it uses a two-pass sampling algorithm with stronger theoretical guarantees and performs the density estimation in the latent space of the model.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In the multi-turn instruction-following task, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) achieved comparable results to larger or proprietary models such as Llama-2-Chat (70B) and Claude V1. Specifically, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-Bench without being exposed to the multi-turn conversation dataset.   \n",
              "\n",
              "   groundedness_score  relevance_score  standalone_score  \n",
              "0                 4.5              4.5               4.5  \n",
              "1                 NaN              NaN               NaN  \n",
              "2                 4.5              4.0               4.5  \n",
              "3                 NaN              NaN               NaN  \n",
              "4                 4.0              NaN               4.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "for col in [\"groundedness_score\", \"relevance_score\", \"standalone_score\"]:\n",
        "    generated_questions[col] = generated_questions[col].fillna(generated_questions[[\"groundedness_score\", \"relevance_score\", \"standalone_score\"]].min(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oBWuOu1b9jVO",
        "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================\n",
            "Final evaluation dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n</td>\n",
              "      <td>The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What specific changes is Toyota implementing to its EV business environment and how does it differ from other companies that have recently embraced a hybrid fleet?\\n\\n</td>\n",
              "      <td>Toyota is implementing changes to its executives' areas of responsibility that require disclosure of further changes to its policies and practices as the larger EV business evolves. The company has not had a hybrid supply chain for years and hybrid research is not allowed inside its laboratories. These changes will only come with the launch of a new integrated EV product division, which has not been finalized yet. Additionally, Toyota's \"Reuturistic and Singular Communication Systems\" are the industry's most ambitious \"big guy\" partnerships, and the company is opening the first car dealerships in the United States as the name suggests. There is no plan to extend out its RAV4 EV program beyond the University of Arizona system in Reno. Consumers are more willing to hop on the electric car bandwagon than at any time in history, and the shift to EV mandates has become what almost everyone has been dying to do for decades.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n</td>\n",
              "      <td>In the multi-turn instruction-following task, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) achieved comparable results to larger or proprietary models such as Llama-2-Chat (70B) and Claude V1. Specifically, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-Bench without being exposed to the multi-turn conversation dataset.</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                              question  \\\n",
              "0                                          Based on the provided context, how does the R EWARD BENCH evaluate models' safety behavior and what is its significance in identifying potential false refusal queries?\\n\\n   \n",
              "2                                              What specific changes is Toyota implementing to its EV business environment and how does it differ from other companies that have recently embraced a hybrid fleet?\\n\\n   \n",
              "4  In a multi-turn instruction-following task, how does the performance of Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) compare to that of larger or proprietary models such as Llama-2-Chat (70B) and Claude V1?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                     The R EWARD BENCH evaluates models' safety behavior by assessing their performance on different subsets of the safety section and their ability to distinguish between prompts they should refuse and those they should not. Models that score highly on prompts they should refuse and poorly on those they should not are likely to falsely refuse queries. The significance of the R EWARD BENCH lies in its ability to quickly identify potential false refusal queries, especially when trained with DPO. This can be especially useful in the context of reward models, where striking a balance between refusing too much or not refusing is a challenge.   \n",
              "2  Toyota is implementing changes to its executives' areas of responsibility that require disclosure of further changes to its policies and practices as the larger EV business evolves. The company has not had a hybrid supply chain for years and hybrid research is not allowed inside its laboratories. These changes will only come with the launch of a new integrated EV product division, which has not been finalized yet. Additionally, Toyota's \"Reuturistic and Singular Communication Systems\" are the industry's most ambitious \"big guy\" partnerships, and the company is opening the first car dealerships in the United States as the name suggests. There is no plan to extend out its RAV4 EV program beyond the University of Arizona system in Reno. Consumers are more willing to hop on the electric car bandwagon than at any time in history, and the shift to EV mandates has become what almost everyone has been dying to do for decades.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In the multi-turn instruction-following task, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) achieved comparable results to larger or proprietary models such as Llama-2-Chat (70B) and Claude V1. Specifically, Mistral- ORPO -α(7B) and Mistral- ORPO -β(7B) scored 7.23 and 7.32 in MT-Bench without being exposed to the multi-turn conversation dataset.   \n",
              "\n",
              "   groundedness_score  relevance_score  standalone_score  \n",
              "0                 4.5              4.5               4.5  \n",
              "2                 4.5              4.0               4.5  \n",
              "4                 4.0              4.0               4.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 3.0)\n",
        "    & (generated_questions[\"relevance_score\"] >= 3.0)\n",
        "    & (generated_questions[\"standalone_score\"] >= 3.0)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%#%writefile -a ../scripts/critique_qa.py\n",
        "generated_questions.to_csv(\"../data/pdfs_ws_mrkp_test/generated_questions_pdf_filtered.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaOMZyu69jVO"
      },
      "source": [
        "- Go through the 181 rows remaining post dropping missing vals and missing value imputation visually, keep the better 120ish questions\n",
        "    - Dropped questions that were off-target for learning about LLMs, relied on the reference section, or mentioned the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#eval_dataset = pd.read_csv(\"../data/generated_questions_filtered_w_human.csv\")\n",
        "eval_dataset = generated_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5s19uTd9jVO"
      },
      "source": [
        "# 2. Build our RAG System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mET8Dy9jVO"
      },
      "source": [
        "### 2.1. Preprocessing documents to build our vector database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzBYfNG79jVO"
      },
      "source": [
        "### 2.2. Retriever - embeddings 🗂️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6y1mQJX9jVO"
      },
      "source": [
        "### 2.3. Reader - LLM 💬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "#embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embed_model_id = 'mixedbread-ai/mxbai-embed-large-v1'\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "vector_store = FAISS.load_local('../data/rag_index_dir', embedder,allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9PdpuWyP9jVP"
      },
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        }
      ],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "reader_config = ExLlamaV2Config()\n",
        "reader_config.model_dir = \"../../ZephyrInference\"\n",
        "reader_config.prepare()\n",
        "\n",
        "reader_model = ExLlamaV2(reader_config)\n",
        "cache = ExLlamaV2Cache(reader_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "reader_model.load_autosplit(cache)\n",
        "\n",
        "reader_tokenizer = ExLlamaV2Tokenizer(reader_config)\n",
        "reader_llm = ExLlamaV2StreamingGenerator(reader_model, cache, reader_tokenizer)\n",
        "#reader_llm.set_stop_conditions([reader_tokenizer.eos_token_id])\n",
        "reader_settings = ExLlamaV2Sampler.Settings()\n",
        "reader_settings.temperature = 0.85\n",
        "reader_settings.top_k = 30\n",
        "reader_settings.top_p = 0.8\n",
        "reader_settings.token_repetition_penalty = 1.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Retrieving documents...\n",
            "=> Reranking documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 22.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
            "{'content': 'SELF-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and\\nself-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training\\nlets an LM Mgenerate text informed by retrieved passages, if needed, and criticize the output by\\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\\nor confirm the output’s relevance, support, or completeness. In contrast, common RAG approaches\\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\\nFormally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple\\nsegments y= [y1, . . . , y T], where ytindicates a sequence of tokens for the t-th segment.3Generated\\ntokens in ytinclude text from the original vocabulary as well as the reflection tokens (Table 1).\\n2All work is arXived within a week of this preprint.\\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\\nsegment unit (i.e., sub-sentence).\\n3Preprint.\\nType Input Output Definitions\\nRetrieve x/x, y {yes, no, continue } Decides when to retrieve with R\\nISREL x, d {relevant , irrelevant } dprovides useful information to solve x.\\nISSUP x, d, y {fully supported , partially\\nsupported, no support }All of the verification-worthy statement in y\\nis supported by d.\\nISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x.\\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\\nthe most desirable critique tokens. x, y, d indicate input, output, and a relevant passage, respectively.\\nAlgorithm 1 SELF-RAGInference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}', 'score': 20.46875, 'rank': 0, 'result_index': 3}\n",
            "Len of relevant_docs: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "   # tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512,\n",
        "    knowledge_index: FAISS = vector_store,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 10, #30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    embedding_vector = core_embeddings_model.embed_query(question)\n",
        "    relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    #Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = RERANKER.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        #print(f\"Type is : {type(relevant_docs[0])}\")\n",
        "        print(dir(relevant_docs[0]))\n",
        "        print(relevant_docs[0])\n",
        "        relevant_docs = [doc['content'] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "    print(f'Len of relevant_docs: {len(relevant_docs)}')\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "    generator.warmup()\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = generator.generate_simple(final_prompt, \n",
        "    settings, max_new_tokens, seed = 1234)\n",
        "    # print(answer)\n",
        "    return answer,relevant_docs\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512,reranker =True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Self-RAG, introduced in the paper \"SELF-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection,\" is an improvement over the ad hoc approach of Retrieval-Augmented Generation (RAG) for enhancing the factuality and quality of large language models (LLMs). While RAG indiscriminately retrieves and incorporates a fixed number of passages regardless of their relevance or necessity, Self-RAG trains a single LLM to adaptively retrieve passages on demand and generates and reflects on retrieved passages and its own generations using special tokens called reflection tokens. Self-RAG enables a customizable decoding algorithm to satisfy hard or soft constraints and improves overall performance, factuality, and verifiability for various tasks. Empirical results show that Self-RAG significantly outperforms pre-trained and instruction-tuned LLMs and conventional RAG approaches for reasoning, long-form generation, and fact verification tasks.'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_end_idx=answer.find('<|assistant|>')\n",
        "answer[prompt_end_idx+14:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- OK, Zephyr seems to work well, under 4s/question with exl2.  Will try to setup reranker, then onto generating questions and relevant docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "   # tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512,\n",
        "    knowledge_index: FAISS = vector_store,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 10, #30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    embedding_vector = core_embeddings_model.embed_query(question)\n",
        "    relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_retrieved_docs]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "    generator.warmup()\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = generator.generate_simple(final_prompt, \n",
        "    settings, max_new_tokens, seed = 1234)\n",
        "    return answer,relevant_docs\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512,reranker = RERANKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiygbqfT9jVP"
      },
      "source": [
        "# 3. Benchmarking the RAG system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'kaist-ai/prometheus-13b-v1.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.vectorstores import VectorStore\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#eval_dataset = pd.read_csv(\"../data/generated_questions_filtered_w_human.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrlMh_ZI9jVP"
      },
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "from collections import namedtuple\n",
        "def run_rag_tests(\n",
        "    dataset: pd.DataFrame,\n",
        "    llm: ExLlamaV2StreamingGenerator,\n",
        "    knowledge_index: VectorStore,\n",
        "    #output_file: str,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    verbose: Optional[bool] = False,\n",
        "    test_settings: Optional[str] = None,  # To document the test settings used\n",
        "):\n",
        "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
        "\n",
        "    dataset_copy = dataset.copy(deep=True)\n",
        "    dataset_copy['retrieved_docs'] = None\n",
        "    for example_row in tqdm(dataset_copy.iterrows()):\n",
        "        index, example = example_row\n",
        "        question = example[\"question\"]\n",
        "        if dataset_copy.loc[index,'retrieved_docs']: #already retrieved\n",
        "            print(f\"Continue for {index} since already processed\")\n",
        "            continue\n",
        "\n",
        "        generated_answer, relevant_docs =  answer_with_rag(question, knowledge_index=knowledge_index, generator=llm,settings=reader_settings,max_new_tokens=512,reranker = reranker)\n",
        "        if verbose:\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'True answer: {example[\"answer\"]}')\n",
        "        dataset_copy.at[index,'retrieved_docs'] = relevant_docs\n",
        "        dataset_copy.loc[index,'true_answer'] = dataset_copy.loc[index,'answer']\n",
        "        dataset_copy.loc[index,'generated_answer'] = generated_answer\n",
        "\n",
        "\n",
        "        if test_settings:\n",
        "            dataset_copy[\"test_settings\"] = test_settings\n",
        "    return dataset_copy #INDENTED ON PURPOSE, TEST RUN!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "ds_rag = run_rag_tests(eval_dataset,reader_llm,vector_store,reranker = None,test_settings='MistralQs-all_MiniLM_L6_v2Embed-ZephyrRead-2000x200chunks-NoRerank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_rag.to_csv(\"../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ds_rag.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_rag = pd.read_csv(\"../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "\n",
        "judge_config = ExLlamaV2Config()\n",
        "judge_config.model_dir = \"../PrometheusEval\"\n",
        "#judge_config.model_dir = '../Mixtral4bit'\n",
        "judge_config.prepare()\n",
        "\n",
        "judge_model = ExLlamaV2(judge_config)\n",
        "cache = ExLlamaV2Cache(judge_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "judge_model.load_autosplit(cache)\n",
        "\n",
        "judge_tokenizer = ExLlamaV2Tokenizer(judge_config)\n",
        "judge_llm = ExLlamaV2StreamingGenerator(judge_model, cache, judge_tokenizer)\n",
        "#judge_llm.set_stop_conditions([judge_tokenizer.eos_token_id])\n",
        "judge_settings = ExLlamaV2Sampler.Settings()\n",
        "judge_settings.temperature = 1.0\n",
        "# judge_settings.top_k = 30\n",
        "# judge_settings.top_p = 0.8\n",
        "# judge_settings.token_repetition_penalty = 1.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae-3KWzK9jVP"
      },
      "outputs": [],
      "source": [
        "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
        "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "\n",
        "###The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "###Response to evaluate:\n",
        "{response}\n",
        "\n",
        "###Reference Answer (Score 5):\n",
        "{reference_answer}\n",
        "\n",
        "###Score Rubrics:\n",
        "[Is the response correct, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\n",
        "###Feedback:\"\"\"\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "\n",
        "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
        "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia9Mvn859jVP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def evaluate_answers(\n",
        "    answer_path: str,\n",
        "    eval_chat_model:ExLlamaV2StreamingGenerator,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    evaluation_prompt: str\n",
        ") -> None:\n",
        "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
        "    answers = []\n",
        "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
        "        answers = pd.read_csv(answer_path)\n",
        "    for example_row in tqdm(answers.iterrows()):\n",
        "        index, example = example_row\n",
        "        if f\"eval_score\" in example:\n",
        "            continue\n",
        "\n",
        "        eval_prompt = evaluation_prompt.format(\n",
        "            instruction=example[\"question\"],\n",
        "            response=example[\"generated_answer\"],\n",
        "            reference_answer=example[\"true_answer\"],\n",
        "        )\n",
        "\n",
        "        eval_chat_model.warmup()\n",
        "        \n",
        "        eval_result = eval_chat_model.generate_simple(eval_prompt, \n",
        "        settings, num_tokens=1024, seed = 1234) #max_new_tokens=1024,\n",
        "        feedback = re.search(r'###Feedback:\\s*(.*)',eval_result,re.DOTALL).group(1)\n",
        "        try:\n",
        "            #score = re.search(r'(\\d+)', feedback).group(1)\n",
        "            score = re.search(r'overall score is (\\d)', feedback).group(1)\n",
        "        except AttributeError:\n",
        "            score = 'NaN'\n",
        "        answers.loc[index,f\"eval_score\"] = score\n",
        "        answers.loc[index,f\"eval_feedback\"] = feedback\n",
        "        print(f'Score: {score}')\n",
        "        print(f'Feedback: {feedback}')\n",
        "    return answers #INDENTED ON PURPOSE, TEST RUN!\n",
        "        # with open(answer_path, \"w\") as f:\n",
        "        #     json.dump(answers, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "# temp=evaluate_answers(answer_path='../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank.csv',\n",
        "#                  eval_chat_model=judge_llm,settings=judge_settings,evaluation_prompt=EVALUATION_PROMPT) # SHOULD BE evaluation_prompt_template !!!\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "temp=evaluate_answers(answer_path='../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank.csv',\n",
        "                 eval_chat_model=judge_llm,settings=judge_settings,evaluation_prompt=evaluation_prompt_template) # SHOULD BE evaluation_prompt_template !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "temp.to_csv(\"../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank-Evaluated.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "import matplotlib.pyplot as plt\n",
        "temp.eval_score.sort_values().hist()\n",
        "plt.title(\"Pdf-MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank\");\n",
        "plt.savefig('../data/pdfs_ws_mrkp_test/Pdf-MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank-Evaluated.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW2nnvUT9jVQ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./output\"):\n",
        "    os.mkdir(\"./output\")\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [True, False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(f\"Running evaluation for {settings_name}:\")\n",
        "\n",
        "            print(\"Loading knowledge base embeddings...\")\n",
        "            knowledge_index = load_embeddings(\n",
        "                RAW_KNOWLEDGE_BASE,\n",
        "                chunk_size=chunk_size,\n",
        "                embedding_model_name=embeddings,\n",
        "            )\n",
        "\n",
        "            print(\"Running RAG...\")\n",
        "            reranker = (\n",
        "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "                if rerank\n",
        "                else None\n",
        "            )\n",
        "            run_rag_tests(\n",
        "                eval_dataset=eval_dataset,\n",
        "                llm=READER_LLM,\n",
        "                knowledge_index=knowledge_index,\n",
        "                output_file=output_file_name,\n",
        "                reranker=reranker,\n",
        "                verbose=False,\n",
        "                test_settings=settings_name,\n",
        "            )\n",
        "\n",
        "            print(\"Running evaluation...\")\n",
        "            evaluate_answers(\n",
        "                output_file_name,\n",
        "                eval_chat_model,\n",
        "                evaluator_name,\n",
        "                evaluation_prompt_template,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tytXV5-h9jVT"
      },
      "source": [
        "### Inspect results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "#outputs = []\n",
        "#RagOverArXiv/data/pdfs_ws_mrkp_test/eval_outputs\n",
        "pdf_files = glob.glob(\"../data/pdfs_ws_mrkp_test/eval_outputs/pdf*.csv\")\n",
        "txt_files = glob.glob(\"../data/pdfs_ws_mrkp_test/eval_outputs/txt*.csv\")\n",
        "all_files = pdf_files+txt_files\n",
        "result = pd.DataFrame()#index=list(all_files.index))\n",
        "for file_path in all_files:\n",
        "    print(file_path)\n",
        "    output = pd.read_csv(file_path)\n",
        "    print(output.columns)\n",
        "    output[\"settings\"] = file_path.split('/')[-1]\n",
        "    result = pd.concat([result,output])\n",
        "print(result.shape)\n",
        "pd.set_option('display.max_colwidth',500)\n",
        "result[['eval_score','eval_feedback']].head(20)\n",
        "#result.eval_score.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(result['eval_score'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result.eval_score.isna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdkXMNvS9jVT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "result[\"eval_score\"] = result[\"eval_score\"].apply(\n",
        "    lambda x: x if not pd.isna(x) else 1\n",
        ")\n",
        "result[\"eval_score\"] = (result[\"eval_score\"] - 1) / 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result[['eval_score','eval_feedback']].head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgxBpid29jVT",
        "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
      },
      "outputs": [],
      "source": [
        "average_scores = result.groupby(\"settings\")[\"eval_score\"].mean()\n",
        "average_scores.sort_values()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
