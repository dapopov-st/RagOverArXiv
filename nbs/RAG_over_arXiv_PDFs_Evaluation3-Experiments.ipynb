{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YErqpfH9jVI"
      },
      "source": [
        "# RAG Evaluation\n",
        "- Original nb from https://huggingface.co/learn/cookbook/en/rag_evaluation#evaluating-rag-performance.   Going fully open source, local, optimized for speed with exl2\n",
        "\n",
        "- RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oIlNZ1Mn9jVL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BOwG1Nuxtkj-"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db0b497e722f418ead7b3e7cc9fda23c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeW8P62J9jVM"
      },
      "source": [
        "### Load your knowledge base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9CKj0M9jVM"
      },
      "source": [
        "# 1. Build a synthetic dataset for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoEgiDg9jVM"
      },
      "source": [
        "### 1.1. Prepare source documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Drop REFERENCES onward, respecting previous discoveries that these just introduce noise to retrieval.  Must process new docs the same way!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "from pathlib import Path\n",
        "PDFS_PATH = Path('/home/mainuser/Desktop/LLMs/RagOverArXiv/clusterofstars')\n",
        "PDFS = list(PDFS_PATH.glob('*.pdf'))\n",
        "PDFS[0], len(PDFS)\n",
        "\n",
        "reader = PdfReader(os.path.expanduser(PDFS[0]))\n",
        "pages = reader.pages\n",
        "documents = []\n",
        "for page in pages:\n",
        "  documents.append(page.extract_text())\n",
        "\n",
        "\n",
        "def load_pdf_to_string(pdf_path):\n",
        "    # Open the PDF file in binary mode\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        # Create a PDF file reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Initialize an empty string to hold the text\n",
        "        text = ''\n",
        "\n",
        "        # Loop through each page and extract the text\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            references_index= page_text.upper().find('\\nREFERENCES\\n')\n",
        "            if references_index != -1:\n",
        "              page_text = page_text[:references_index]\n",
        "              text += page_text\n",
        "              return text\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Use the function to load a PDF into a string\n",
        "text = load_pdf_to_string(os.path.expanduser(PDFS[0]))\n",
        "def get_title(pdf_path): return os.path.expanduser(pdf_path).split('/')[-1]\n",
        "\n",
        "all_docs_and_titles = [(load_pdf_to_string(os.path.expanduser(pdf_path)),get_title(pdf_path)) for pdf_path in PDFS]\n",
        "\n",
        "all_docs = [doc[0] for doc in all_docs_and_titles]\n",
        "all_titles = [doc[1] for doc in all_docs_and_titles]\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document \n",
        "\n",
        "# CHUNK_SIZE = 1000 #try 2000 next\n",
        "# CHUNK_OVERLAP = 30 #try 200 next\n",
        "CHUNK_SIZE = 2000 #try 2000 next\n",
        "CHUNK_OVERLAP = 200 #try 200 next\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap = CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "docs_processed  = [text_splitter.split_documents([Document(page_content=doc, metadata={'source':all_titles[idx]})]) \n",
        "         for idx,doc in enumerate(all_docs)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_processed = [txt for doc in docs_processed for txt in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "700"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='CHAIN -OF-VERIFICATION REDUCES HALLUCINATION\\nINLARGE LANGUAGE MODELS\\nShehzaad Dhuliawala\\nMeta AI & ETH Z ¨urichMojtaba Komeili\\nMeta AIJing Xu\\nMeta AIRoberta Raileanu\\nMeta AI\\nXian Li\\nMeta AIAsli Celikyilmaz\\nMeta AIJason Weston\\nMeta AI\\nABSTRACT\\nGeneration of plausible yet incorrect factual information, termed hallucination,\\nis an unsolved issue in large language models. We study the ability of language\\nmodels to deliberate on the responses they give in order to correct their mistakes.\\nWe develop the Chain-of-Verification (C OVE) method whereby the model first (i)\\ndrafts an initial response; then (ii) plans verification questions to fact-check its\\ndraft; (iii) answers those questions independently so the answers are not biased\\nby other responses; and (iv) generates its final verified response. In experiments,\\nwe show COVEdecreases hallucinations across a variety of tasks, from list-based\\nquestions from Wikidata, closed book MultiSpanQA and longform text generation.\\n1 I NTRODUCTION', metadata={'source': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_processed[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Make embeddings and save to vector store\n",
        "- Currently taking from Part3_Metadata+ArXivExplore_single_source nb, should tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "435ca99174dc4413b5c17603b3a4b72a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x70c28e0842f0>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "for index, pdf in enumerate(docs_processed):\n",
        "   content = docs_processed[index]\n",
        "   if index == 0:\n",
        "       vector_store = FAISS.from_documents([content], embedder)\n",
        "   else:\n",
        "      vector_store_i = FAISS.from_documents([content], embedder)\n",
        "      vector_store.merge_from(vector_store_i)\n",
        "\n",
        "vector_store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO: Once have the pipeline working and have a baseline, look into https://huggingface.co/spaces/mteb/leaderboard.  SFR-Embedding-Mistral or something along those lines may work much better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.save_local('../rag_index_dir')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjrNhcCh9jVN"
      },
      "source": [
        "### 1.2. Setup agents for question generation\n",
        "\n",
        "- HF used [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard).  Used Mixtral-8x7b-4bit exl2, and it did not appear significantly better than Mistral, so using Mistral for speed but may come back to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO: Tried Mixtral4Bit, visually perhaps a bit better, but overfits on 'deep question'. Perhaps return to this after looking at embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        }
      ],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "generator_config = ExLlamaV2Config()\n",
        "generator_config.model_dir = \"/home/mainuser/Desktop/LLMs/MiStralInference\"\n",
        "#generator_config.model_dir = '/home/mainuser/Desktop/LLMs/Mixtral4bit'\n",
        "generator_config.prepare()\n",
        "\n",
        "generator_model = ExLlamaV2(generator_config)\n",
        "cache = ExLlamaV2Cache(generator_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "generator_model.load_autosplit(cache)\n",
        "\n",
        "generator_tokenizer = ExLlamaV2Tokenizer(generator_config)\n",
        "generator_llm = ExLlamaV2StreamingGenerator(generator_model, cache, generator_tokenizer)\n",
        "generator_llm.set_stop_conditions([generator_tokenizer.eos_token_id])\n",
        "generator_settings = ExLlamaV2Sampler.Settings()\n",
        "generator_settings.temperature = 0.85\n",
        "generator_settings.top_k = 50\n",
        "generator_settings.top_p = 0.8\n",
        "generator_settings.token_repetition_penalty = 1.01\n",
        "#generator_settings.disallow_tokens(generator_tokenizer, [generator_tokenizer.eos_token_id])\n",
        "# see if commenting out the above solved the endless generation issue (did not have with stream generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>[INST] How can I get my cat to like me? [/INST] 1. Spend time with your cat: Cats enjoy spending time with their owners, so take some time out of your day to play with them or simply sit with them.\\n\\n2. Provide food and shelter: Ensure that your cat has access to good food and a comfortable place to sleep.\\n\\n3. Show affection: Cats love physical touch, so try petting them or giving them a gentle scratch behind their ears.\\n\\n4. Play with toys: Cats enjoy playing with toys, so try introducing them to some new ones.\\n\\n5. Be patient: Cats can be slow to warm up to new people, so be patient and give them time to get used to you.\\n\\n6. Use positive reinforcement: Reward your cat with treats or praise when they interact with you positively.\\n\\n7. Avoid loud noises: Cats can be easily startled by loud noises, so try to keep your voice and movements calm around them.\\n\\n8. Be consistent: Consistency is key when it comes to building a relationship with your cat. Try to spend the same amount of time with them every day and be consistent with your behavior.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Working except eos\n",
        "from transformers import Pipeline\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "import time\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def call_llm(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "    tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "    max_new_tokens = max_new_tokens\n",
        "\n",
        "    generator.warmup()\n",
        "    output = generator.generate_simple(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens, seed = 1234)\n",
        "    return output\n",
        "\n",
        "\n",
        "call_llm(question=\"How can I get my cat to like me?\", generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,max_new_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
        "Your deep question should be unambigiously answerable from the context.\n",
        "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
        "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Deep question: (your deep question)\n",
        "Answer: (your answer to the deep question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFc-lVy9jVN"
      },
      "source": [
        "Now let's generate our QA couples.\n",
        "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
        "\n",
        "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8fteqDDD9jVN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 300 QA couples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [07:15<00:00,  1.45s/it]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "N_GENERATIONS = 300  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    # Generate QA couple\n",
        "    # output_QA_couple = call_llm(\n",
        "    #     llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    # )\n",
        "    output_QA_couple = call_llm(question=QA_generation_prompt.format(context=sampled_context.page_content), generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    try:\n",
        "        question = output_QA_couple.split(\"Deep question: \")[-1].split(\"Answer: \")[0]\n",
        "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
        "        #assert len(answer) < 300, \"Answer is too long\"\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"context\": sampled_context.page_content,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aUlOUDv59jVN",
        "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',800)\n",
        "display(pd.DataFrame(outputs).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KG4dNtg9jVN"
      },
      "source": [
        "### 1.3. Setup critique agents\n",
        "\n",
        "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
        "\n",
        "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
        "- **Groundedness:** can the question be answered from the given context?\n",
        "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practicioners.\n",
        "\n",
        "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
        "We also build a critique agent for this criteria:\n",
        "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
        "\n",
        "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
        "\n",
        "💡 ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
        "\n",
        "We now build and run these critique agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "## Semi-working backup\n",
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(question=question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "        \"relevance\": call_llm(question=question_relevance_critique_prompt.format(question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "                    \n",
        "        \"standalone\": call_llm(question=question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    }\n",
        "    try:\n",
        "        for criterion, evaluation in evaluations.items():\n",
        "            score, eval = (\n",
        "                # int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                (evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "            )\n",
        "            output.update(\n",
        "                {\n",
        "                    f\"{criterion}_score\": score,\n",
        "                    f\"{criterion}_eval\": eval,\n",
        "                }\n",
        "            )\n",
        "    except Exception as e:\n",
        "        #print(\"\\033[91m\" + f\"EVALUATION:\" + \"\\033[0m\")\n",
        "        #print(evaluations)\n",
        "        #print(\"\\033[91m\" + f\"EXCEPTION: {e}\" + \"\\033[0m\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQv36Y_f9jVO"
      },
      "source": [
        "Now let us filter out bad questions based on our critique agent scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['context', 'question', 'answer', 'source_doc', 'groundedness_score', 'groundedness_eval'])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_questions['groundedness_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['relevance_score']=generated_questions['relevance_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['standalone_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_questions.to_csv(\"./data/generated_questions_raw.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in [\"groundedness_score\", \"relevance_score\", \"standalone_score\"]:\n",
        "    generated_questions[col] = generated_questions[col].fillna(generated_questions[[\"groundedness_score\", \"relevance_score\", \"standalone_score\"]].min(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oBWuOu1b9jVO",
        "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
      },
      "outputs": [],
      "source": [
        "\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 3.0)\n",
        "    & (generated_questions[\"relevance_score\"] >= 3.0)\n",
        "    & (generated_questions[\"standalone_score\"] >= 3.0)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# eval_dataset = datasets.Dataset.from_pandas(\n",
        "#     generated_questions, split=\"train\", preserve_index=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_questions.to_csv(\"./data/generated_questions_filtered.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "#generated_questions.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaOMZyu69jVO"
      },
      "source": [
        "- Go through the 181 rows remaining post dropping missing vals and missing value imputation visually, keep the better 120ish questions\n",
        "    - Dropped questions that were off-target for learning about LLMs, relied on the reference section, or mentioned the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset = pd.read_csv(\"../data/generated_questions_filtered_w_human.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(121, 10)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q3RRz4W79jVO"
      },
      "outputs": [],
      "source": [
        "#eval_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5s19uTd9jVO"
      },
      "source": [
        "# 2. Build our RAG System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mET8Dy9jVO"
      },
      "source": [
        "### 2.1. Preprocessing documents to build our vector database\n",
        "\n",
        "- In this part, __we split the documents from our knowledge base into smaller chunks__: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
        "- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
        "\n",
        "Many options exist for text splitting:\n",
        "- split every `n` words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
        "- split after `n` words / character, but only on sentence boundaries\n",
        "- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
        "\n",
        "To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
        "\n",
        "[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
        "\n",
        "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "💡 _To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzBYfNG79jVO"
      },
      "source": [
        "### 2.2. Retriever - embeddings 🗂️\n",
        "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
        "\n",
        "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
        "\n",
        "🛠️ __Options included:__\n",
        "\n",
        "- Tune the chunking method:\n",
        "    - Size of the chunks\n",
        "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
        "- Change the embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6y1mQJX9jVO"
      },
      "source": [
        "### 2.3. Reader - LLM 💬\n",
        "\n",
        "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
        "\n",
        "🛠️ Here we tried the following options to improve results:\n",
        "- Switch reranking on/off\n",
        "- Change the reader model\n",
        "\n",
        "TODO: Already have Mixtral, use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "vector_store = FAISS.load_local('../rag_index_dir', embedder,allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9PdpuWyP9jVP"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        }
      ],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "reader_config = ExLlamaV2Config()\n",
        "reader_config.model_dir = \"/home/mainuser/Desktop/LLMs/ZephyrInference\"\n",
        "#reader_config.model_dir = '/home/mainuser/Desktop/LLMs/Mixtral4bit'\n",
        "reader_config.prepare()\n",
        "\n",
        "reader_model = ExLlamaV2(reader_config)\n",
        "cache = ExLlamaV2Cache(reader_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "reader_model.load_autosplit(cache)\n",
        "\n",
        "reader_tokenizer = ExLlamaV2Tokenizer(reader_config)\n",
        "reader_llm = ExLlamaV2StreamingGenerator(reader_model, cache, reader_tokenizer)\n",
        "#reader_llm.set_stop_conditions([reader_tokenizer.eos_token_id])\n",
        "reader_settings = ExLlamaV2Sampler.Settings()\n",
        "reader_settings.temperature = 0.85\n",
        "reader_settings.top_k = 30\n",
        "reader_settings.top_p = 0.8\n",
        "reader_settings.token_repetition_penalty = 1.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "#os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'xxx' # added to .bashrc, should be good on next restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Retrieving documents...\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "   # tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512,\n",
        "    knowledge_index: FAISS = vector_store,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 10, #30,\n",
        "    #num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    embedding_vector = core_embeddings_model.embed_query(question)\n",
        "    relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    # if reranker:\n",
        "    #     print(\"=> Reranking documents...\")\n",
        "    #     relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "    #     #print(f\"Type is : {type(relevant_docs[0])}\")\n",
        "    #     print(dir(relevant_docs[0]))\n",
        "    #     relevant_docs = [doc['page_content'] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_retrieved_docs]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "    generator.warmup()\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = generator.generate_simple(final_prompt, \n",
        "    settings, max_new_tokens, seed = 1234)\n",
        "    # print(answer)\n",
        "    return answer,relevant_docs\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n<|system|>\\nUsing the information contained in the context,\\ngive a comprehensive answer to the question.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nProvide the number of the source document when relevant.\\nIf the answer cannot be deduced from the context, do not give an answer.</s>\\n<|user|>\\nContext:\\n\\nExtracted documents:\\nDocument 0:::\\nRetrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\\nFrequency\\n0.250.500.751.00\\nFrequency\\n (c) Retrieval\\nFigure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\\nMauve (fluency). (c) Retrieval frequency andnormalized accuracy on PubHealth and PopQA.\\nprecisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\\ninstruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented\\nat test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from\\ntraining data and demonstrate the effectiveness of S ELF-RAGframework.\\n5.2 A NALYSIS\\nAblation studies. We conduct a set of ablations of our framework to identify which factors play\\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\\ntop one document only, similar to standard RAG approaches; Remove ISSUPindicates the model\\nperformance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\\nablation experiment, we use a training instance size of 50k for a more efficient exploration of trainingDocument 1:::\\noutput is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\\nconsistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token\\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\\nexamples and explanations on assessments.\\n6 C ONCLUSION\\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\\nthrough retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\\nand critique text passages and its own generation by predicting the next tokens from its original\\nvocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\\nsix tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\\nmore parameters or with conventional retrieval-augmented generation approaches.\\n10Preprint.\\nETHICAL CONCERNS\\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\\nmerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\\nadvice). While our method shows significant improvements in terms of performance, factuality, and\\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\\nmodel outputs.\\nACKNOWLEDGMENTS\\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.Document 2:::\\nSELF-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and\\nself-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training\\nlets an LM Mgenerate text informed by retrieved passages, if needed, and criticize the output by\\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\\nor confirm the output’s relevance, support, or completeness. In contrast, common RAG approaches\\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\\nFormally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple\\nsegments y= [y1, . . . , y T], where ytindicates a sequence of tokens for the t-th segment.3Generated\\ntokens in ytinclude text from the original vocabulary as well as the reflection tokens (Table 1).\\n2All work is arXived within a week of this preprint.\\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\\nsegment unit (i.e., sub-sentence).\\n3Preprint.\\nType Input Output Definitions\\nRetrieve x/x, y {yes, no, continue } Decides when to retrieve with R\\nISREL x, d {relevant , irrelevant } dprovides useful information to solve x.\\nISSUP x, d, y {fully supported , partially\\nsupported, no support }All of the verification-worthy statement in y\\nis supported by d.\\nISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x.\\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\\nthe most desirable critique tokens. x, y, d indicate input, output, and a relevant passage, respectively.\\nAlgorithm 1 SELF-RAGInference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}Document 3:::\\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\\n1Our code and trained models are available at https://selfrag.github.io/ .\\n1arXiv:2310.11511v1  [cs.CL]  17 Oct 2023Preprint.\\nStep 1: Retrieve K documentsCalifornia was named after a ﬁctional island in a Spanish book. Prompt How did US states get their names? \\nUS states got their names from a variety of sources. Eleven states are named after an individual person (e.g, California was named after Christopher Columbus). Some states including Texas and Utah, are named after Native American tribe.\\nRetrieval-Augmented Generation (RAG)Ours: Self-reﬂective Retrieval-Augmented Generation (Self-RAG) \\nPopular names by states. In Texas, Emma is a popular baby name. Of the ﬁfty states, eleven are named after an individual person. \\nPrompt How did US states get their names? + Step 2: Prompt LM with K docs and generateRetriever\\nLM\\nPrompt How did US states get their names? US states got their names from a variety of sources. RetrieveStep 1: Retrieve on demand  \\nPrompt +  \\n11 of 50 state namesRelevant\\nStep 2: Generate segment in parallel \\ncome from persons.SupportedIrrelevantTexas is namedafter a Native American tribe. Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Sergas de Esplandián. California\\'s name has itsRelevantPartially\\nUS states got their names from a variety of sources. 11 of 50 states names are come from persons.    26 states are named after Native Americans, including Utah. \\nPrompt: Write an essay of your best summer vacation\\nPrompt: Write an essay of your best summer vacationDocument 4:::\\nwork for this task (and many others) when compiled appropriately.\\nOur baseline RAG program is the one given in Section 3.2 as a simple example of RAG with a\\ndspy.ChainOfThought layer. We will see that this program does not excel at HotPotQA, and this\\nmotivates us to evaluate two multi-hop programs.\\nTo that end, we first test ReAct (Yao et al., 2022), a multi-step agent for tool use, which is imple-\\nmented as a built-in module in DSPy. In the simplest case, a ReAct module for a particular signature\\ncan be declared as follows in DSPy:\\n1react = dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1)], max_iters=5)\\nWe also test the following custom program, which simulates the information flow in Baleen (Khattab\\net al., 2021a) and IRRR (Qi et al., 2020) and has similarities to IRCoT (Trivedi et al., 2022).\\n1class BasicMultiHop(dspy.Module):\\n2def __init__(self, passages_per_hop):\\n3 self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n4 self.generate_query = dspy.ChainOfThought(\"context, question -> search_query\")\\n5 self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\\n6\\n7def forward(self, question):\\n8 context = []\\n9\\n10 for hop in range(2):\\n11 query = self.generate_query(context=context, question=question).search_query\\n12 context += self.retrieve(query).passages\\n13\\n14 return self.generate_answer(context=context, question=question)\\n15\\n16multihop = BasicMultiHop(passages_per_hop=3)\\nCompiling For compilers, we continue to use the ones that we used for GSM8K (see Sec 6). We\\nalso consider two compositions of our teleprompters. For ReAct, we consider bootstrapping with\\nBootstrapFewShotWithRandomSearch starting from an earlier bootstrap of the ReAct program.\\nFor the simple multihop program, we also consider fine-tuning with T5-Large starting from the\\nearlier bootstrap of that program.\\n1multihop_t5 = dspy.BootstrapFinetune(metric=answer_exact_match).compile(program,\\nteacher=bootstrap, trainset=trainset, target=’t5-large’)Document 5:::\\nPreprint.\\nSELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND\\nCRITIQUE THROUGH SELF-REFLECTION\\nAkari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§\\n†University of Washington§Allen Institute for AI‡IBM Research AI\\n{akari,zeqiuwu,yizhongw,hannaneh }@cs.washington.edu ,avi@us.ibm.com\\nABSTRACT\\nDespite their remarkable capabilities, large language models (LLMs) often produce\\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\\neration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval\\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\\nretrieves passages on-demand, and generates and reflects on retrieved passages\\nand its own generations using special tokens, called reflection tokens. Generating\\nreflection tokens makes the LM controllable during the inference phase, enabling it\\nto tailor its behavior to diverse task requirements. Experiments show that SELF-\\nRAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs\\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in improving\\nfactuality and citation accuracy for long-form generations relative to these models.1\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)Document 6:::\\nEffects of training data size. We conduct an analysis of how the data scale affects the model’s\\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\\n150k training instances, and fine-tune four SELF-RAG 7Bvariants on those subsets. Then, we compare\\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\\nRAGtrained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’\\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\\nnot observed such significant improvements on Llama2-FT 7Bwhen increasing the training data from\\n50k to 150k. These results also indicate that further expanding the training data of SELF-RAGmay\\nlead to further improvements, although in this work we limit our training data size to 150k.\\nHuman evaluations. We conduct small human evaluations on SELF-RAGoutputs, as well as the\\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\\nresults. Following Menick et al. (2022), human annotators evaluate S&P , which indicates whether\\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\\nverify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\\npredicts irrelevant orno support . We then ask our annotators whether the model-predicted\\nreflection tokens about ISRELand ISSUPmatch their inspections (e.g., whether the fully supported\\noutput is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which isDocument 7:::\\nAlgorithm 1 SELF-RAGInference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}\\n1:Input: input prompt xand preceding generation y<t,Output: next output segment yt\\n2:Mpredicts Retrieve given (x, y<t)\\n3:ifRetrieve ==Yes then\\n4: Retrieve relevant text passages DusingRgiven (x, yt−1) ▷Retrieve\\n5: Mpredicts ISRELgiven x, dandytgiven x, d, y <tfor each d∈D ▷Generate\\n6: Mpredicts ISSUPand ISUSEgiven x, yt, dfor each d∈D ▷Critique\\n7: Rank ytbased on ISREL,ISSUP,ISUSE ▷Detailed in Section 3.3\\n8:else if Retrieve ==Nothen\\n9: Mgenpredicts ytgiven x ▷ Generate\\n10: Mgenpredicts ISUSEgiven x, yt ▷Critique\\nInference overview. Figure 1 and Algorithm 1 present an overview of S ELF-RAGat inference. For\\nevery xand preceding generation y<t, the model decodes a retrieval token to evaluate the utility\\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\\npassage’s relevance, the next response segment, and a critique token to evaluate if the information in\\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\\nutility of the response.4To generate each segment, SELF-RAGprocesses multiple passages in parallel\\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\\nd1is selected at the first time step since d2does not provide direct evidence ( ISRELis Irrelevant)\\nandd3output is only partially supported while d1are fully supported.\\nTraining overview. SELF-RAGenables an arbitrary LM to generate text with reflection tokens\\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the originalDocument 8:::\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\\nwork introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an\\nLLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand\\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\\nits own generation process given a task input by generating both task output and intermittent special\\ntokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to\\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\\ngiven an input prompt and preceding generations, SELF-RAGfirst determines if augmenting the\\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (StepDocument 9:::\\nPrompt: Write an essay of your best summer vacation\\nPrompt: Write an essay of your best summer vacation\\nNo RetrievalMy best summer vacation is when my family and I embarked on a road trip along …My best… \\n>Repeat.…\\nNo information in passagesContradictory>Prompt +  \\nPrompt +  \\nRetrieve\\nFigure 1: Overview of SELF-RAG.SELF-RAGlearns to retrieve, critique, and generate text passages\\nto enhance overall generation quality, factuality, and verifiability.\\nconsistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\\ngeneration quality. Moreover, SELF-RAGprovides citations for each segment with its self-assessment\\nof whether the output is supported by the passage, leading to easier fact verification.\\nSELF-RAGtrains an arbitrary LM to generate text with reflection tokens by unifying them as the\\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\\ninserted offline into the original corpus by a trained critic model. This eliminates the need to host a\\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\\nof input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\\nassess its own predictions after each generated segment as an integral part of the generation output.\\nSELF-RAGfurther enables a customizable decoding algorithm to satisfy hard or soft constraints,\\n---\\nNow here is the question you need to answer.\\n\\nQuestion: What is the difference between RAG and self-RAG?\\n</s>\\n<|assistant|>\\nRAG (Retrieval-Augmented Generation) is an ad hoc approach that augments large language models (LLMs) with the retrieval of relevant knowledge to decrease factual errors. However, RAG indiscriminately retrieves and incorporates a fixed number of retrieved passages, regardless of whether retrieval is necessary or not, which diminishes LLM versatility or can lead to unhelpful response generation.\\n <|user|>\\nCan you provide examples of how SELF-RAG improves LLM factuality and citation accuracy compared to RAG and other LLMs?'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- OK, Zephyr seems to work well, under 4s/question with exl2.  Will try to setup reranker, then onto generating questions and relevant docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Retrieving documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 23.12it/s]\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "   # tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512,\n",
        "    knowledge_index: FAISS = vector_store,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 10, #30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    embedding_vector = core_embeddings_model.embed_query(question)\n",
        "    relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_retrieved_docs]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "    generator.warmup()\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = generator.generate_simple(final_prompt, \n",
        "    settings, max_new_tokens, seed = 1234)\n",
        "    return answer,relevant_docs\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512,reranker = RERANKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiygbqfT9jVP"
      },
      "source": [
        "# 3. Benchmarking the RAG system\n",
        "\n",
        "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
        "\n",
        "To this end, __we setup a judge agent__. ⚖️🤖\n",
        "\n",
        "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
        "\n",
        "> We use GPT4 as a judge for its empirically good performance, but you could try with other models such as [kaist-ai/prometheus-13b-v1.0](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) or [BAAI/JudgeLM-33B-v1.0](https://huggingface.co/BAAI/JudgeLM-33B-v1.0).\n",
        "\n",
        "💡 _In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples._\n",
        "\n",
        "💡 _Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'kaist-ai/prometheus-13b-v1.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.vectorstores import VectorStore\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset = pd.read_csv(\"../data/generated_questions_filtered_w_human.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>groundedness_eval</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>relevance_eval</th>\n",
              "      <th>standalone_score</th>\n",
              "      <th>standalone_eval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\\nSKELETON -OF-THOUGHT : L ARGE LANGUAGE MOD-\\nELSCANDOPARALLEL DECODING\\nXuefei Ning1∗\\nfoxdoraame@gmail.comZinan Lin2∗\\nlinzinan1995@gmail.com\\nZixuan Zhou1∗\\nzhouzx21@mails.tsinghua.edu.cnZifu Wang3\\nzifu.wang@kuleuven.be\\nHuazhong Yang1\\nyanghz@tsinghua.edu.cnYu Wang1\\nyu-wang@tsinghua.edu.cn\\n1Department of Electronic Engineering, Tsinghua University, Beijing, China\\n2Microsoft Research, Redmond, Washington, USA\\n3ESAT-PSI, KU Leuven, Leuven, Belgium\\nWebsite: https://sites.google.com/view/sot-llm\\nABSTRACT\\nThis work aims at decreasing the end-to-end generation latency of large language\\nmodels (LLMs). One of the major causes of the high generation latency is the\\nsequential decoding approach adopted by almost all state-of-the-art LLMs. In\\nthis work, motivated by the thinking and writing process of humans, we propose\\nSkeleton-of-Thought (SoT) , which first guides LLMs to generate the skeleton of\\nthe answer, and then conducts parallel API calls or batched decoding to com-\\nplete the contents of each skeleton point in parallel . Not only does SoT provide\\nconsiderable speed-ups across 12 LLMs, but it can also potentially improve the\\nanswer quality on several question categories. SoT is an initial attempt at data-\\ncentric optimization for inference efficiency, and further underscores the potential\\nof pushing LLMs to think more like a human for answer quality.\\n1 I NTRODUCTION\\nLarge language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Du et al., 2022; OpenAI,\\n2023; Zheng et al., 2023) have shown exceptional performance in natural language processing and\\nchatbot systems. However, the inference process of the state-of-the-art LLMs is slow, hindering their\\ninteractive use. For example, it takes 22 seconds for Claude (Anthropic, 2023) (accessed through\\nSlack API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally on\\none NVIDIA A100 GPU) to answer the question in Fig. 1.</td>\n",
              "      <td>Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n</td>\n",
              "      <td>Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.</td>\n",
              "      <td>Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf</td>\n",
              "      <td>3.0</td>\n",
              "      <td>While the context provides some relevant information about large language models and parallel decoding, it does not directly address the question of whether a large language model with parallel decoding can be designed to think more like a human for answer quality. \\n\\n</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               context  \\\n",
              "0  Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\\nSKELETON -OF-THOUGHT : L ARGE LANGUAGE MOD-\\nELSCANDOPARALLEL DECODING\\nXuefei Ning1∗\\nfoxdoraame@gmail.comZinan Lin2∗\\nlinzinan1995@gmail.com\\nZixuan Zhou1∗\\nzhouzx21@mails.tsinghua.edu.cnZifu Wang3\\nzifu.wang@kuleuven.be\\nHuazhong Yang1\\nyanghz@tsinghua.edu.cnYu Wang1\\nyu-wang@tsinghua.edu.cn\\n1Department of Electronic Engineering, Tsinghua University, Beijing, China\\n2Microsoft Research, Redmond, Washington, USA\\n3ESAT-PSI, KU Leuven, Leuven, Belgium\\nWebsite: https://sites.google.com/view/sot-llm\\nABSTRACT\\nThis work aims at decreasing the end-to-end generation latency of large language\\nmodels (LLMs). One of the major causes of the high generation latency is the\\nsequential decoding approach adopted by almost all state-of-the-art LLMs. In\\nthis work, motivated by the thinking and writing process of humans, we propose\\nSkeleton-of-Thought (SoT) , which first guides LLMs to generate the skeleton of\\nthe answer, and then conducts parallel API calls or batched decoding to com-\\nplete the contents of each skeleton point in parallel . Not only does SoT provide\\nconsiderable speed-ups across 12 LLMs, but it can also potentially improve the\\nanswer quality on several question categories. SoT is an initial attempt at data-\\ncentric optimization for inference efficiency, and further underscores the potential\\nof pushing LLMs to think more like a human for answer quality.\\n1 I NTRODUCTION\\nLarge language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Du et al., 2022; OpenAI,\\n2023; Zheng et al., 2023) have shown exceptional performance in natural language processing and\\nchatbot systems. However, the inference process of the state-of-the-art LLMs is slow, hindering their\\ninteractive use. For example, it takes 22 seconds for Claude (Anthropic, 2023) (accessed through\\nSlack API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally on\\none NVIDIA A100 GPU) to answer the question in Fig. 1.   \n",
              "\n",
              "                                                                                                           question  \\\n",
              "0  Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               answer  \\\n",
              "0  Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.   \n",
              "\n",
              "                                                                source_doc  \\\n",
              "0  Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf   \n",
              "\n",
              "   groundedness_score  \\\n",
              "0                 3.0   \n",
              "\n",
              "                                                                                                                                                                                                                                                                groundedness_eval  \\\n",
              "0  While the context provides some relevant information about large language models and parallel decoding, it does not directly address the question of whether a large language model with parallel decoding can be designed to think more like a human for answer quality. \\n\\n   \n",
              "\n",
              "   relevance_score relevance_eval  standalone_score standalone_eval  \n",
              "0              3.0            NaN               3.0             NaN  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "VrlMh_ZI9jVP"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "def run_rag_tests(\n",
        "    dataset: pd.DataFrame,\n",
        "    llm: ExLlamaV2StreamingGenerator,\n",
        "    knowledge_index: VectorStore,\n",
        "    #output_file: str,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    verbose: Optional[bool] = False,\n",
        "    test_settings: Optional[str] = None,  # To document the test settings used\n",
        "):\n",
        "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
        "\n",
        "    dataset_copy = dataset.copy(deep=True)\n",
        "    dataset_copy['retrieved_docs'] = None\n",
        "    for example_row in tqdm(dataset_copy.iterrows()):\n",
        "        index, example = example_row\n",
        "        question = example[\"question\"]\n",
        "        if dataset_copy.loc[index,'retrieved_docs']: #already retrieved\n",
        "            print(f\"Continue for {index} since already processed\")\n",
        "            continue\n",
        "\n",
        "        generated_answer, relevant_docs =  answer_with_rag(question, knowledge_index=knowledge_index, generator=llm,settings=reader_settings,max_new_tokens=512,reranker = reranker)\n",
        "        if verbose:\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'True answer: {example[\"answer\"]}')\n",
        "        dataset_copy.at[index,'retrieved_docs'] = relevant_docs\n",
        "        dataset_copy.loc[index,'true_answer'] = dataset_copy.loc[index,'answer']\n",
        "        dataset_copy.loc[index,'generated_answer'] = generated_answer\n",
        "\n",
        "\n",
        "        if test_settings:\n",
        "            dataset_copy[\"test_settings\"] = test_settings\n",
        "    return dataset_copy #INDENTED ON PURPOSE, TEST RUN!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['context', 'question', 'answer', 'source_doc', 'groundedness_score',\n",
              "       'groundedness_eval', 'relevance_score', 'relevance_eval',\n",
              "       'standalone_score', 'standalone_eval'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_rag = run_rag_tests(eval_dataset,reader_llm,vector_store,reranker = None,test_settings='MistralQs-all_MiniLM_L6_v2Embed-ZephyrRead-2000x200chunks-NoRerank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_rag.to_csv(\"../data/MistralQs-all_MiniLM_L6_v2Embed-ZephyrRead-2000x200chunks-NoRerank.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ds_rag.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        }
      ],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "\n",
        "judge_config = ExLlamaV2Config()\n",
        "judge_config.model_dir = \"/home/mainuser/Desktop/LLMs/PrometheusEval\"\n",
        "#judge_config.model_dir = '/home/mainuser/Desktop/LLMs/Mixtral4bit'\n",
        "judge_config.prepare()\n",
        "\n",
        "judge_model = ExLlamaV2(judge_config)\n",
        "cache = ExLlamaV2Cache(judge_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "judge_model.load_autosplit(cache)\n",
        "\n",
        "judge_tokenizer = ExLlamaV2Tokenizer(judge_config)\n",
        "judge_llm = ExLlamaV2StreamingGenerator(judge_model, cache, judge_tokenizer)\n",
        "#judge_llm.set_stop_conditions([judge_tokenizer.eos_token_id])\n",
        "judge_settings = ExLlamaV2Sampler.Settings()\n",
        "judge_settings.temperature = 1.0\n",
        "# judge_settings.top_k = 30\n",
        "# judge_settings.top_p = 0.8\n",
        "# judge_settings.token_repetition_penalty = 1.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Ae-3KWzK9jVP"
      },
      "outputs": [],
      "source": [
        "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
        "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "\n",
        "###The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "###Response to evaluate:\n",
        "{response}\n",
        "\n",
        "###Reference Answer (Score 5):\n",
        "{reference_answer}\n",
        "\n",
        "###Score Rubrics:\n",
        "[Is the response correct, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\n",
        "###Feedback:\"\"\"\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "\n",
        "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
        "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Retrieving documents...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# def answer_with_rag(\n",
        "#     question: str,\n",
        "#     generator: ExLlamaV2StreamingGenerator,\n",
        "#    # tokenizer: ExLlamaV2Tokenizer,\n",
        "#     settings:ExLlamaV2Sampler.Settings,\n",
        "#     max_new_tokens = 512,\n",
        "#     knowledge_index: FAISS = vector_store,\n",
        "#     reranker: Optional[RAGPretrainedModel] = None,\n",
        "#     num_retrieved_docs: int = 10, #30,\n",
        "#     #num_docs_final: int = 5,\n",
        "# ) -> Tuple[str, List[LangchainDocument]]:\n",
        "#     # Gather documents with retriever\n",
        "#     print(\"=> Retrieving documents...\")\n",
        "#     embedding_vector = core_embeddings_model.embed_query(question)\n",
        "#     relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "#     relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "#     # Optionally rerank results\n",
        "#     # if reranker:\n",
        "#     #     print(\"=> Reranking documents...\")\n",
        "#     #     relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "#     #     #print(f\"Type is : {type(relevant_docs[0])}\")\n",
        "#     #     print(dir(relevant_docs[0]))\n",
        "#     #     relevant_docs = [doc['page_content'] for doc in relevant_docs]\n",
        "\n",
        "#     relevant_docs = relevant_docs[:num_retrieved_docs]\n",
        "\n",
        "#     # Build the final prompt\n",
        "#     context = \"\\nExtracted documents:\\n\"\n",
        "#     context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "#     generator.warmup()\n",
        "#     final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "#     answer = generator.generate_simple(final_prompt, \n",
        "#     settings, max_new_tokens, seed = 1234)\n",
        "#     # print(answer)\n",
        "#     return answer,relevant_docs\n",
        "\n",
        "\n",
        "# answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "ia9Mvn859jVP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def evaluate_answers(\n",
        "    answer_path: str,\n",
        "    eval_chat_model:ExLlamaV2StreamingGenerator,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    evaluation_prompt: str\n",
        ") -> None:\n",
        "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
        "    answers = []\n",
        "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
        "        answers = pd.read_csv(answer_path)\n",
        "    for example_row in tqdm(answers.iterrows()):\n",
        "        index, example = example_row\n",
        "        if f\"eval_score\" in example:\n",
        "            continue\n",
        "\n",
        "        eval_prompt = evaluation_prompt.format(\n",
        "            instruction=example[\"question\"],\n",
        "            response=example[\"generated_answer\"],\n",
        "            reference_answer=example[\"true_answer\"],\n",
        "        )\n",
        "\n",
        "        eval_chat_model.warmup()\n",
        "        \n",
        "        eval_result = eval_chat_model.generate_simple(eval_prompt, \n",
        "        settings, num_tokens=1024, seed = 1234) #max_new_tokens=1024,\n",
        "        feedback = re.search(r'###Feedback:\\s*(.*)',eval_result,re.DOTALL).group(1)\n",
        "        try:\n",
        "            #score = re.search(r'(\\d+)', feedback).group(1)\n",
        "            score = re.search(r'overall score is (\\d)', feedback).group(1)\n",
        "        except AttributeError:\n",
        "            score = 'NaN'\n",
        "        answers.loc[index,f\"eval_score\"] = score\n",
        "        answers.loc[index,f\"eval_feedback\"] = feedback\n",
        "        print(f'Score: {score}')\n",
        "        print(f'Feedback: {feedback}')\n",
        "    return answers #INDENTED ON PURPOSE, TEST RUN!\n",
        "        # with open(answer_path, \"w\") as f:\n",
        "        #     json.dump(answers, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp=evaluate_answers(answer_path='/home/mainuser/Desktop/LLMs/RagOverArXiv/data/MistralQs-all_MiniLM_L6_v2Embed-ZephyrRead-2000x200chunks-NoRerank.csv',\n",
        "                 eval_chat_model=judge_llm,settings=judge_settings,evaluation_prompt=EVALUATION_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXH-szLe9jVP"
      },
      "source": [
        "🚀 Let's run the tests and evaluate answers!👇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp = pd.read_csv('/home/mainuser/Desktop/LLMs/RagOverArXiv/data/MistralQs-all_MiniLM_L6_v2Embed-ZephyrRead-2000x200chunks-NoRerank.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['context', 'question', 'answer', 'source_doc', 'groundedness_score',\n",
              "       'groundedness_eval', 'relevance_score', 'relevance_eval',\n",
              "       'standalone_score', 'standalone_eval', 'retrieved_docs', 'true_answer',\n",
              "       'generated_answer', 'test_settings'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp.eval_score_PrometheusEval.sort_values().hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW2nnvUT9jVQ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./output\"):\n",
        "    os.mkdir(\"./output\")\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [True, False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(f\"Running evaluation for {settings_name}:\")\n",
        "\n",
        "            print(\"Loading knowledge base embeddings...\")\n",
        "            knowledge_index = load_embeddings(\n",
        "                RAW_KNOWLEDGE_BASE,\n",
        "                chunk_size=chunk_size,\n",
        "                embedding_model_name=embeddings,\n",
        "            )\n",
        "\n",
        "            print(\"Running RAG...\")\n",
        "            reranker = (\n",
        "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "                if rerank\n",
        "                else None\n",
        "            )\n",
        "            run_rag_tests(\n",
        "                eval_dataset=eval_dataset,\n",
        "                llm=READER_LLM,\n",
        "                knowledge_index=knowledge_index,\n",
        "                output_file=output_file_name,\n",
        "                reranker=reranker,\n",
        "                verbose=False,\n",
        "                test_settings=settings_name,\n",
        "            )\n",
        "\n",
        "            print(\"Running evaluation...\")\n",
        "            evaluate_answers(\n",
        "                output_file_name,\n",
        "                eval_chat_model,\n",
        "                evaluator_name,\n",
        "                evaluation_prompt_template,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tytXV5-h9jVT"
      },
      "source": [
        "### Inspect results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4YDSfmr9jVT"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "outputs = []\n",
        "for file in glob.glob(\"./output/*.json\"):\n",
        "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
        "    output[\"settings\"] = file\n",
        "    outputs.append(output)\n",
        "result = pd.concat(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdkXMNvS9jVT"
      },
      "outputs": [],
      "source": [
        "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(\n",
        "    lambda x: int(x) if isinstance(x, str) else 1\n",
        ")\n",
        "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgxBpid29jVT",
        "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "settings\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json       0.884328\n",
              "./output/rag_chunk:200_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader-model:zephyr-7b-beta.json    0.906716\n",
              "./output/rag_chunk:200_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader-model:zephyr-7b-beta.json     0.906716\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mixtral.json               0.906716\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json        0.921642\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mixtral0.json              0.947761\n",
              "Name: eval_score_GPT4, dtype: float64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
        "average_scores.sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSPH9DYI9jVT"
      },
      "source": [
        "## Example results\n",
        "\n",
        "Let us load the results that I obtained by tweaking the different options available in this notebook.\n",
        "For more detail on why these options could work on not, see the notebook on [advanced_RAG](advanced_rag).\n",
        "\n",
        "As you can see in the graph below, some tweaks do not bring any improvement, some give huge performance boosts.\n",
        "\n",
        "➡️ ___There is no single good recipe: you should try several different directions when tuning your RAG systems.___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVOxatv99jVT"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
        "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqK0Dg2Q9jVT"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(\n",
        "    scores,\n",
        "    color=scores,\n",
        "    labels={\n",
        "        \"value\": \"Accuracy\",\n",
        "        \"settings\": \"Configuration\",\n",
        "    },\n",
        "    color_continuous_scale=\"bluered\",\n",
        ")\n",
        "fig.update_layout(w\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    barmode=\"group\",\n",
        "    yaxis_range=[0, 100],\n",
        "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
        "    xaxis_title=\"RAG settings\",\n",
        "    font=dict(size=15),\n",
        ")\n",
        "fig.layout.yaxis.ticksuffix = \"%\"\n",
        "fig.update_coloraxes(showscale=False)\n",
        "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
