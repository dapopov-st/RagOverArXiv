{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YErqpfH9jVI"
      },
      "source": [
        "# RAG Evaluation\n",
        "- Original nb from https://huggingface.co/learn/cookbook/en/rag_evaluation#evaluating-rag-performance.   Going fully open source, local, optimized for speed with exl2\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIlNZ1Mn9jVL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "import os\n",
        "import sys, os\n",
        "#sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "# Get the current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# Add the parent directory to the system path\n",
        "sys.path.append(os.path.dirname(cwd))\n",
        "from scripts import utils\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9CKj0M9jVM"
      },
      "source": [
        "# 1. Build a synthetic dataset for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoEgiDg9jVM"
      },
      "source": [
        "### 1.1. Prepare source documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Drop REFERENCES onward, respecting previous discoveries that these just introduce noise to retrieval.  Must process new docs the same way!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/get_vector_store.py\n",
        "\n",
        "import os\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "from pathlib import Path\n",
        "FILES_PATH = Path('../data/pdfs_ws_mrkp_test/pdfs')\n",
        "FILES = list(FILES_PATH.glob('*.pdf'))\n",
        "FILES[0], len(FILES)\n",
        "\n",
        "reader = PdfReader(os.path.expanduser(FILES[0]))\n",
        "pages = reader.pages\n",
        "documents = []\n",
        "for page in pages:\n",
        "  documents.append(page.extract_text())\n",
        "\n",
        "\n",
        "def load_pdf_to_string(pdf_path):\n",
        "    # Open the PDF file in binary mode\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        # Create a PDF file reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Initialize an empty string to hold the text\n",
        "        text = ''\n",
        "\n",
        "        # Loop through each page and extract the text\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            references_index= page_text.upper().find('\\nREFERENCES\\n')\n",
        "            if references_index != -1:\n",
        "              page_text = page_text[:references_index]\n",
        "              text += page_text\n",
        "              return text\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Use the function to load a PDF into a string\n",
        "text = load_pdf_to_string(os.path.expanduser(FILES[0]))\n",
        "def get_title(pdf_path): return os.path.expanduser(pdf_path).split('/')[-1]\n",
        "\n",
        "all_docs_and_titles = [(load_pdf_to_string(os.path.expanduser(pdf_path)),get_title(pdf_path)) for pdf_path in FILES]\n",
        "\n",
        "all_docs = [doc[0] for doc in all_docs_and_titles]\n",
        "all_titles = [doc[1] for doc in all_docs_and_titles]\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document \n",
        "\n",
        "\n",
        "CHUNK_SIZE = 2000 #try 2000 next\n",
        "CHUNK_OVERLAP = 200 #try 200 next\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap = CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "docs_processed  = [text_splitter.split_documents([Document(page_content=doc, metadata={'filename':FILES[idx].name,'title':utils.get_title(FILES[idx].name.split('_')[0],use_logging=False)})]) \n",
        "         for idx,doc in enumerate(all_docs)]\n",
        "\n",
        "\n",
        "# docs_processed = []\n",
        "# for i in range(len(FILES)):\n",
        "#     doc = text_splitter.create_documents([FILES[i].read_text()],metadatas=[{'filename':FILES[i].name,'title':utils.get_title(FILES[i].name.split('_')[0])}])\n",
        "#     docs_processed.extend(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_docs_from_pdf(files):\n",
        "    all_docs = [load_pdf_to_string(os.path.expanduser(pdf_path)) for  pdf_path in files]\n",
        "    docs_processed  = [text_splitter.split_documents([Document(page_content=doc, metadata={'filename':files[idx].name,'title':utils.get_title(files[idx].name.split('_')[0],use_logging=False)})]) \n",
        "            for idx,doc in enumerate(all_docs)]\n",
        "    docs_processed = [txt for doc in docs_processed for txt in doc]\n",
        "    return docs_processed\n",
        "docs_processed = get_docs_from_pdf(FILES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(docs_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([d.metadata['title'] for d in docs_processed],columns=['title']).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Make embeddings and save to vector store\n",
        "- Currently taking from Part3_Metadata+ArXivExplore_single_source nb, should tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "#embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embed_model_id = 'mixedbread-ai/mxbai-embed-large-v1'\n",
        "\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "# for index, pdf in enumerate(docs_processed):\n",
        "#    pdf.metadata['vs_index'] = index\n",
        "#    content = docs_processed[index]\n",
        "#    if index == 0:\n",
        "#        vector_store = FAISS.from_documents([content], embedder)\n",
        "#    else:\n",
        "#       vector_store_i = FAISS.from_documents([content], embedder)\n",
        "#       vector_store.merge_from(vector_store_i)\n",
        "\n",
        "# vector_store\n",
        "docs = docs_processed\n",
        "for i, doc in enumerate(docs):\n",
        "        doc.metadata['vs_index'] = i\n",
        "        if i == 0:\n",
        "            vector_store = FAISS.from_documents([doc], embedder)\n",
        "        else:\n",
        "            vector_store_i = FAISS.from_documents([doc], embedder)\n",
        "            vector_store.merge_from(vector_store_i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vs_indices = [doc.metadata['vs_index'] for doc in docs_processed]\n",
        "for page in docs_processed:\n",
        "  print(page.page_content)\n",
        "  print(page.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO: Once have the pipeline working and have a baseline, look into https://huggingface.co/spaces/mteb/leaderboard.  SFR-Embedding-Mistral or something along those lines may work much better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.save_local('../data/rag_index_dir')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjrNhcCh9jVN"
      },
      "source": [
        "### 1.2. Setup agents for question generation\n",
        "\n",
        "- HF used [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard).  Used Mixtral-8x7b-4bit exl2, and it did not appear significantly better than Mistral, so using Mistral for speed but may come back to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO: Tried Mixtral4Bit, visually perhaps a bit better, but overfits on 'deep question'. Perhaps return to this after looking at embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "generator_config = ExLlamaV2Config()\n",
        "generator_config.model_dir = \"../../MiStralInference\"\n",
        "generator_config.prepare()\n",
        "\n",
        "generator_model = ExLlamaV2(generator_config)\n",
        "cache = ExLlamaV2Cache(generator_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "generator_model.load_autosplit(cache)\n",
        "\n",
        "generator_tokenizer = ExLlamaV2Tokenizer(generator_config)\n",
        "generator_llm = ExLlamaV2StreamingGenerator(generator_model, cache, generator_tokenizer)\n",
        "generator_llm.set_stop_conditions([generator_tokenizer.eos_token_id])\n",
        "generator_settings = ExLlamaV2Sampler.Settings()\n",
        "generator_settings.temperature = 0.85\n",
        "generator_settings.top_k = 50\n",
        "generator_settings.top_p = 0.8\n",
        "generator_settings.token_repetition_penalty = 1.01\n",
        "#generator_settings.disallow_tokens(generator_tokenizer, [generator_tokenizer.eos_token_id])\n",
        "# see if commenting out the above solved the endless generation issue (did not have with stream generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Working except eos\n",
        "from transformers import Pipeline\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "import time\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def call_llm(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "    tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "    max_new_tokens = max_new_tokens\n",
        "\n",
        "    generator.warmup()\n",
        "    output = generator.generate_simple(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens, seed = 1234)\n",
        "    return output\n",
        "\n",
        "\n",
        "call_llm(question=\"How can I get my cat to like me?\", generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,max_new_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
        "Your deep question should be unambigiously answerable from the context.\n",
        "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
        "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Deep question: (your deep question)\n",
        "Answer: (your answer to the deep question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFc-lVy9jVN"
      },
      "source": [
        "Now let's generate our QA couples.\n",
        "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
        "\n",
        "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fteqDDD9jVN"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "N_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    # Generate QA couple\n",
        "    # output_QA_couple = call_llm(\n",
        "    #     llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    # )\n",
        "    output_QA_couple = call_llm(question=QA_generation_prompt.format(context=sampled_context.page_content), generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    try:\n",
        "        question = output_QA_couple.split(\"Deep question: \")[-1].split(\"Answer: \")[0]\n",
        "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
        "        #assert len(answer) < 300, \"Answer is too long\"\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"context\": sampled_context.page_content,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"source_doc\": sampled_context.metadata[\"title\"],\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUlOUDv59jVN",
        "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',800)\n",
        "display(pd.DataFrame(outputs).head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KG4dNtg9jVN"
      },
      "source": [
        "### 1.3. Setup critique agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(question=question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "        \"relevance\": call_llm(question=question_relevance_critique_prompt.format(question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "                    \n",
        "        \"standalone\": call_llm(question=question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    }\n",
        "    try:\n",
        "        for criterion, evaluation in evaluations.items():\n",
        "            score, eval = (\n",
        "                # int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                (evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "            )\n",
        "            output.update(\n",
        "                {\n",
        "                    f\"{criterion}_score\": score,\n",
        "                    f\"{criterion}_eval\": eval,\n",
        "                }\n",
        "            )\n",
        "    except Exception as e:\n",
        "        #print(\"\\033[91m\" + f\"EVALUATION:\" + \"\\033[0m\")\n",
        "        #print(evaluations)\n",
        "        #print(\"\\033[91m\" + f\"EXCEPTION: {e}\" + \"\\033[0m\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQv36Y_f9jVO"
      },
      "source": [
        "Now let us filter out bad questions based on our critique agent scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "generated_questions['groundedness_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['relevance_score']=generated_questions['relevance_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['standalone_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_questions.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "generated_questions.to_csv(\"../data/pdfs_ws_mrkp_test/generated_questions_pdf_raw.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "for col in [\"groundedness_score\", \"relevance_score\", \"standalone_score\"]:\n",
        "    generated_questions[col] = generated_questions[col].fillna(generated_questions[[\"groundedness_score\", \"relevance_score\", \"standalone_score\"]].min(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBWuOu1b9jVO",
        "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
      },
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/critique_qa.py\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 3.0)\n",
        "    & (generated_questions[\"relevance_score\"] >= 3.0)\n",
        "    & (generated_questions[\"standalone_score\"] >= 3.0)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%#%writefile -a ../scripts/critique_qa.py\n",
        "generated_questions.to_csv(\"../data/pdfs_ws_mrkp_test/generated_questions_pdf_filtered.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaOMZyu69jVO"
      },
      "source": [
        "- Go through the 181 rows remaining post dropping missing vals and missing value imputation visually, keep the better 120ish questions\n",
        "    - Dropped questions that were off-target for learning about LLMs, relied on the reference section, or mentioned the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#eval_dataset = pd.read_csv(\"../data/generated_questions_filtered_w_human.csv\")\n",
        "eval_dataset = generated_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5s19uTd9jVO"
      },
      "source": [
        "# 2. Build our RAG System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mET8Dy9jVO"
      },
      "source": [
        "### 2.1. Preprocessing documents to build our vector database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzBYfNG79jVO"
      },
      "source": [
        "### 2.2. Retriever - embeddings 🗂️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6y1mQJX9jVO"
      },
      "source": [
        "### 2.3. Reader - LLM 💬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "#embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embed_model_id = 'mixedbread-ai/mxbai-embed-large-v1'\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "vector_store = FAISS.load_local('../data/rag_index_dir', embedder,allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PdpuWyP9jVP"
      },
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "reader_config = ExLlamaV2Config()\n",
        "reader_config.model_dir = \"../../ZephyrInference\"\n",
        "reader_config.prepare()\n",
        "\n",
        "reader_model = ExLlamaV2(reader_config)\n",
        "cache = ExLlamaV2Cache(reader_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "reader_model.load_autosplit(cache)\n",
        "\n",
        "reader_tokenizer = ExLlamaV2Tokenizer(reader_config)\n",
        "reader_llm = ExLlamaV2StreamingGenerator(reader_model, cache, reader_tokenizer)\n",
        "#reader_llm.set_stop_conditions([reader_tokenizer.eos_token_id])\n",
        "reader_settings = ExLlamaV2Sampler.Settings()\n",
        "reader_settings.temperature = 0.85\n",
        "reader_settings.top_k = 30\n",
        "reader_settings.top_p = 0.8\n",
        "reader_settings.token_repetition_penalty = 1.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "   # tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512,\n",
        "    knowledge_index: FAISS = vector_store,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 10, #30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    embedding_vector = core_embeddings_model.embed_query(question)\n",
        "    relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    #Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = RERANKER.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        #print(f\"Type is : {type(relevant_docs[0])}\")\n",
        "        print(dir(relevant_docs[0]))\n",
        "        print(relevant_docs[0])\n",
        "        relevant_docs = [doc['content'] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "    print(f'Len of relevant_docs: {len(relevant_docs)}')\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "    generator.warmup()\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = generator.generate_simple(final_prompt, \n",
        "    settings, max_new_tokens, seed = 1234)\n",
        "    # print(answer)\n",
        "    return answer,relevant_docs\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512,reranker =True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_end_idx=answer.find('<|assistant|>')\n",
        "answer[prompt_end_idx+14:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- OK, Zephyr seems to work well, under 4s/question with exl2.  Will try to setup reranker, then onto generating questions and relevant docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/answer_w_rag_for_eval.py\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "   # tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512,\n",
        "    knowledge_index: FAISS = vector_store,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 10, #30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    embedding_vector = core_embeddings_model.embed_query(question)\n",
        "    relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_retrieved_docs]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "    generator.warmup()\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = generator.generate_simple(final_prompt, \n",
        "    settings, max_new_tokens, seed = 1234)\n",
        "    return answer,relevant_docs\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512,reranker = RERANKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiygbqfT9jVP"
      },
      "source": [
        "# 3. Benchmarking the RAG system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'kaist-ai/prometheus-13b-v1.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.vectorstores import VectorStore\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#eval_dataset = pd.read_csv(\"../data/generated_questions_filtered_w_human.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrlMh_ZI9jVP"
      },
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "from collections import namedtuple\n",
        "def run_rag_tests(\n",
        "    dataset: pd.DataFrame,\n",
        "    llm: ExLlamaV2StreamingGenerator,\n",
        "    knowledge_index: VectorStore,\n",
        "    #output_file: str,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    verbose: Optional[bool] = False,\n",
        "    test_settings: Optional[str] = None,  # To document the test settings used\n",
        "):\n",
        "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
        "\n",
        "    dataset_copy = dataset.copy(deep=True)\n",
        "    dataset_copy['retrieved_docs'] = None\n",
        "    for example_row in tqdm(dataset_copy.iterrows()):\n",
        "        index, example = example_row\n",
        "        question = example[\"question\"]\n",
        "        if dataset_copy.loc[index,'retrieved_docs']: #already retrieved\n",
        "            print(f\"Continue for {index} since already processed\")\n",
        "            continue\n",
        "\n",
        "        generated_answer, relevant_docs =  answer_with_rag(question, knowledge_index=knowledge_index, generator=llm,settings=reader_settings,max_new_tokens=512,reranker = reranker)\n",
        "        if verbose:\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'True answer: {example[\"answer\"]}')\n",
        "        dataset_copy.at[index,'retrieved_docs'] = relevant_docs\n",
        "        dataset_copy.loc[index,'true_answer'] = dataset_copy.loc[index,'answer']\n",
        "        dataset_copy.loc[index,'generated_answer'] = generated_answer\n",
        "\n",
        "\n",
        "        if test_settings:\n",
        "            dataset_copy[\"test_settings\"] = test_settings\n",
        "    return dataset_copy #INDENTED ON PURPOSE, TEST RUN!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "ds_rag = run_rag_tests(eval_dataset,reader_llm,vector_store,reranker = None,test_settings='MistralQs-all_MiniLM_L6_v2Embed-ZephyrRead-2000x200chunks-NoRerank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_rag.to_csv(\"../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ds_rag.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_rag = pd.read_csv(\"../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "\n",
        "judge_config = ExLlamaV2Config()\n",
        "judge_config.model_dir = \"../PrometheusEval\"\n",
        "#judge_config.model_dir = '../Mixtral4bit'\n",
        "judge_config.prepare()\n",
        "\n",
        "judge_model = ExLlamaV2(judge_config)\n",
        "cache = ExLlamaV2Cache(judge_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "judge_model.load_autosplit(cache)\n",
        "\n",
        "judge_tokenizer = ExLlamaV2Tokenizer(judge_config)\n",
        "judge_llm = ExLlamaV2StreamingGenerator(judge_model, cache, judge_tokenizer)\n",
        "#judge_llm.set_stop_conditions([judge_tokenizer.eos_token_id])\n",
        "judge_settings = ExLlamaV2Sampler.Settings()\n",
        "judge_settings.temperature = 1.0\n",
        "# judge_settings.top_k = 30\n",
        "# judge_settings.top_p = 0.8\n",
        "# judge_settings.token_repetition_penalty = 1.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae-3KWzK9jVP"
      },
      "outputs": [],
      "source": [
        "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
        "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "\n",
        "###The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "###Response to evaluate:\n",
        "{response}\n",
        "\n",
        "###Reference Answer (Score 5):\n",
        "{reference_answer}\n",
        "\n",
        "###Score Rubrics:\n",
        "[Is the response correct, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\n",
        "###Feedback:\"\"\"\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "\n",
        "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
        "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia9Mvn859jVP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def evaluate_answers(\n",
        "    answer_path: str,\n",
        "    eval_chat_model:ExLlamaV2StreamingGenerator,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    evaluation_prompt: str\n",
        ") -> None:\n",
        "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
        "    answers = []\n",
        "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
        "        answers = pd.read_csv(answer_path)\n",
        "    for example_row in tqdm(answers.iterrows()):\n",
        "        index, example = example_row\n",
        "        if f\"eval_score\" in example:\n",
        "            continue\n",
        "\n",
        "        eval_prompt = evaluation_prompt.format(\n",
        "            instruction=example[\"question\"],\n",
        "            response=example[\"generated_answer\"],\n",
        "            reference_answer=example[\"true_answer\"],\n",
        "        )\n",
        "\n",
        "        eval_chat_model.warmup()\n",
        "        \n",
        "        eval_result = eval_chat_model.generate_simple(eval_prompt, \n",
        "        settings, num_tokens=1024, seed = 1234) #max_new_tokens=1024,\n",
        "        feedback = re.search(r'###Feedback:\\s*(.*)',eval_result,re.DOTALL).group(1)\n",
        "        try:\n",
        "            #score = re.search(r'(\\d+)', feedback).group(1)\n",
        "            score = re.search(r'overall score is (\\d)', feedback).group(1)\n",
        "        except AttributeError:\n",
        "            score = 'NaN'\n",
        "        answers.loc[index,f\"eval_score\"] = score\n",
        "        answers.loc[index,f\"eval_feedback\"] = feedback\n",
        "        print(f'Score: {score}')\n",
        "        print(f'Feedback: {feedback}')\n",
        "    return answers #INDENTED ON PURPOSE, TEST RUN!\n",
        "        # with open(answer_path, \"w\") as f:\n",
        "        #     json.dump(answers, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "# temp=evaluate_answers(answer_path='../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank.csv',\n",
        "#                  eval_chat_model=judge_llm,settings=judge_settings,evaluation_prompt=EVALUATION_PROMPT) # SHOULD BE evaluation_prompt_template !!!\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "temp=evaluate_answers(answer_path='../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank.csv',\n",
        "                 eval_chat_model=judge_llm,settings=judge_settings,evaluation_prompt=evaluation_prompt_template) # SHOULD BE evaluation_prompt_template !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "temp.to_csv(\"../data/pdfs_ws_mrkp_test/MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank-Evaluated.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile -a ../scripts/run_rag_tests.py\n",
        "import matplotlib.pyplot as plt\n",
        "temp.eval_score.sort_values().hist()\n",
        "plt.title(\"Pdf-MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank\");\n",
        "plt.savefig('../data/pdfs_ws_mrkp_test/Pdf-MistralQs-mxbai_embed-ZephyrRead-2000x200chunks-NoRerank-Evaluated.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW2nnvUT9jVQ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./output\"):\n",
        "    os.mkdir(\"./output\")\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [True, False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(f\"Running evaluation for {settings_name}:\")\n",
        "\n",
        "            print(\"Loading knowledge base embeddings...\")\n",
        "            knowledge_index = load_embeddings(\n",
        "                RAW_KNOWLEDGE_BASE,\n",
        "                chunk_size=chunk_size,\n",
        "                embedding_model_name=embeddings,\n",
        "            )\n",
        "\n",
        "            print(\"Running RAG...\")\n",
        "            reranker = (\n",
        "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "                if rerank\n",
        "                else None\n",
        "            )\n",
        "            run_rag_tests(\n",
        "                eval_dataset=eval_dataset,\n",
        "                llm=READER_LLM,\n",
        "                knowledge_index=knowledge_index,\n",
        "                output_file=output_file_name,\n",
        "                reranker=reranker,\n",
        "                verbose=False,\n",
        "                test_settings=settings_name,\n",
        "            )\n",
        "\n",
        "            print(\"Running evaluation...\")\n",
        "            evaluate_answers(\n",
        "                output_file_name,\n",
        "                eval_chat_model,\n",
        "                evaluator_name,\n",
        "                evaluation_prompt_template,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tytXV5-h9jVT"
      },
      "source": [
        "### Inspect results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "#outputs = []\n",
        "#RagOverArXiv/data/pdfs_ws_mrkp_test/eval_outputs\n",
        "pdf_files = glob.glob(\"../data/pdfs_ws_mrkp_test/eval_outputs/pdf*.csv\")\n",
        "txt_files = glob.glob(\"../data/pdfs_ws_mrkp_test/eval_outputs/txt*.csv\")\n",
        "all_files = pdf_files+txt_files\n",
        "result = pd.DataFrame()#index=list(all_files.index))\n",
        "for file_path in all_files:\n",
        "    print(file_path)\n",
        "    output = pd.read_csv(file_path)\n",
        "    print(output.columns)\n",
        "    output[\"settings\"] = file_path.split('/')[-1]\n",
        "    result = pd.concat([result,output])\n",
        "print(result.shape)\n",
        "pd.set_option('display.max_colwidth',500)\n",
        "result[['eval_score','eval_feedback']].head(20)\n",
        "#result.eval_score.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(result['eval_score'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result.eval_score.isna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdkXMNvS9jVT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "result[\"eval_score\"] = result[\"eval_score\"].apply(\n",
        "    lambda x: x if not pd.isna(x) else 1\n",
        ")\n",
        "result[\"eval_score\"] = (result[\"eval_score\"] - 1) / 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result[['eval_score','eval_feedback']].head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgxBpid29jVT",
        "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
      },
      "outputs": [],
      "source": [
        "average_scores = result.groupby(\"settings\")[\"eval_score\"].mean()\n",
        "average_scores.sort_values()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
