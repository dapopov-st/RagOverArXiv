{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dapopov-st/RagOverArXiv/blob/main/GenerateQaDatasetWithT5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bdd511-4fc0-4bbd-9f4b-df76bbcb756a",
      "metadata": {
        "id": "e1bdd511-4fc0-4bbd-9f4b-df76bbcb756a"
      },
      "source": [
        "# Generate QA Pairs with T5 for RAG Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmGuzVZDv5vM",
        "outputId": "2300bd4a-1ae1-4c26-8fa6-80f53acaa468"
      },
      "id": "cmGuzVZDv5vM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.9.15-py3-none-any.whl (964 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.2/964.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.2 (from llama-index)\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from llama-index)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Collecting httpx (from llama-index)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Collecting openai>=1.1.0 (from llama-index)\n",
            "  Downloading openai-1.3.9-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-protobuf<5.0.0.0,>=4.24.0.4 (from llama-index)\n",
            "  Downloading types_protobuf-4.24.0.4-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx->llama-index)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (3.6)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Installing collected packages: types-protobuf, mypy-extensions, marshmallow, h11, deprecated, beautifulsoup4, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-index\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beautifulsoup4-4.12.2 dataclasses-json-0.6.3 deprecated-1.2.14 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 llama-index-0.9.15 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.3.9 tiktoken-0.5.2 types-protobuf-4.24.0.4 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.35.2 accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7 datasets==2.10.1 wandb==0.16.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRh79kpLwTMv",
        "outputId": "e68a6229-a7d3-4cc4-c108-969993a46164"
      },
      "id": "RRh79kpLwTMv",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/244.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ctransformers[cuda]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pogj-xqyCvQ",
        "outputId": "c7118652-65c6-4886-c336-09250b1083c6"
      },
      "id": "8Pogj-xqyCvQ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ctransformers[cuda]\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from ctransformers[cuda]) (0.19.4)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers[cuda]) (9.0.0)\n",
            "Collecting nvidia-cublas-cu12 (from ctransformers[cuda])\n",
            "  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12 (from ctransformers[cuda])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (2023.11.17)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu12, nvidia-cublas-cu12, ctransformers\n",
            "Successfully installed ctransformers-0.2.27 nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-runtime-cu12-12.3.101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQoibKpo9zCK",
        "outputId": "e29be8cb-b8ce-4e11-bc10-9895b98a3010"
      },
      "id": "XQoibKpo9zCK",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.350-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain)\n",
            "  Downloading langchain_core-0.1.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.70-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: jsonpointer, langsmith, jsonpatch, langchain-core, langchain-community, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.350 langchain-community-0.0.3 langchain-core-0.1.0 langsmith-0.0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lmqg"
      ],
      "metadata": {
        "id": "EiXcGsIDTZPX"
      },
      "id": "EiXcGsIDTZPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/PdfRag/finetune_output_dir'\n",
        "logging_dir = '/content/drive/MyDrive/PdfRag/finetune_logging_dir'\n",
        "index_dir = '/content/drive/MyDrive/PdfRag/finetune_index_dir'\n",
        "papers_dir = '/content/drive/MyDrive/PdfRag/clusterofstars'\n",
        "data_dir = '/content/drive/MyDrive/PdfRag/data'\n",
        "%cd /content/drive/MyDrive/PdfRag\n",
        "!ls ./clusterofstars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KppvKd3384q",
        "outputId": "ef32aaec-6409-457f-e831-2ff4e33b5b29"
      },
      "id": "_KppvKd3384q",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/PdfRag\n",
            "'Atlas:  Few-shot Learning with Retrieval Augmented Language Models.pdf'\n",
            " ChainOfThought\n",
            "'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'\n",
            "'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'\n",
            "'DSPy:  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'\n",
            "'Evaluating Large Language Models Trained on Code.pdf'\n",
            " ExcludeSurveysAndLitReviews\n",
            "'FlashAttention:  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'\n",
            "'From Sparse to Dense:  GPT-4 Summarization with Chain of Density Prompting.pdf'\n",
            "'Graph of Thoughts: Solving Elaborate Problems with Large Language Models.pdf'\n",
            "'In-Context Retrieval-Augmented Language Models.pdf'\n",
            "'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf'\n",
            "'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.pdf'\n",
            "'LoRA: Low-Rank Adaptation of Large Language Models.pdf'\n",
            " Mistral7B.pdf\n",
            "'Plan-and-Solve Prompting:  Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.pdf'\n",
            "'Program of Thoughts Prompting:  Disentangling Computation from Reasoning for Numerical Reasoning Tasks.pdf'\n",
            "'QLORA: Efficient Finetuning of Quantized LLMs.pdf'\n",
            "'ReAct: Synergizing Reasoning and Acting in Language Models.pdf'\n",
            "'Self-Rag: Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'\n",
            "'SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking.pdf'\n",
            "'Show Your Work:  Scratchpads for Intermediate Computation with Language Models.pdf'\n",
            "'Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding.pdf'\n",
            "'TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'\n",
            "'Toolformer: Language Models Can Teach Themselves to Use Tools.pdf'\n",
            "'Training language models to follow instructions with human feedback.pdf'\n",
            "'Tree of Thoughts:  Deliberate Problem Solving with Large Language Models.pdf'\n",
            "'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,gc,traceback\n",
        "import torch\n",
        "def clean_ipython_hist():\n",
        "    # Code in this function mainly copied from IPython source\n",
        "    if not 'get_ipython' in globals(): return\n",
        "    ip = get_ipython()\n",
        "    user_ns = ip.user_ns\n",
        "    ip.displayhook.flush()\n",
        "    pc = ip.displayhook.prompt_count + 1\n",
        "    for n in range(1, pc): user_ns.pop('_i'+repr(n),None)\n",
        "    user_ns.update(dict(_i='',_ii='',_iii=''))\n",
        "    hm = ip.history_manager\n",
        "    hm.input_hist_parsed[:] = [''] * pc\n",
        "    hm.input_hist_raw[:] = [''] * pc\n",
        "    hm._i = hm._ii = hm._iii = hm._i00 =  ''\n",
        "\n",
        "\n",
        "\n",
        "def clean_tb():\n",
        "    # h/t Piotr Czapla\n",
        "    if hasattr(sys, 'last_traceback'):\n",
        "        traceback.clear_frames(sys.last_traceback)\n",
        "        delattr(sys, 'last_traceback')\n",
        "    if hasattr(sys, 'last_type'): delattr(sys, 'last_type')\n",
        "    if hasattr(sys, 'last_value'): delattr(sys, 'last_value')\n",
        "\n",
        "def clean_mem():\n",
        "    clean_tb()\n",
        "    clean_ipython_hist()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "clean_mem()"
      ],
      "metadata": {
        "id": "jtJPTh9imNFQ"
      },
      "id": "jtJPTh9imNFQ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "20iIlWKdxb2N"
      },
      "id": "20iIlWKdxb2N",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_PATH = \"TheBloke/Amethyst-13B-Mistral-GGUF\"\n",
        "# MODEL_FILE = 'amethyst-13b-mistral.Q6_K.gguf'\n",
        "MODEL_PATH = \"FPHam/Reverso_Expanded_13b_Q_Generator_GPTQ\"\n",
        "#MODEL_FILE = 'amethyst-13b-mistral.Q6_K.gguf'"
      ],
      "metadata": {
        "id": "MAbcSgmz9_Th"
      },
      "id": "MAbcSgmz9_Th",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ctransformers[gptq]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-__pBhP-guV",
        "outputId": "a61b8d25-3a71-4510-93d3-97f5e2c813b2"
      },
      "id": "F-__pBhP-guV",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ctransformers[gptq] in /usr/local/lib/python3.10/dist-packages (0.2.27)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from ctransformers[gptq]) (0.19.4)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers[gptq]) (9.0.0)\n",
            "Collecting exllama==0.1.0 (from ctransformers[gptq])\n",
            "  Downloading exllama-0.1.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja==1.11.1 (from exllama==0.1.0->ctransformers[gptq])\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors==0.3.1 (from exllama==0.1.0->ctransformers[gptq])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece>=0.1.97 (from exllama==0.1.0->ctransformers[gptq])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from exllama==0.1.0->ctransformers[gptq]) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[gptq]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[gptq]) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[gptq]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[gptq]) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[gptq]) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[gptq]) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[gptq]) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->exllama==0.1.0->ctransformers[gptq]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->exllama==0.1.0->ctransformers[gptq]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->exllama==0.1.0->ctransformers[gptq]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->exllama==0.1.0->ctransformers[gptq]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[gptq]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[gptq]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[gptq]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[gptq]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->exllama==0.1.0->ctransformers[gptq]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->exllama==0.1.0->ctransformers[gptq]) (1.3.0)\n",
            "Installing collected packages: sentencepiece, safetensors, ninja, exllama\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.4.1\n",
            "    Uninstalling safetensors-0.4.1:\n",
            "      Successfully uninstalled safetensors-0.4.1\n",
            "Successfully installed exllama-0.1.0 ninja-1.11.1 safetensors-0.3.1 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c262e939-9eef-421e-8a94-c1d8a6cf861d",
      "metadata": {
        "tags": [],
        "id": "c262e939-9eef-421e-8a94-c1d8a6cf861d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from llama_index import SimpleDirectoryReader\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.schema import MetadataMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "32eaaf6b-e3dc-4838-aaf5-f1d6305e2426",
      "metadata": {
        "id": "32eaaf6b-e3dc-4838-aaf5-f1d6305e2426"
      },
      "outputs": [],
      "source": [
        "TRAIN_FILES = ['./clusterofstars/LoRA: Low-Rank Adaptation of Large Language Models.pdf']\n",
        "VAL_FILES = ['./clusterofstars/QLORA: Efficient Finetuning of Quantized LLMs.pdf']\n",
        "\n",
        "TRAIN_CORPUS_FPATH = './data/train_corpus.json'\n",
        "VAL_CORPUS_FPATH = './data/val_corpus.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMPJn5ZDSj9t",
        "outputId": "a0b52d47-11ba-46d6-b10d-be21c1b7e07f"
      },
      "id": "MMPJn5ZDSj9t",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import re\n",
        "import pandas as pd\n",
        "text = extract_text(TRAIN_FILES[0])"
      ],
      "metadata": {
        "id": "r_VeYXE9tJGd"
      },
      "id": "r_VeYXE9tJGd",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ORIGINAL from docs\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.converter import PDFResourceManager, PDFPageAggregator\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.layout import LTTextBoxHorizontal\n",
        "document = open(TRAIN_FILES[0], 'rb')\n",
        "#Create resource manager\n",
        "rsrcmgr = PDFResourceManager()\n",
        "# Set parameters for analysis.\n",
        "laparams = LAParams()\n",
        "# Create a PDF page aggregator object.\n",
        "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
        "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "for page in PDFPage.get_pages(document):\n",
        "  interpreter.process_page(page)\n",
        "  # receive the LTPage object for the page.\n",
        "  layout = device.get_result()\n",
        "  for element in layout:\n",
        "    if isinstance(element, LTTextBoxHorizontal)\n",
        "      print(element.get_text())"
      ],
      "metadata": {
        "id": "PxmaB2Nq3a4e"
      },
      "id": "PxmaB2Nq3a4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mostly_numbers(text, threshold=0.5):\n",
        "    # Find all numeric characters in the text\n",
        "    numbers = re.findall(r'[\\d()±/. ]', text)\n",
        "    percentage = len(numbers) / len(text) if text else 1\n",
        "    return percentage >= threshold\n",
        "def starts_with_figure_or_table(text):\n",
        "  pattern = r\"^(Figure|Table)\\s+\\d+\\s*:\"\n",
        "  return bool(re.match(pattern, text))\n",
        "\n",
        "def ends_with_period_or_number(text):\n",
        "    \"\"\"...or dash\"\"\"\n",
        "    pattern = r\"[.\\d-]$\"\n",
        "    return bool(re.search(pattern, text))\n",
        "\n",
        "def join_hyphenated_words(text):\n",
        "    pattern = r\"([a-z]+)-\\n\\s*([a-z]+)\"\n",
        "    return re.sub(pattern, r'\\1\\2', text)\n",
        "\n",
        "def ignore_footnote(text):\n",
        "    \"Works OK, just want to trim what's carried over on the next line in footnote\"\n",
        "    pattern = r\"\\n\\d[A-Z][^.]*\\.\"\n",
        "    return bool(re.search(pattern, text))\n",
        "\n",
        "\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.converter import PDFResourceManager, PDFPageAggregator\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.layout import LTTextBoxHorizontal\n",
        "def parse_paragraphs():\n",
        "  df = pd.DataFrame(columns=['abstract'])\n",
        "  document = open(TRAIN_FILES[0], 'rb')\n",
        "  #Create resource manager\n",
        "  rsrcmgr = PDFResourceManager()\n",
        "  # Set parameters for analysis.\n",
        "  laparams = LAParams()\n",
        "  # Create a PDF page aggregator object.\n",
        "  device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
        "  interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "  SEEN_ABSTRACT = False\n",
        "  buffer = ''\n",
        "  for page in PDFPage.get_pages(document):\n",
        "    interpreter.process_page(page)\n",
        "    # receive the LTPage object for the page.\n",
        "    layout = device.get_result()\n",
        "\n",
        "    for element in layout:\n",
        "      if isinstance(element, LTTextBoxHorizontal):\n",
        "        text = element.get_text()\n",
        "        text = join_hyphenated_words(text)\n",
        "\n",
        "        if \"REFERENCES\\n\" in text.upper():return df\n",
        "\n",
        "        if not SEEN_ABSTRACT and 'ABSTRACT' not in text.upper(): continue\n",
        "        else : SEEN_ABSTRACT = True\n",
        "\n",
        "        if (mostly_numbers(text) or starts_with_figure_or_table(text) or text.startswith('*')\n",
        "            or ignore_footnote(text)):\n",
        "          #print(f\"<<<WiLL IGNORE: {text}>>>\")\n",
        "          continue\n",
        "        elif (text[-1] == '-' or text[-1]==':' or len(text)<10\n",
        "              or not ends_with_period_or_number(text)):\n",
        "          buffer += text+\" \"\n",
        "        else:\n",
        "          if buffer:# and not ignore_footnote(text):\n",
        "            text = buffer + \" \"+ text\n",
        "            buffer = ''\n",
        "          if len(df) and df['abstract'].iloc[-1].endswith('-'): df['abstract'].iloc[-1] += text\n",
        "          elif len(text)>50: df = df.append({'abstract':text.strip()}, ignore_index=True)\n",
        "  return df\n",
        "df=parse_paragraphs()"
      ],
      "metadata": {
        "id": "A1LqfXuDHY2Y"
      },
      "id": "A1LqfXuDHY2Y",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3582
        },
        "id": "Lqho-oC6Mi6a",
        "outputId": "029b403e-b60f-43cc-8195-b4236562ec55"
      },
      "id": "Lqho-oC6Mi6a",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                   abstract\n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...\n",
              "1   INTRODUCTION\\n  Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done v...\n",
              "2   Many sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciﬁc parameters in ad...\n",
              "3   often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\\ndepth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Hambard...\n",
              "4   We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\ncha...\n",
              "..                                                                                                                                                                                                      ...\n",
              "58  To answer these questions, we project W onto the r-dimensional subspace of ∆W by computing U (cid:62)W V (cid:62), with U /V being the left/right singular-vector matrix of ∆W . Then, we compare th...\n",
              "59                                                                                                                   ∆Wq Wq\\n Random ∆Wq Wq\\n Random\\n  ||U (cid:62)WqV (cid:62)||F = 0.32\\n||Wq||F = 61.95\n",
              "60  We draw several conclusions from Table 7. First, ∆W has a stronger correlation with W compared\\nto a random matrix, indicating that ∆W ampliﬁes some features that are already in W . Second,\\ninste...\n",
              "61  8 CONCLUSION AND FUTURE WORK\\n  Fine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\nand the storage/switching cost for hosting independent instances ...\n",
              "62  There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...\n",
              "\n",
              "[63 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53b25d38-4a6a-468e-88db-8aea8b7c8e39\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>INTRODUCTION\\n  Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Many sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciﬁc parameters in ad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\\ndepth or reduce the model’s usable sequence length (Li &amp; Liang, 2021; Lester et al., 2021; Hambard...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\ncha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>To answer these questions, we project W onto the r-dimensional subspace of ∆W by computing U (cid:62)W V (cid:62), with U /V being the left/right singular-vector matrix of ∆W . Then, we compare th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>∆Wq Wq\\n Random ∆Wq Wq\\n Random\\n  ||U (cid:62)WqV (cid:62)||F = 0.32\\n||Wq||F = 61.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>We draw several conclusions from Table 7. First, ∆W has a stronger correlation with W compared\\nto a random matrix, indicating that ∆W ampliﬁes some features that are already in W . Second,\\ninste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>8 CONCLUSION AND FUTURE WORK\\n  Fine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\nand the storage/switching cost for hosting independent instances ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53b25d38-4a6a-468e-88db-8aea8b7c8e39')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-53b25d38-4a6a-468e-88db-8aea8b7c8e39 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-53b25d38-4a6a-468e-88db-8aea8b7c8e39');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6f49508a-7fc0-470e-845b-da289f2514b3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f49508a-7fc0-470e-845b-da289f2514b3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6f49508a-7fc0-470e-845b-da289f2514b3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_955d2a8e-809b-47b1-929f-ac5e66fd73dc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_955d2a8e-809b-47b1-929f-ac5e66fd73dc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kXFP403S9oyR"
      },
      "id": "kXFP403S9oyR"
    },
    {
      "cell_type": "code",
      "source": [
        "from lmqg import TransformersQG\n",
        "MODEL_ID = 'lmqg/t5-base-squad-qag'\n",
        "LLM_QA = TransformersQG(MODEL_ID)"
      ],
      "metadata": {
        "id": "-BBExpWlA3Jq"
      },
      "id": "-BBExpWlA3Jq",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "TOKENIZER_QA = AutoTokenizer.from_pretrained(MODEL_ID)"
      ],
      "metadata": {
        "id": "byohl0S8ZB0r"
      },
      "id": "byohl0S8ZB0r",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_qa_with_sliding_window(text, model, tokenizer, window_size, overlap):\n",
        "    # Tokenize the text into tokens\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Create a sliding window over the tokens\n",
        "    windows = [tokens[i:i+window_size] for i in range(0, len(tokens), window_size-overlap)]\n",
        "\n",
        "    # Apply the model to each window and concatenate the results\n",
        "    results = []\n",
        "    for window in windows:\n",
        "        window_text = tokenizer.convert_tokens_to_string(window)\n",
        "        qa_list = model.generate_qa(window_text)\n",
        "        results.extend(qa_list)\n",
        "    return results\n",
        "\n",
        "# # Initialize the tokenizer\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "# # Define window size and overlap\n",
        "# window_size = 512\n",
        "# overlap = 50\n",
        "\n",
        "# Apply the function to the 'abstract' column\n",
        "df['qa'] = df['abstract'].apply(lambda x: generate_qa_with_sliding_window(x, LLM_QA, TOKENIZER_QA, window_size=500, overlap=50)) #allow room for special tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIIF0DPYeHVq",
        "outputId": "34c73252-7211-416d-c4d1-7aadff77b30e"
      },
      "id": "xIIF0DPYeHVq",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 481.38it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 725.53it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 815.85it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 830.23it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 721.91it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 935.60it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1070.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1077.67it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 948.29it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 701.27it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1109.60it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 793.62it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 874.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 833.53it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 679.02it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 520.00it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1232.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 979.29it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1180.50it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 397.83it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 936.44it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 716.98it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1065.36it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 699.63it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 559.46it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 974.51it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 536.22it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1126.90it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1021.26it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1226.76it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1109.60it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1060.24it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 884.87it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1019.02it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 344.87it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 373.76it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1007.52it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 923.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 657.72it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 839.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 797.85it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 621.29it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 685.90it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 428.69it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 432.36it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 940.85it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 929.59it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 673.24it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1032.83it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 990.86it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1207.69it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 909.04it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1207.34it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 707.66it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1031.56it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 868.93it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1146.92it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 996.98it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 908.05it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1025.75it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 828.59it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 940.01it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 519.80it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 868.39it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 849.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['abstract','qa']].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "xPOpdUUoDbon",
        "outputId": "058d0fe2-c840-4635-852d-281a6aa6e2d0"
      },
      "id": "xPOpdUUoDbon",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                  abstract  \\\n",
              "0  ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "1  INTRODUCTION\\n  Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done v...   \n",
              "2  Many sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciﬁc parameters in ad...   \n",
              "3  often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\\ndepth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Hambard...   \n",
              "4  We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\ncha...   \n",
              "\n",
              "                                                                                                                                                                                                        qa  \n",
              "0  [(What is an important paradigm of natural language processing?, large-scale pretraining on general domain data and adaptation to particular tasks or domains), (What becomes less feasible as we pr...  \n",
              "1  [(What is the major downside of fine-tuning?, the new model contains as many parameters as in the original model), (What is the major downside of fine-tuning?, the new model contains as many param...  \n",
              "2  [(What did many seek to mitigate by adapting only some parameters or learning external modules for new tasks?, adapting only some parameters or learning external modules), (What did this way, we o...  \n",
              "3  [(What does inference latency introduce?, by extending model depth or reduce the model’s usable sequence length), (What is a trade-off between efficiency and model quality?, these method often fai...  \n",
              "4  [(What do we take inspiration from?, Li et al. (2018a); Aghajanyan et al. (2020)), (What do we hypothesize that the change in weights during model adaptation also has?, a low “intrinsic rank”), (W...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4578487d-d1ed-4aed-ac42-f68a0f760d99\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>qa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>[(What is an important paradigm of natural language processing?, large-scale pretraining on general domain data and adaptation to particular tasks or domains), (What becomes less feasible as we pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>INTRODUCTION\\n  Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done v...</td>\n",
              "      <td>[(What is the major downside of fine-tuning?, the new model contains as many parameters as in the original model), (What is the major downside of fine-tuning?, the new model contains as many param...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Many sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciﬁc parameters in ad...</td>\n",
              "      <td>[(What did many seek to mitigate by adapting only some parameters or learning external modules for new tasks?, adapting only some parameters or learning external modules), (What did this way, we o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\\ndepth or reduce the model’s usable sequence length (Li &amp; Liang, 2021; Lester et al., 2021; Hambard...</td>\n",
              "      <td>[(What does inference latency introduce?, by extending model depth or reduce the model’s usable sequence length), (What is a trade-off between efficiency and model quality?, these method often fai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\ncha...</td>\n",
              "      <td>[(What do we take inspiration from?, Li et al. (2018a); Aghajanyan et al. (2020)), (What do we hypothesize that the change in weights during model adaptation also has?, a low “intrinsic rank”), (W...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4578487d-d1ed-4aed-ac42-f68a0f760d99')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4578487d-d1ed-4aed-ac42-f68a0f760d99 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4578487d-d1ed-4aed-ac42-f68a0f760d99');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6513023d-3a3d-47e6-8066-d4b51026ec99\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6513023d-3a3d-47e6-8066-d4b51026ec99')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6513023d-3a3d-47e6-8066-d4b51026ec99 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.explode('qa')\n",
        "df.head(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2026
        },
        "id": "0Dn3Y3LjDFvB",
        "outputId": "ec34d8ee-909f-475b-af2b-d527c4786851"
      },
      "id": "0Dn3Y3LjDFvB",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                   abstract  \\\n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "1   INTRODUCTION\\n  Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done v...   \n",
              "..                                                                                                                                                                                                      ...   \n",
              "22  4.2 APPLYING LORA TO TRANSFORMER\\n  In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architect...   \n",
              "22  4.2 APPLYING LORA TO TRANSFORMER\\n  In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architect...   \n",
              "22  4.2 APPLYING LORA TO TRANSFORMER\\n  In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architect...   \n",
              "22  4.2 APPLYING LORA TO TRANSFORMER\\n  In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architect...   \n",
              "23  Practical Beneﬁts and Limitations. The most signiﬁcant beneﬁt comes from the reduction in\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\nusage by up to ...   \n",
              "\n",
              "                                                                                                                                                               qa  \n",
              "0   (What is an important paradigm of natural language processing?, large-scale pretraining on general domain data and adaptation to particular tasks or domains)  \n",
              "0                                                                                   (What becomes less feasible as we pre-train larger models?, full fine-tuning)  \n",
              "0    (What does LoRA do?, freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture)  \n",
              "0                                                                              (How many times can LoRA reduce the number of trainable parameters?, 10,000 times)  \n",
              "1                                                (What is the major downside of fine-tuning?, the new model contains as many parameters as in the original model)  \n",
              "..                                                                                                                                                            ...  \n",
              "22                                                                                                         (How many weight matrices are in the MLP module?, two)  \n",
              "22                                                                                             (How many weight matrices are in the self-attention module?, four)  \n",
              "22                                                                                         (What do we treat Wq as?, a single matrix of dimension dmodel  dmodel)  \n",
              "22                                                                          (What do we limit our study to?, adapting the attention weights for downstream tasks)  \n",
              "23                                                           (What is the most significant benefit of training with Adam?, reduction in memory and storage usage)  \n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f61ce6f3-f54d-42dc-aaa4-9e40011cac76\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>qa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>(What is an important paradigm of natural language processing?, large-scale pretraining on general domain data and adaptation to particular tasks or domains)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>(What becomes less feasible as we pre-train larger models?, full fine-tuning)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>(What does LoRA do?, freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>(How many times can LoRA reduce the number of trainable parameters?, 10,000 times)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>INTRODUCTION\\n  Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done v...</td>\n",
              "      <td>(What is the major downside of fine-tuning?, the new model contains as many parameters as in the original model)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.2 APPLYING LORA TO TRANSFORMER\\n  In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architect...</td>\n",
              "      <td>(How many weight matrices are in the MLP module?, two)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.2 APPLYING LORA TO TRANSFORMER\\n  In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architect...</td>\n",
              "      <td>(How many weight matrices are in the self-attention module?, four)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.2 APPLYING LORA TO TRANSFORMER\\n  In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architect...</td>\n",
              "      <td>(What do we treat Wq as?, a single matrix of dimension dmodel  dmodel)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.2 APPLYING LORA TO TRANSFORMER\\n  In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architect...</td>\n",
              "      <td>(What do we limit our study to?, adapting the attention weights for downstream tasks)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Practical Beneﬁts and Limitations. The most signiﬁcant beneﬁt comes from the reduction in\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\nusage by up to ...</td>\n",
              "      <td>(What is the most significant benefit of training with Adam?, reduction in memory and storage usage)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f61ce6f3-f54d-42dc-aaa4-9e40011cac76')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f61ce6f3-f54d-42dc-aaa4-9e40011cac76 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f61ce6f3-f54d-42dc-aaa4-9e40011cac76');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bea6c1fc-ef9f-47cb-ba80-cbd57001471b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bea6c1fc-ef9f-47cb-ba80-cbd57001471b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bea6c1fc-ef9f-47cb-ba80-cbd57001471b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['question']=df['qa'].apply(lambda x:x[0])\n",
        "df['answer']=df['qa'].apply(lambda x:x[1])\n",
        "df.drop(['qa'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "SqPEzCXYFCer"
      },
      "id": "SqPEzCXYFCer",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop_duplicates()\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1423
        },
        "id": "G46dELbwm-0u",
        "outputId": "6ed50a71-a45a-4030-a663-650b95baa963"
      },
      "id": "G46dELbwm-0u",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                   abstract  \\\n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "0   ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...   \n",
              "1   INTRODUCTION\\n  Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done v...   \n",
              "..                                                                                                                                                                                                      ...   \n",
              "62  There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...   \n",
              "62  There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...   \n",
              "62  There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...   \n",
              "62  There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...   \n",
              "62  There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...   \n",
              "\n",
              "                                                                       question  \\\n",
              "0                 What is an important paradigm of natural language processing?   \n",
              "0                     What becomes less feasible as we pre-train larger models?   \n",
              "0                                                            What does LoRA do?   \n",
              "0            How many times can LoRA reduce the number of trainable parameters?   \n",
              "1                                    What is the major downside of fine-tuning?   \n",
              "..                                                                          ...   \n",
              "62                                              What can LoRA be combined with?   \n",
              "62                                                      What is far from clear?   \n",
              "62            What do we believe makes it more tractable than full fine-tuning?   \n",
              "62  What do we mostly depend on to select the weight matrices to apply LoRA to?   \n",
              "62                                What suggests that W could be rank-deficient?   \n",
              "\n",
              "                                                                                                                                    answer  \n",
              "0                                             large-scale pretraining on general domain data and adaptation to particular tasks or domains  \n",
              "0                                                                                                                         full fine-tuning  \n",
              "0   freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture  \n",
              "0                                                                                                                             10,000 times  \n",
              "1                                                                       the new model contains as many parameters as in the original model  \n",
              "..                                                                                                                                     ...  \n",
              "62                                                                                                      other efficient adaptation methods  \n",
              "62                                                                                                The mechanism behind fine-tuning or LoRA  \n",
              "62                                                                                                                                    LoRA  \n",
              "62                                                                                                                              heuristics  \n",
              "62                                                                                                                          rank-deficient  \n",
              "\n",
              "[220 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de96f62b-6c4f-415e-9fd9-98723ea8812e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>What is an important paradigm of natural language processing?</td>\n",
              "      <td>large-scale pretraining on general domain data and adaptation to particular tasks or domains</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>What becomes less feasible as we pre-train larger models?</td>\n",
              "      <td>full fine-tuning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>What does LoRA do?</td>\n",
              "      <td>freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABSTRACT\\n  An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger m...</td>\n",
              "      <td>How many times can LoRA reduce the number of trainable parameters?</td>\n",
              "      <td>10,000 times</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>INTRODUCTION\\n  Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done v...</td>\n",
              "      <td>What is the major downside of fine-tuning?</td>\n",
              "      <td>the new model contains as many parameters as in the original model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...</td>\n",
              "      <td>What can LoRA be combined with?</td>\n",
              "      <td>other efficient adaptation methods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...</td>\n",
              "      <td>What is far from clear?</td>\n",
              "      <td>The mechanism behind fine-tuning or LoRA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...</td>\n",
              "      <td>What do we believe makes it more tractable than full fine-tuning?</td>\n",
              "      <td>LoRA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...</td>\n",
              "      <td>What do we mostly depend on to select the weight matrices to apply LoRA to?</td>\n",
              "      <td>heuristics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>There are many directions for future works. 1) LoRA can be combined with other efﬁcient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoR...</td>\n",
              "      <td>What suggests that W could be rank-deficient?</td>\n",
              "      <td>rank-deficient</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de96f62b-6c4f-415e-9fd9-98723ea8812e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-de96f62b-6c4f-415e-9fd9-98723ea8812e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-de96f62b-6c4f-415e-9fd9-98723ea8812e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-25c1d46a-7317-4c4a-a4c9-a35472e9f699\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-25c1d46a-7317-4c4a-a4c9-a35472e9f699')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-25c1d46a-7317-4c4a-a4c9-a35472e9f699 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9bc9b092-a5e5-448c-a5c1-0e2e95717ea9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9bc9b092-a5e5-448c-a5c1-0e2e95717ea9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(data_dir+'/'+'LORA.csv',index=False)"
      ],
      "metadata": {
        "id": "ZDFPjyUsj4pk"
      },
      "id": "ZDFPjyUsj4pk",
      "execution_count": 100,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}